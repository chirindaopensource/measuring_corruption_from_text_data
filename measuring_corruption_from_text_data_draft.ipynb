{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_wAXD3TOFOy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`README.md`**\n",
        "\n",
        "# Measuring Corruption from Text Data: Automated Quantification of Institutional Quality\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2512.09652-b31b1b.svg)](https://arxiv.org/abs/2512.09652)\n",
        "[![Journal](https://img.shields.io/badge/Journal-Political%20Economy%20(econ.GN)-003366)](https://arxiv.org/abs/2512.09652)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/measuring_corruption_from_text_data)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Political%20Economy%20%7C%20NLP-00529B)](https://github.com/chirindaopensource/measuring_corruption_from_text_data)\n",
        "[![Data Sources](https://img.shields.io/badge/Data-CGU%20Audit%20Reports-lightgrey)](https://www.gov.br/cgu/pt-br)\n",
        "[![Data Sources](https://img.shields.io/badge/Data-IBGE%20(Municipal%20Covariates)-lightgrey)](https://www.ibge.gov.br/)\n",
        "[![Data Sources](https://img.shields.io/badge/Data-Ferraz%20%26%20Finan%20(2011)-lightgrey)](https://www.aeaweb.org/articles?id=10.1257/aer.101.4.1274)\n",
        "[![Data Sources](https://img.shields.io/badge/Data-Timmons%20%26%20Garfias%20(2015)-lightgrey)](https://www.sciencedirect.com/science/article/abs/pii/S030438781400138X)\n",
        "[![Core Method](https://img.shields.io/badge/Method-Dictionary--Based%20Classification-orange)](https://github.com/chirindaopensource/measuring_corruption_from_text_data)\n",
        "[![Analysis](https://img.shields.io/badge/Analysis-Principal%20Component%20Analysis%20(PCA)-red)](https://github.com/chirindaopensource/measuring_corruption_from_text_data)\n",
        "[![Validation](https://img.shields.io/badge/Validation-Econometric%20Fixed%20Effects-green)](https://github.com/chirindaopensource/measuring_corruption_from_text_data)\n",
        "[![Robustness](https://img.shields.io/badge/Robustness-Supervised%20Learning%20(LR%2FNB)-yellow)](https://github.com/chirindaopensource/measuring_corruption_from_text_data)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type%20checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![Scikit-Learn](https://img.shields.io/badge/scikit--learn-%23F7931E.svg?style=flat&logo=scikit-learn&logoColor=white)](https://scikit-learn.org/)\n",
        "[![NLTK](https://img.shields.io/badge/NLTK-%23339933.svg?style=flat&logo=python&logoColor=white)](https://www.nltk.org/)\n",
        "[![Statsmodels](https://img.shields.io/badge/statsmodels-blue?logo=python&logoColor=white)](https://www.statsmodels.org/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/measuring_corruption_from_text_data`\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2025 paper entitled **\"Measuring Corruption from Text Data\"** by:\n",
        "\n",
        "*   **Arieda Muço** (Central European University)\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for replicating the paper's findings. It delivers a modular, auditable, and extensible pipeline that executes the entire research workflow: from the heuristic extraction of irregularities from unstructured audit reports and dictionary-based classification to dimensionality reduction via PCA, rigorous econometric validation against human experts, and supervised learning robustness checks.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: `execute_full_research_pipeline`](#key-callable-execute_full_research_pipeline)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [Recommended Extensions](#recommended-extensions)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the analytical framework presented in Muço (2025). The core of this repository is the iPython Notebook `measuring_corruption_from_text_data_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings. The pipeline is designed to be a generalizable toolkit for quantifying institutional quality from unstructured administrative text, specifically focusing on the Brazilian municipal audit program (CGU).\n",
        "\n",
        "The paper addresses the fundamental challenge of measuring corruption—a hidden phenomenon—by leveraging the \"administrative exhaust\" of government audits. This codebase operationalizes the paper's framework, allowing users to:\n",
        "-   Rigorously validate and manage the entire experimental configuration via a single `config.yaml` file.\n",
        "-   Extract granular irregularity segments from heterogeneous PDF-derived text using regex-based heuristics.\n",
        "-   Classify irregularities as \"severe\" or \"non-severe\" using a domain-specific Portuguese dictionary.\n",
        "-   Construct a latent **Corruption Index** via Principal Component Analysis (PCA) on text-derived features.\n",
        "-   Validate the automated measure against hand-coded datasets (Ferraz & Finan, Timmons & Garfias) using fixed-effects regression models.\n",
        "-   Verify robustness using supervised machine learning classifiers (Logistic Regression, Naive Bayes) and Leave-One-Out (LOO) sensitivity analysis.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods combine techniques from Natural Language Processing (NLP), Unsupervised Learning, and Econometrics.\n",
        "\n",
        "**1. Text-as-Data Extraction & Classification:**\n",
        "The pipeline treats audit reports as data. It handles structural shifts in reporting (introduction of summaries in later lotteries) to isolate \"irregularity segments.\"\n",
        "-   **Dictionary Method:** A deterministic rule classifies an irregularity $I_{ij}$ as severe if it contains specific n-grams (e.g., \"empresa fantasma\", \"fraud\") from a curated lexicon $\\mathcal{L}$:\n",
        "    $$ Severe(I_{ij}) = \\mathbb{1}\\{\\exists \\ell \\in \\mathcal{L} : Match(\\ell, \\phi(I_{ij}))\\} $$\n",
        "    where $\\phi(\\cdot)$ represents the text normalization pipeline (stemming, stopword removal).\n",
        "\n",
        "**2. Dimensionality Reduction (PCA):**\n",
        "To synthesize a single measure from correlated text features (image counts, page counts, severe irregularity counts), PCA is applied to the standardized feature matrix $Z$. The first principal component $v_1$ serves as the index:\n",
        "    $$ \\text{Corruption Index}_i = Z_i^\\top v_1 $$\n",
        "This component captures ~80% of the common variation, representing the latent \"severity\" dimension.\n",
        "\n",
        "**3. Econometric Validation:**\n",
        "The automated index is validated by regressing human-coded corruption counts ($HC_i$) on the index, controlling for state fixed effects ($\\tau_t$) to account for auditor team heterogeneity:\n",
        "    $$ HC_i = \\alpha + \\beta \\text{Corruption Index}_i + \\tau_t + \\varepsilon_i $$\n",
        "Strong predictive power ($R^2 > 0.70$) in high-agreement samples confirms criterion validity.\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`measuring_corruption_from_text_data_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Modular, Multi-Task Architecture:** The pipeline is decomposed into 19 distinct, modular tasks, each with its own orchestrator function.\n",
        "-   **Configuration-Driven Design:** All study parameters (lottery cutoffs, regex markers, PCA thresholds) are managed in an external `config.yaml` file.\n",
        "-   **Rigorous Data Validation:** A multi-stage validation process checks schema integrity, key uniqueness, and logical consistency of the corpus and validation data.\n",
        "-   **Advanced NLP Pipeline:** Implements NFD normalization, accent stripping, and Porter stemming tailored for Portuguese administrative text.\n",
        "-   **Robustness Verification:** Includes automated Leave-One-Out (LOO) analysis and a parallel Supervised Learning pipeline to cross-validate the dictionary-based measure.\n",
        "-   **Reproducible Artifacts:** Generates structured dictionaries and serializable outputs for every intermediate result, ensuring full auditability.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Validation & Cleansing (Tasks 1-6):** Ingests raw corpus and validation data, normalizes identifiers (7-digit IBGE codes), cleanses text encoding, and validates numeric metadata.\n",
        "2.  **Extraction & NLP (Tasks 7-9):** Applies heuristic parsing to extract irregularity segments, normalizes text and lexicon into a shared matching space, and classifies irregularities as severe/non-severe.\n",
        "3.  **Index Construction (Task 10):** Builds the feature matrix, standardizes data, computes PCA, and generates the Corruption Index.\n",
        "4.  **Econometric Validation (Tasks 11-14):** Merges the index with external datasets, constructs agreement samples, and estimates validation regressions (Tables 1, 2, and 3).\n",
        "5.  **Robustness Checks (Tasks 16-19):** Performs LOO sensitivity analysis and executes a full supervised learning pipeline (training classifiers, rebuilding the index with ML predictions) to confirm result stability.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `measuring_corruption_from_text_data_draft.ipynb` notebook is structured as a logical pipeline with modular orchestrator functions for each of the 19 major tasks. All functions are self-contained, fully documented with type hints and docstrings, and designed for professional-grade execution.\n",
        "\n",
        "## Key Callable: `execute_full_research_pipeline`\n",
        "\n",
        "The project is designed around a single, top-level user-facing interface function:\n",
        "\n",
        "-   **`execute_full_research_pipeline`:** This master orchestrator function runs the entire automated research pipeline from end-to-end. A single call to this function reproduces the entire computational portion of the project, managing data flow between the main analysis, LOO robustness, and supervised learning robustness modules.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   Core dependencies: `pandas`, `numpy`, `scikit-learn`, `statsmodels`, `nltk`, `pyyaml`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/measuring_corruption_from_text_data.git\n",
        "    cd measuring_corruption_from_text_data\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install pandas numpy scikit-learn statsmodels nltk pyyaml\n",
        "    ```\n",
        "\n",
        "4.  **Download NLTK Data:**\n",
        "    The pipeline will attempt to download necessary NLTK data (stopwords) automatically, but you can pre-install them:\n",
        "    ```python\n",
        "    import nltk\n",
        "    nltk.download('stopwords')\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline requires two primary DataFrames:\n",
        "1.  **`df_raw_corpus`**: The corpus of audit reports with columns: `report_id`, `municipality_id`, `report_full_text`, `report_summary_text`, `lottery`, `year`, `image_count`, `page_count`, etc.\n",
        "2.  **`df_validation_raw`**: External validation data with columns: `municipality_id`, `ff_corruption_count`, `gt_corruption_count`, `cgu_severe_count`, and municipal covariates (`literacy_rate`, `gdp_per_capita`, etc.).\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `measuring_corruption_from_text_data_draft.ipynb` notebook provides a complete, step-by-step guide. The primary workflow is to execute the final cell of the notebook, which demonstrates how to use the top-level `execute_full_research_pipeline` orchestrator:\n",
        "\n",
        "```python\n",
        "# Final cell of the notebook\n",
        "\n",
        "# This block serves as the main entry point for the entire project.\n",
        "if __name__ == '__main__':\n",
        "    # 1. Load the master configuration from the YAML file.\n",
        "    import yaml\n",
        "    with open('config.yaml', 'r') as f:\n",
        "        study_config = yaml.safe_load(f)\n",
        "    \n",
        "    # 2. Load raw datasets (Example using synthetic generator provided in the notebook)\n",
        "    # In production, load from CSV/Parquet: pd.read_csv(...)\n",
        "    df_raw_corpus = ...\n",
        "    df_validation_raw = ...\n",
        "    \n",
        "    # 3. Define Lexicon\n",
        "    raw_lexicon_list = [\"empresa fantasma\", \"fraud\", \"conluio\", ...]\n",
        "\n",
        "    # 4. Execute the entire replication study.\n",
        "    results = execute_full_research_pipeline(\n",
        "        df_raw_corpus=df_raw_corpus,\n",
        "        df_validation_raw=df_validation_raw,\n",
        "        language=\"Portuguese\",\n",
        "        raw_lexicon_list=raw_lexicon_list,\n",
        "        study_configuration=study_config\n",
        "    )\n",
        "    \n",
        "    # 5. Access results\n",
        "    print(f\"Main Pipeline R2 (Strict Agreement): {results['main_pipeline']['table1_results']['Strict Agreement']['R2']}\")\n",
        "```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The pipeline returns a master dictionary containing all analytical artifacts:\n",
        "-   **`main_pipeline`**: Contains `df_corpus_with_index` (the final index), `table1_results` (validation regressions), `table2_results` (CGU validation), `table3_results` (correlates), and `pca_artifacts`.\n",
        "-   **`loo_analysis`**: Contains `detailed_results` (per-iteration stats) and `summary` (min/max ranges for $\\beta$ and $R^2$).\n",
        "-   **`supervised_robustness`**: Contains `classification_reports`, `ml_pca_index`, and `comparison_stats` (correlation between dictionary and ML indices).\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "measuring_corruption_from_text_data/\n",
        "│\n",
        "├── measuring_corruption_from_text_data_draft.ipynb  # Main implementation notebook\n",
        "├── config.yaml                                      # Master configuration file\n",
        "├── requirements.txt                                 # Python package dependencies\n",
        "│\n",
        "├── LICENSE                                          # MIT Project License File\n",
        "└── README.md                                        # This file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `config.yaml` file. Users can modify study parameters such as:\n",
        "-   **Parsing Logic:** `lottery_cutoff`, `regex_start_marker`.\n",
        "-   **NLP Settings:** `stemmer_algorithm`, `dictionary_ngram_range`.\n",
        "-   **PCA Settings:** `pca_input_features`, `eigenvalue_threshold`.\n",
        "-   **Econometrics:** `robust_se_type`, `validation_fixed_effects`.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## Recommended Extensions\n",
        "\n",
        "Future extensions could include:\n",
        "-   **LLM Integration:** Replacing the dictionary classifier with Large Language Models (e.g., BERT, GPT) to test if contextual embeddings improve severity classification.\n",
        "-   **Temporal Analysis:** Extending the model to analyze trends in corruption severity over time.\n",
        "-   **Cross-National Application:** Adapting the dictionary and extraction logic for audit reports from other countries (e.g., Mexico, Puerto Rico).\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{muco2025measuring,\n",
        "  title={Measuring Corruption from Text Data},\n",
        "  author={Muço, Arieda},\n",
        "  journal={arXiv preprint arXiv:2512.09652},\n",
        "  year={2025}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). Automated Quantification of Institutional Quality: An Open Source Implementation.\n",
        "GitHub repository: https://github.com/chirindaopensource/measuring_corruption_from_text_data\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to **Arieda Muço** for the foundational research that forms the entire basis for this computational replication.\n",
        "-   This project is built upon the exceptional tools provided by the open-source community. Sincere thanks to the developers of the scientific Python ecosystem, including **Pandas, NumPy, Scikit-Learn, and Statsmodels**.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of the `measuring_corruption_from_text_data_draft.ipynb` notebook and follows best practices for research software documentation.*\n"
      ],
      "metadata": {
        "id": "2ZhPEdWDe7hp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*Measuring Corruption from Text Data*\"\n",
        "\n",
        "Authors: Arieda Muço\n",
        "\n",
        "E-Journal Submission Date: 10 December 2025\n",
        "\n",
        "Link: https://arxiv.org/abs/2512.09652\n",
        "\n",
        "Authors' Github Project Reprository: : https://github.com/ariedamuco/Audit-reports/blob/master/list-words\n",
        "\n",
        "Reformulated Abstract:\n",
        "\n",
        "Corruption undermines democratic governance and economic development, yet its hidden nature poses fundamental measurement challenges. Traditional approaches—perception-based indices and manual audit coding—suffer from subjectivity, limited scalability, and high costs. This paper introduces an automated corruption measure constructed from the full text of Brazilian municipal audit reports (2003–2011, N = 2,194 inspections). The methodology combines a domain-specific dictionary that classifies irregularities by severity with principal component analysis to extract a latent corruption dimension. The first principal component captures 80% of the common variation across five text-derived features. Validation against independent human coders demonstrates strong criterion validity, with the automated index explaining 71–73% of variance in hand-coded corruption counts where coder agreement is highest. The index also predicts official government severity classifications (R² ≈ 0.31) and correlates with municipal characteristics theoretically linked to corruption risk, while capturing substantial information beyond these observables (incremental R² = 0.17). Robustness checks using supervised learning classifiers (logistic regression, Naive Bayes) yield nearly identical municipal rankings (R² > 0.98), confirming that the dictionary approach recovers the same underlying construct. The method offers advantages over both manual coding—in scalability and consistency—and large language models—in transparency, cost, and long-term replicability. This validated measure expands feasible sample sizes by a factor of three to five, enabling fine-grained analyses of corruption's political and economic consequences."
      ],
      "metadata": {
        "id": "xbk0kXgUOc6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### The Identification Problem & Data Generating Process\n",
        "The paper identifies a critical bottleneck in political economy: corruption is inherently hidden, and existing measures are either subjective (perception indices) or non-scalable (manual coding of audit reports).\n",
        "*   **The Data Source:** The study utilizes the corpus of Brazilian municipal audit reports generated by the *Controladoria Geral da União* (CGU) between 2003 and 2011. These audits are assigned via a random lottery, providing a quasi-experimental setting.\n",
        "*   **The Challenge:** The data is high-dimensional and unstructured text. The objective is to map this text to a scalar index representing \"corruption severity.\"\n",
        "\n",
        "### Feature Extraction (The Dictionary Approach)\n",
        "Instead of relying immediately on \"black box\" deep learning, the author employs a transparent, rule-based extraction method (Natural Language Processing).\n",
        "*   **Preprocessing:** Text is stemmed and tokenized.\n",
        "*   **Dictionary Construction:** A domain-specific dictionary of n-grams is created to distinguish \"severe\" irregularities (e.g., *fraud, ghost firm, procurement simulation*) from administrative errors.\n",
        "*   **Feature Vector:** For each municipality $i$, the algorithm extracts a vector of variables:\n",
        "    1.  Count of severe irregularities (via dictionary).\n",
        "    2.  Total count of irregularities.\n",
        "    3.  Metadata proxies for audit depth: Number of pages, number of lines, and number of photographic images (evidence).\n",
        "\n",
        "### Dimensionality Reduction (Principal Component Analysis)\n",
        "The extracted features exhibit high multicollinearity (pairwise correlations range from 0.56 to 0.96). To isolate the latent variable—corruption severity—the author applies Principal Component Analysis (PCA).\n",
        "*   **The First Component:** The first principal component accounts for **80% of the common variation** and is the only component with an eigenvalue greater than one (Kaiser criterion).\n",
        "*   **Loadings:** All variables load positively and with similar magnitude (0.37–0.48), confirming that the index is a composite measure of severity and audit intensity, not merely a proxy for report length.\n",
        "\n",
        "### Econometric Validation (Criterion Validity)\n",
        "The core contribution is the rigorous validation of this automated index against \"ground truth\" human coding.\n",
        "*   **Benchmarks:** The index is regressed against hand-coded datasets from seminal papers: Ferraz and Finan (2011) and Timmons and Garfias (2015).\n",
        "*   **The Model:** $HC_i = \\alpha + \\beta \\text{Index}_i + \\tau_t + \\epsilon_i$, where $HC$ is the human count and $\\tau$ are state fixed effects.\n",
        "*   **Results:**\n",
        "    *   In samples where human coders are in **strict agreement**, the automated index explains **71–73% of the variation** ($R^2 \\approx 0.72$).\n",
        "    *   This suggests the automated measure captures the same underlying signal as expert humans, with the remaining error likely attributable to human inconsistency in ambiguous cases.\n",
        "\n",
        "### External and Construct Validity\n",
        "The author tests the index against alternative specifications to ensure it is not measuring noise or unrelated structural factors.\n",
        "*   **CGU Validation:** The index correlates with the CGU’s own later classification of \"severe\" irregularities ($R^2 \\approx 0.31$). The lower correlation is expected, as CGU focuses on fiscal impact while the index (and academic coders) focuses on criminality/intent.\n",
        "*   **Orthogonality to Covariates:** The index correlates with municipal characteristics in theoretically predicted ways (negative correlation with literacy/GDP; positive with distance to capital). Crucially, these covariates explain only **17%** of the variance in the index. This implies the text data contains substantial unique signal that cannot be predicted simply by looking at a municipality's socio-economic status.\n",
        "\n",
        "### Robustness via Supervised Learning\n",
        "To verify the dictionary approach, the author trains supervised machine learning models (Logistic Regression and Naive Bayes) using the human-coded data as labels.\n",
        "*   **Feature Importance:** The supervised models identify bigrams (e.g., \"procurement contract,\" \"release resources\") that largely overlap with the hand-crafted dictionary.\n",
        "*   **Convergence:** A PCA constructed from the supervised learning predictions yields a ranking that is **98% correlated** with the dictionary-based PCA.\n",
        "*   **Implication:** The dictionary method is robust; it recovers the same latent dimension as supervised methods but without the dependency on expensive, potentially noisy labeled training data.\n",
        "\n",
        "### Conclusion\n",
        "The paper demonstrates that a transparent, dictionary-based PCA approach provides a valid, cost-effective, and scalable measure of corruption. It offers distinct advantages over Large Language Models (LLMs) regarding long-run replicability (avoiding API drift) and transparency, allowing researchers to expand corruption studies to the full universe of Brazilian municipalities."
      ],
      "metadata": {
        "id": "8s6ljupcV8eh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "qE9Fki7Ybp1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ==============================================================================#\n",
        "#\n",
        "#  Automated Quantification of Institutional Quality\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  text-as-data framework presented in \"Measuring Corruption from Text Data\" by\n",
        "#  Arieda Muço (2025). It delivers a scalable, transparent, and reproducible\n",
        "#  system for quantifying corruption severity from unstructured audit reports,\n",
        "#  enabling high-fidelity measurement of institutional quality at the municipal\n",
        "#  level without reliance on subjective perception indices or costly manual coding.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • Heuristic extraction of irregularity segments from heterogeneous audit documents\n",
        "#  • Dictionary-based classification of corruption severity using domain-specific n-grams\n",
        "#  • Dimensionality reduction via Principal Component Analysis (PCA) to synthesize a latent index\n",
        "#  • Rigorous econometric validation against independent human expert coding and official benchmarks\n",
        "#  • Robustness verification using supervised machine learning classifiers (Logistic Regression, Naive Bayes)\n",
        "#  • Leave-One-Out (LOO) sensitivity analysis to ensure index stability\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • Deterministic text normalization pipeline (NFD, stemming, stopword removal)\n",
        "#  • Regex-based parsing logic handling regime shifts in document structure\n",
        "#  • Sparse matrix operations for efficient TF-IDF feature engineering\n",
        "#  • Fixed-effects regression modeling with heteroskedasticity-robust inference\n",
        "#  • Comprehensive data quality assurance and quarantine protocols\n",
        "#  • Reproducible artifact generation with full configuration and environment manifest\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Muço, A. (2025). Measuring Corruption from Text Data.\n",
        "#  arXiv preprint arXiv:2512.09652.\n",
        "#  https://arxiv.org/abs/2512.09652\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ==============================================================================#\n",
        "\n",
        "import sys\n",
        "import json\n",
        "import hashlib\n",
        "import logging\n",
        "import re\n",
        "import unicodedata\n",
        "import string\n",
        "from datetime import datetime\n",
        "from typing import (\n",
        "    List,\n",
        "    Dict,\n",
        "    Any,\n",
        "    Tuple,\n",
        "    Optional,\n",
        "    Set,\n",
        "    Union,\n",
        "    Callable\n",
        ")\n",
        "from dataclasses import dataclass, asdict\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.formula.api as smf\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n"
      ],
      "metadata": {
        "id": "rYmDzFZGbt-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "JyczVlM7bvX2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Draft 1\n",
        "\n",
        "## **Discussion of the Inputs-Processes-Outputs of Key Callables**\n",
        "\n",
        "### **1. `validate_corpus_inputs` (Task 1 Orchestrator)**\n",
        "\n",
        "*   **Inputs**: `df_raw_corpus` (pd.DataFrame).\n",
        "*   **Processes**: Sequentially invokes `validate_corpus_schema`, `validate_corpus_uniqueness`, and `validate_lottery_summary_consistency`. It aggregates errors and warnings from these sub-steps.\n",
        "*   **Outputs**: None (returns `None` on success, raises `ValueError` on failure).\n",
        "*   **Transformation**: This is a validation gate; it does not transform data but ensures the input state is valid for downstream processing.\n",
        "*   **Research Role**: Implements the data integrity checks required before any text processing can begin. It ensures the raw data conforms to the structure implied by the audit program description (Section 2), specifically validating the existence of critical metadata like `lottery` round and `report_id` which drive the extraction logic.\n",
        "\n",
        "### **2. `validate_validation_inputs` (Task 2 Orchestrator)**\n",
        "\n",
        "*   **Inputs**: `df_validation_raw` (pd.DataFrame), `df_raw_corpus` (pd.DataFrame).\n",
        "*   **Processes**: Invokes `validate_validation_schema`, `validate_join_feasibility`, and `validate_covariate_completeness`.\n",
        "*   **Outputs**: None (returns `None` on success, raises `ValueError` on failure).\n",
        "*   **Transformation**: Validation gate.\n",
        "*   **Research Role**: Ensures the external validation datasets (Ferraz & Finan, Timmons & Garfias, CGU) and municipal covariates are present and joinable. This is a prerequisite for the econometric validation in Section 4, ensuring that Equation (1) ($HC_i = \\alpha + \\beta \\text{Corruption Index}_i + \\tau_t + \\varepsilon_i$) can be estimated.\n",
        "\n",
        "### **3. `validate_config_inputs` (Task 3 Orchestrator)**\n",
        "\n",
        "*   **Inputs**: `language` (str), `raw_lexicon_list` (List[str]), `study_configuration` (Dict).\n",
        "*   **Processes**: Validates language settings, lexicon content, and configuration coherence (e.g., PCA parameters, lottery cutoffs).\n",
        "*   **Outputs**: `run_manifest` (Dict).\n",
        "*   **Transformation**: Transforms raw configuration inputs into a validated \"run manifest\" dictionary.\n",
        "*   **Research Role**: Enforces the methodological parameters defined in the paper, such as the lottery cutoff for summary extraction (Section 2) and the Kaiser criterion for PCA (Section 3.1).\n",
        "\n",
        "### **4. `cleanse_identifiers` (Task 4 Orchestrator)**\n",
        "\n",
        "*   **Inputs**: `df_raw_corpus` (pd.DataFrame), `df_validation_raw` (pd.DataFrame).\n",
        "*   **Processes**: Normalizes municipality IDs (to 7-digit strings) and state codes; quarantines rows with missing identifiers.\n",
        "*   **Outputs**: `df_corpus_clean`, `df_validation_clean`, `df_corpus_quarantined`, `df_validation_quarantined` (all pd.DataFrame).\n",
        "*   **Transformation**: Converts raw identifiers into canonical formats (`_canon` columns) and splits datasets based on validity.\n",
        "*   **Research Role**: Prepares the join keys for merging audit data with external validation sets. Accurate identifiers are critical for the fixed effects $\\tau_t$ in Equation (1), which rely on correct state codes.\n",
        "\n",
        "### **5. `cleanse_text_fields` (Task 5 Orchestrator)**\n",
        "\n",
        "*   **Inputs**: `df_raw_corpus` (pd.DataFrame).\n",
        "*   **Processes**: Normalizes text encoding (UTF-8) and line endings; validates text content presence; computes quality metrics.\n",
        "*   **Outputs**: `df_clean`, `df_quarantined` (pd.DataFrame), `metrics` (Dict).\n",
        "*   **Transformation**: Cleans text columns (`report_full_text`, `report_summary_text`) and filters out rows with empty text.\n",
        "*   **Research Role**: Ensures the unstructured text input is readable and consistent for the NLP pipeline. This step is foundational for the \"Text-as-Data\" approach described in Section 3.\n",
        "\n",
        "### **6. `cleanse_numeric_fields` (Task 6 Orchestrator)**\n",
        "\n",
        "*   **Inputs**: `df_raw_corpus` (pd.DataFrame).\n",
        "*   **Processes**: Validates and coerces numeric metadata (page count, image count, etc.); checks definition consistency; flags outliers.\n",
        "*   **Outputs**: `df_clean` (pd.DataFrame), `stats` (Dict).\n",
        "*   **Transformation**: Coerces numeric columns to integers and adds outlier flag columns.\n",
        "*   **Research Role**: Prepares the structural features ($x_i$) used in the PCA index construction (Section 3.1). Specifically, it ensures `image_count`, `page_count`, and `report_lines_count` are valid inputs for the matrix $X$.\n",
        "\n",
        "### **7. `extract_irregularities` (Task 7 Orchestrator)**\n",
        "\n",
        "*   **Inputs**: `df_raw_corpus` (pd.DataFrame), `study_configuration` (Dict).\n",
        "*   **Processes**: Applies the two-regime extraction logic (Summary parsing vs. Regex/Fallback) to each report; materializes the irregularity-level dataset.\n",
        "*   **Outputs**: `df_irregularities` (pd.DataFrame).\n",
        "*   **Transformation**: Explodes report-level text into a granular DataFrame where each row is a single irregularity segment.\n",
        "*   **Research Role**: Implements the heuristic extraction algorithm described in Appendix A1.3. It handles the structural heterogeneity of audit reports (summaries vs. full text) to isolate the units of analysis (irregularities).\n",
        "\n",
        "### **8. `normalize_text_and_lexicon` (Task 8 Orchestrator)**\n",
        "\n",
        "*   **Inputs**: `df_irregularities` (pd.DataFrame), `raw_lexicon_list` (List[str]), `study_configuration` (Dict).\n",
        "*   **Processes**: Defines the normalization function $\\phi(\\cdot)$; normalizes the lexicon; normalizes irregularity texts.\n",
        "*   **Outputs**: `df_norm` (pd.DataFrame), `norm_lexicon` (Set[str]), `norm_map` (Dict).\n",
        "*   **Transformation**: Applies NLP transformations (NFD, stemming, etc.) to text and lexicon, creating a shared matching space.\n",
        "*   **Research Role**: Implements the text preprocessing pipeline $\\phi(s)$ described in Appendix A2. This ensures that variations in surface forms (e.g., \"licitatórios\" vs. \"licitatório\") map to the same stem (\"licitatori\") for accurate dictionary matching.\n",
        "\n",
        "### **9. `classify_severe_and_count` (Task 9 Orchestrator)**\n",
        "\n",
        "*   **Inputs**: `df_irregularities` (pd.DataFrame), `normalized_lexicon` (Set[str]), `df_raw_corpus` (pd.DataFrame).\n",
        "*   **Processes**: Classifies each irregularity as severe/non-severe; aggregates counts to report level; merges counts back to corpus.\n",
        "*   **Outputs**: `df_classified` (pd.DataFrame), `df_corpus_counts` (pd.DataFrame).\n",
        "*   **Transformation**: Adds `is_severe` column to irregularities and `severe_irregularities_count` to the corpus.\n",
        "*   **Research Role**: Implements the dictionary-based classification rule: $Severe(I_{ij}) = 1 \\iff \\exists \\ell \\in \\mathcal{L} : Match(\\ell, \\phi(I_{ij}))$. It produces the `severe_irregularities_count` feature, a key input for the PCA index (Section 3.1).\n",
        "\n",
        "### **10. `build_pca_index` (Task 10 Orchestrator)**\n",
        "\n",
        "*   **Inputs**: `df_corpus_with_counts` (pd.DataFrame), `study_configuration` (Dict).\n",
        "*   **Processes**: Constructs matrix $X$; standardizes to $Z$; computes PCA; scores the index.\n",
        "*   **Outputs**: `df_final` (pd.DataFrame), `artifacts` (Dict).\n",
        "*   **Transformation**: Adds the `corruption_index` column to the corpus DataFrame.\n",
        "*   **Research Role**: Implements the dimensionality reduction described in Section 3.1. It computes the first principal component $v_1$ of the standardized feature matrix $Z$ and projects the data: $\\text{Corruption Index}_i = Z_i^\\top v_1$.\n",
        "\n",
        "### **11. `prepare_validation_samples` (Task 11 Orchestrator)**\n",
        "\n",
        "*   **Inputs**: `df_corpus_with_index` (pd.DataFrame), `df_validation_raw` (pd.DataFrame).\n",
        "*   **Processes**: Merges corpus and validation data; constructs agreement flags (`is_strict_agreement`, `is_near_agreement`); computes mean human-coded outcome.\n",
        "*   **Outputs**: `df_analysis` (pd.DataFrame).\n",
        "*   **Transformation**: Creates a merged analysis dataset with flags for validation subsamples.\n",
        "*   **Research Role**: Constructs the specific samples used for validation in Table 1. It operationalizes \"Strict Agreement\" ($FF_i = GT_i$) and \"Near Agreement\" ($|FF_i - GT_i| \\le 1$).\n",
        "\n",
        "### **12. `run_table1_regressions` (Task 12 Orchestrator)**\n",
        "\n",
        "*   **Inputs**: `df_analysis` (pd.DataFrame), `study_configuration` (Dict).\n",
        "*   **Processes**: Estimates Equation (1) for four specifications (Strict, Near, Full FF, Full GT); validates against targets.\n",
        "*   **Outputs**: `all_results` (Dict).\n",
        "*   **Transformation**: Produces regression statistics ($\\beta, R^2, N$) from the analysis data.\n",
        "*   **Research Role**: Reproduces Table 1 (\"Predicting Human Coders\"). It estimates $HC_i = \\alpha + \\beta \\text{Corruption Index}_i + \\tau_t + \\varepsilon_i$ to demonstrate criterion validity.\n",
        "\n",
        "### **13. `run_table2_regressions` (Task 13 Orchestrator)**\n",
        "\n",
        "*   **Inputs**: `df_analysis` (pd.DataFrame), `study_configuration` (Dict).\n",
        "*   **Processes**: Prepares CGU samples (Levels, Log); estimates regressions; validates against targets.\n",
        "*   **Outputs**: `results` (Dict).\n",
        "*   **Transformation**: Produces regression statistics for CGU outcomes.\n",
        "*   **Research Role**: Reproduces Table 2 (\"Predicting CGU\"). It validates the index against official severity classifications using both levels ($CGU_i$) and logs ($\\log(CGU_i)$).\n",
        "\n",
        "### **14. `run_table3_regressions` (Task 14 Orchestrator)**\n",
        "\n",
        "*   **Inputs**: `df_analysis` (pd.DataFrame), `study_configuration` (Dict).\n",
        "*   **Processes**: Prepares covariates (scaling, logs); estimates bivariate and multivariate regressions.\n",
        "*   **Outputs**: `results` (Dict).\n",
        "*   **Transformation**: Produces regression statistics for municipal correlates.\n",
        "*   **Research Role**: Reproduces Table 3 (\"Correlates of Corruption\"). It tests construct validity by showing the index correlates with theoretical predictors like literacy and GDP per capita: $\\text{Corruption Index}_i = \\alpha + \\gamma X_i + u_i$.\n",
        "\n",
        "### **15. `run_main_pipeline` (Task 15 Orchestrator)**\n",
        "\n",
        "*   **Inputs**: Raw dataframes, language, lexicon, configuration.\n",
        "*   **Processes**: Calls Tasks 1-14 sequentially (Validation -> Cleansing -> Extraction -> NLP -> PCA -> Regression).\n",
        "*   **Outputs**: `PipelineArtifacts` (Dataclass).\n",
        "*   **Transformation**: Orchestrates the primary research pipeline from raw data to final regression tables.\n",
        "*   **Research Role**: The central engine of the replication. It produces the \"Automated Corruption Index\" and all primary validation results (Tables 1, 2, 3).\n",
        "\n",
        "### **16. `run_loo_analysis` (Task 16 Orchestrator)**\n",
        "\n",
        "*   **Inputs**: `df_analysis` (pd.DataFrame), `study_configuration` (Dict).\n",
        "*   **Processes**: Iteratively drops one observation, re-estimates the validation model, and records stability metrics.\n",
        "*   **Outputs**: `results` (Dict containing detailed and summary stats).\n",
        "*   **Transformation**: Produces a distribution of $\\beta$ and $R^2$ values.\n",
        "*   **Research Role**: Implements the Leave-One-Out sensitivity analysis (Figure 2). It assesses whether the validation results are driven by outliers, ensuring robustness of the measure.\n",
        "\n",
        "### **17. `run_supervised_robustness` (Task 17 Orchestrator)**\n",
        "\n",
        "*   **Inputs**: `df_irregularities`, `df_analysis`, `df_corpus_counts`, `study_configuration`.\n",
        "*   **Processes**: Calls `build_supervised_data` and `train_and_compare`.\n",
        "*   **Outputs**: `SupervisedArtifacts` (Dataclass).\n",
        "*   **Transformation**: Orchestrates the entire supervised robustness pipeline.\n",
        "*   **Research Role**: High-level wrapper for the supervised learning analysis described in Section 5.2 and Appendix A4.\n",
        "\n",
        "### **18. `build_supervised_data` (Task 18 Orchestrator)**\n",
        "\n",
        "*   **Inputs**: `df_irregularities` (pd.DataFrame), `df_analysis` (pd.DataFrame), `study_configuration` (Dict).\n",
        "*   **Processes**: Constructs binary labels via median split; builds document corpus; vectorizes text (TF-IDF).\n",
        "*   **Outputs**: `bundle` (Dict containing $X_{train}, y_{train}$, vectorizer).\n",
        "*   **Transformation**: Converts text and labels into a format suitable for machine learning.\n",
        "*   **Research Role**: Prepares the data for the supervised learning robustness check (Appendix A4). It implements the label definition $y_i = \\mathbb{1}\\{Score_i \\ge \\text{Median}\\}$ and the TF-IDF weighting scheme.\n",
        "\n",
        "### **19. `train_and_compare` (Task 19 Orchestrator)**\n",
        "\n",
        "*   **Inputs**: `training_bundle` (Dict), `df_irregularities` (pd.DataFrame), `df_corpus_counts` (pd.DataFrame), `study_configuration` (Dict).\n",
        "*   **Processes**: Trains classifiers (LR, NB, SVM); predicts severity for all irregularities; rebuilds PCA index; compares indices.\n",
        "*   **Outputs**: `results` (Dict).\n",
        "*   **Transformation**: Produces trained models, a new ML-based index, and comparison statistics.\n",
        "*   **Research Role**: Executes the supervised learning robustness check. It verifies that an index built from ML-predicted severity ($CI_{ML}$) is highly correlated with the dictionary-based index ($CI_{dict}$), confirming the dictionary captures the same latent construct ($R^2 \\approx 0.98$).\n",
        "\n",
        "### **20. `execute_full_research_pipeline` (Top-Level Orchestrator)**\n",
        "\n",
        "*   **Inputs**: Raw dataframes, language, lexicon, configuration.\n",
        "*   **Processes**: Calls `run_main_pipeline`, `run_loo_analysis`, and `run_supervised_robustness`.\n",
        "*   **Outputs**: `final_output` (Dict).\n",
        "*   **Transformation**: Aggregates all research artifacts into a single return structure.\n",
        "*   **Research Role**: The master controller for the entire project. It ensures that the main analysis, sensitivity checks, and robustness verifications are executed in the correct order and with consistent data, delivering the complete set of evidence presented in the paper.\n",
        "<br><br>\n",
        "\n",
        "### **Example Usage: End-to-End Corruption Measurement Pipeline**\n",
        "\n",
        "This example demonstrates the execution of the `execute_full_research_pipeline` function. It covers:\n",
        "1.  **Configuration Loading**: Reading the study parameters from `config.yaml`.\n",
        "2.  **Data Synthesis**: Generating synthetic `df_raw_corpus` and `df_validation_raw` DataFrames that strictly adhere to the required schemas and data types.\n",
        "3.  **Pipeline Execution**: Running the orchestrator and capturing the results.\n",
        "4.  **Result Inspection**: Accessing the generated artifacts (indices, regression tables, robustness checks).\n",
        "\n",
        "#### **1. Setup and Configuration Loading**\n",
        "\n",
        "First, we import necessary libraries and load the study configuration. We assume `config.yaml` exists in the working directory.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yaml\n",
        "import logging\n",
        "\n",
        "# Ensure logging is configured to see pipeline progress\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def load_study_configuration(config_path: str = \"config.yaml\") -> dict:\n",
        "    \"\"\"\n",
        "    Loads the study configuration from a YAML file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(config_path, 'r') as file:\n",
        "            config = yaml.safe_load(file)\n",
        "        logging.info(f\"Configuration loaded successfully from {config_path}\")\n",
        "        return config\n",
        "    except FileNotFoundError:\n",
        "        logging.error(f\"Configuration file not found at {config_path}\")\n",
        "        raise\n",
        "    except yaml.YAMLError as e:\n",
        "        logging.error(f\"Error parsing YAML file: {e}\")\n",
        "        raise\n",
        "\n",
        "# Load configuration\n",
        "# Note: In a real scenario, ensure 'config.yaml' is present.\n",
        "# For this example, we assume the dict structure matches the YAML content provided previously.\n",
        "study_config = load_study_configuration()\n",
        "```\n",
        "\n",
        "#### **2. Synthetic Data Generation**\n",
        "\n",
        "We generate synthetic data to simulate the raw inputs. This ensures the example is runnable and demonstrates the expected data structures.\n",
        "\n",
        "**A. `df_raw_corpus` Generation**\n",
        "\n",
        "This DataFrame represents the raw audit reports. We simulate both early lotteries (requiring regex extraction) and later lotteries (using summaries).\n",
        "\n",
        "```python\n",
        "def generate_synthetic_corpus(n_reports: int = 100) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generates a synthetic df_raw_corpus adhering to the strict schema.\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    data = {\n",
        "        'report_id': [f\"RPT_{i:05d}\" for i in range(n_reports)],\n",
        "        'municipality_id': np.random.randint(1000000, 9999999, size=n_reports), # 7-digit IBGE\n",
        "        'state': np.random.choice(['SP', 'MG', 'RJ', 'BA', 'RS'], size=n_reports),\n",
        "        'lottery': np.random.randint(2, 35, size=n_reports),\n",
        "        'year': np.random.randint(2003, 2012, size=n_reports),\n",
        "        'text_extraction_mode': np.random.choice(['native_text', 'ocr'], size=n_reports),\n",
        "        'page_count': np.random.randint(10, 200, size=n_reports),\n",
        "        'image_count': np.random.randint(0, 50, size=n_reports),\n",
        "        'image_count_definition': [\"PDF XObjects\"] * n_reports,\n",
        "        'report_lines_count': np.random.randint(500, 5000, size=n_reports),\n",
        "        'lines_count_definition': [\"newline-delimited\"] * n_reports,\n",
        "        'pdf_source': [f\"http://cgu.gov.br/reports/report_{i}.pdf\" for i in range(n_reports)]\n",
        "    }\n",
        "    \n",
        "    df = pd.DataFrame(data)\n",
        "    \n",
        "    # Logic for text fields based on lottery round\n",
        "    # Lottery < 8: Full text extraction required\n",
        "    # Lottery >= 8: Summary text preferred\n",
        "    \n",
        "    full_texts = []\n",
        "    summary_texts = []\n",
        "    has_summary = []\n",
        "    \n",
        "    for lottery in df['lottery']:\n",
        "        # Simulate irregularity text\n",
        "        # We inject known keywords from the lexicon to ensure matches\n",
        "        irreg_text = \"Constatação da Fiscalização: Indícios de fraude em licitação e empresa fantasma. Fato: Ocorreu desvio.\"\n",
        "        \n",
        "        if lottery < 8:\n",
        "            full_texts.append(f\"Header... {irreg_text} ... Footer\")\n",
        "            summary_texts.append(None) # No summary in early reports\n",
        "            has_summary.append(False)\n",
        "        else:\n",
        "            full_texts.append(f\"Full report text content...\")\n",
        "            # Simulate enumerated summary\n",
        "            summary_texts.append(\"1.1 Irregularidade em licitação.\\n1.2 Falta de medicamentos.\")\n",
        "            has_summary.append(True)\n",
        "            \n",
        "    df['report_full_text'] = full_texts\n",
        "    df['report_summary_text'] = summary_texts\n",
        "    df['has_summary_detected'] = has_summary\n",
        "    \n",
        "    return df\n",
        "\n",
        "df_raw_corpus = generate_synthetic_corpus(n_reports=200)\n",
        "logging.info(f\"Generated df_raw_corpus with shape: {df_raw_corpus.shape}\")\n",
        "```\n",
        "\n",
        "**B. `df_validation_raw` Generation**\n",
        "\n",
        "This DataFrame contains the human-coded data and municipal covariates. We ensure overlap with the corpus `municipality_id` to allow merging.\n",
        "\n",
        "```python\n",
        "def generate_synthetic_validation(corpus_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generates synthetic df_validation_raw matching corpus municipalities.\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Get unique municipalities from corpus to ensure join matches\n",
        "    unique_munis = corpus_df[['municipality_id', 'state']].drop_duplicates()\n",
        "    n_munis = len(unique_munis)\n",
        "    \n",
        "    # Generate validation data\n",
        "    data = {\n",
        "        'municipality_id': unique_munis['municipality_id'].values,\n",
        "        'state': unique_munis['state'].values,\n",
        "        'year': np.random.choice([2003, 2004, 2005], size=n_munis), # Audit year context\n",
        "        \n",
        "        # Human coded counts (Poisson-like)\n",
        "        'ff_corruption_count': np.random.poisson(lam=1.5, size=n_munis).astype(float),\n",
        "        'gt_corruption_count': np.random.poisson(lam=2.0, size=n_munis).astype(float),\n",
        "        \n",
        "        # CGU counts (higher magnitude, only for some)\n",
        "        'cgu_severe_count': np.random.poisson(lam=5.0, size=n_munis).astype(float),\n",
        "        \n",
        "        # Covariates (Reference Year 2000)\n",
        "        'covariate_reference_year': [2000] * n_munis,\n",
        "        'literacy_rate': np.random.uniform(0.70, 0.99, size=n_munis),\n",
        "        'gdp_per_capita': np.random.lognormal(mean=8, sigma=1, size=n_munis),\n",
        "        'urban_population_share': np.random.beta(a=2, b=1, size=n_munis),\n",
        "        'population_total': np.random.randint(5000, 500000, size=n_munis).astype(float),\n",
        "        'distance_to_capital_km': np.random.uniform(100, 3000, size=n_munis),\n",
        "        'has_radio_fm': np.random.choice([0, 1], size=n_munis)\n",
        "    }\n",
        "    \n",
        "    df = pd.DataFrame(data)\n",
        "    \n",
        "    # Introduce some missingness to test robustness\n",
        "    mask_missing_cgu = np.random.rand(n_munis) < 0.3\n",
        "    df.loc[mask_missing_cgu, 'cgu_severe_count'] = np.nan\n",
        "    \n",
        "    # Ensure population_log is derivable or present\n",
        "    df['population_log'] = np.log(df['population_total'])\n",
        "    \n",
        "    return df\n",
        "\n",
        "df_validation_raw = generate_synthetic_validation(df_raw_corpus)\n",
        "logging.info(f\"Generated df_validation_raw with shape: {df_validation_raw.shape}\")\n",
        "```\n",
        "\n",
        "#### **3. Pipeline Execution**\n",
        "\n",
        "We define the language and lexicon parameters, then invoke the top-level orchestrator.\n",
        "\n",
        "```python\n",
        "# Define Language\n",
        "language = \"Portuguese\"\n",
        "\n",
        "# Define Raw Lexicon List (Portuguese n-grams)\n",
        "# This matches the structure expected by the dictionary normalization task\n",
        "raw_lexicon_list = [\n",
        "    \"empresa fantasma\",\n",
        "    \"empresa inexistente\",\n",
        "    \"fraud\",\n",
        "    \"conluio\",\n",
        "    \"fals\",\n",
        "    \"simulacao licitat\",\n",
        "    \"montagem licitat\",\n",
        "    \"superfaturamento\"\n",
        "]\n",
        "\n",
        "# Execute the Pipeline\n",
        "try:\n",
        "    pipeline_results = execute_full_research_pipeline(\n",
        "        df_raw_corpus=df_raw_corpus,\n",
        "        df_validation_raw=df_validation_raw,\n",
        "        language=language,\n",
        "        raw_lexicon_list=raw_lexicon_list,\n",
        "        study_configuration=study_config\n",
        "    )\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PIPELINE EXECUTION SUCCESSFUL\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\nPipeline Execution Failed: {e}\")\n",
        "```\n",
        "\n",
        "#### **4. Result Inspection**\n",
        "\n",
        "The `pipeline_results` dictionary contains all artifacts. We can inspect specific outputs to verify the analysis.\n",
        "\n",
        "```python\n",
        "if 'pipeline_results' in locals():\n",
        "    # 1. Inspect Main Pipeline Artifacts\n",
        "    main_artifacts = pipeline_results['main_pipeline']\n",
        "    \n",
        "    print(\"\\n--- Main Pipeline Artifacts ---\")\n",
        "    print(f\"Cleaned Corpus Size: {len(main_artifacts['df_corpus_clean'])}\")\n",
        "    print(f\"Extracted Irregularities: {len(main_artifacts['df_irregularities'])}\")\n",
        "    \n",
        "    # Check PCA Index\n",
        "    print(\"\\n--- Corruption Index (Head) ---\")\n",
        "    print(main_artifacts['df_corpus_with_index'][['report_id', 'corruption_index']].head())\n",
        "    \n",
        "    # Check Table 1 Regression Results (Validation vs Human Coders)\n",
        "    print(\"\\n--- Table 1: Regression Results (Strict Agreement) ---\")\n",
        "    t1_res = main_artifacts['table1_results']['Strict Agreement']\n",
        "    print(f\"Beta: {t1_res.get('beta', 'N/A'):.4f}\")\n",
        "    print(f\"R2:   {t1_res.get('R2', 'N/A'):.4f}\")\n",
        "    print(f\"N:    {t1_res.get('N', 'N/A')}\")\n",
        "\n",
        "    # 2. Inspect Leave-One-Out Robustness\n",
        "    loo_results = pipeline_results['loo_analysis']\n",
        "    print(\"\\n--- LOO Robustness Summary (Strict Agreement) ---\")\n",
        "    if 'summary' in loo_results and 'Strict' in loo_results['summary']:\n",
        "        stats = loo_results['summary']['Strict']\n",
        "        print(f\"Beta Range: [{stats['beta_min']:.4f}, {stats['beta_max']:.4f}]\")\n",
        "        print(f\"R2 Range:   [{stats['r2_min']:.4f}, {stats['r2_max']:.4f}]\")\n",
        "\n",
        "    # 3. Inspect Supervised Learning Robustness\n",
        "    sup_results = pipeline_results['supervised_robustness']\n",
        "    print(\"\\n--- Supervised Learning Comparison ---\")\n",
        "    comp = sup_results['comparison_stats']\n",
        "    print(f\"Correlation (Dict-PCA vs ML-PCA): {comp.get('correlation', 0):.4f}\")\n",
        "    print(f\"R2 (Dict-PCA vs ML-PCA):          {comp.get('R2', 0):.4f}\")\n",
        "```\n"
      ],
      "metadata": {
        "id": "ZFGFdlOzQTEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1 — Validate schema, types, and uniqueness of `df_raw_corpus`\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 1: Validate schema, types, and uniqueness of df_raw_corpus\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Step 1: Verify presence and data types of all 15 required columns\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_corpus_schema(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Validates the presence and semantic types of the 15 required columns in the raw corpus DataFrame.\n",
        "\n",
        "    This function enforces the schema requirements derived from the research design. It checks for\n",
        "    the existence of specific columns and validates their content types (e.g., integer-likeness\n",
        "    for IDs, string format for states). It accumulates all validation errors and raises a single\n",
        "    ValueError if any critical constraints are violated.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The raw corpus DataFrame containing audit report data.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If any required column is missing or if any column violates type constraints.\n",
        "    \"\"\"\n",
        "    # List of required columns as specified in the task\n",
        "    required_columns = [\n",
        "        \"report_id\", \"municipality_id\", \"state\", \"lottery\", \"year\",\n",
        "        \"report_full_text\", \"report_summary_text\", \"has_summary_detected\",\n",
        "        \"text_extraction_mode\", \"page_count\", \"image_count\",\n",
        "        \"image_count_definition\", \"report_lines_count\", \"lines_count_definition\",\n",
        "        \"pdf_source\"\n",
        "    ]\n",
        "\n",
        "    # 1. Check for missing columns\n",
        "    # Get the set of columns present in the DataFrame\n",
        "    existing_columns = set(df.columns)\n",
        "    # Identify any missing required columns\n",
        "    missing_columns = set(required_columns) - existing_columns\n",
        "\n",
        "    # Log extra columns for awareness (not a failure condition)\n",
        "    extra_columns = existing_columns - set(required_columns)\n",
        "    if extra_columns:\n",
        "        logger.info(f\"Extra columns detected in corpus: {extra_columns}\")\n",
        "\n",
        "    # Fail if columns are missing\n",
        "    if missing_columns:\n",
        "        error_msg = f\"Missing required columns in df_raw_corpus: {missing_columns}\"\n",
        "        logger.error(error_msg)\n",
        "        raise ValueError(error_msg)\n",
        "\n",
        "    # 2. Validate semantic types\n",
        "    validation_errors = []\n",
        "\n",
        "    # Helper to check integer-likeness (handles floats like 123.0)\n",
        "    def is_integer_like(series: pd.Series) -> bool:\n",
        "        # Drop NaNs for type check (null checks are separate)\n",
        "        clean_series = series.dropna()\n",
        "        if clean_series.empty:\n",
        "            return True\n",
        "        # Check if all values are numeric and equal to their floor\n",
        "        is_numeric = pd.to_numeric(clean_series, errors='coerce').notna().all()\n",
        "        if not is_numeric:\n",
        "            return False\n",
        "        # Check if values are effectively integers (e.g. 5.0 == 5)\n",
        "        return np.all(np.mod(clean_series, 1) == 0)\n",
        "\n",
        "    # Validate report_id (must be non-null)\n",
        "    if df['report_id'].isnull().any():\n",
        "        validation_errors.append(\"Column 'report_id' contains null values.\")\n",
        "\n",
        "    # Validate municipality_id (must be non-null, integer-like, 7 digits)\n",
        "    if df['municipality_id'].isnull().any():\n",
        "        validation_errors.append(\"Column 'municipality_id' contains null values.\")\n",
        "    elif not is_integer_like(df['municipality_id']):\n",
        "        validation_errors.append(\"Column 'municipality_id' contains non-integer values.\")\n",
        "\n",
        "    # Validate state (non-null, 2 chars)\n",
        "    if df['state'].isnull().any():\n",
        "        validation_errors.append(\"Column 'state' contains null values.\")\n",
        "    # Check string length for non-nulls (convert to string first to handle potential object types)\n",
        "    state_lengths = df['state'].dropna().astype(str).str.strip().str.len()\n",
        "    if not (state_lengths == 2).all():\n",
        "        validation_errors.append(\"Column 'state' contains values that are not 2 characters long.\")\n",
        "\n",
        "    # Validate lottery (non-null, integer-like, positive)\n",
        "    if df['lottery'].isnull().any():\n",
        "        validation_errors.append(\"Column 'lottery' contains null values.\")\n",
        "    elif not is_integer_like(df['lottery']):\n",
        "        validation_errors.append(\"Column 'lottery' contains non-integer values.\")\n",
        "    elif (df['lottery'].dropna() <= 0).any():\n",
        "        validation_errors.append(\"Column 'lottery' contains non-positive values.\")\n",
        "\n",
        "    # Validate year (non-null, integer-like, range 2003-2011)\n",
        "    if df['year'].isnull().any():\n",
        "        validation_errors.append(\"Column 'year' contains null values.\")\n",
        "    elif not is_integer_like(df['year']):\n",
        "        validation_errors.append(\"Column 'year' contains non-integer values.\")\n",
        "    else:\n",
        "        # Check range\n",
        "        years = df['year'].dropna()\n",
        "        if not ((years >= 2003) & (years <= 2011)).all():\n",
        "             validation_errors.append(\"Column 'year' contains values outside the 2003-2011 range.\")\n",
        "\n",
        "    # Validate report_full_text (non-null)\n",
        "    if df['report_full_text'].isnull().any():\n",
        "        validation_errors.append(\"Column 'report_full_text' contains null values.\")\n",
        "\n",
        "    # Validate report_summary_text (nulls permitted, no check needed for nulls)\n",
        "\n",
        "    # Validate has_summary_detected (non-null, boolean)\n",
        "    if df['has_summary_detected'].isnull().any():\n",
        "        validation_errors.append(\"Column 'has_summary_detected' contains null values.\")\n",
        "    # Check if values are boolean-like (True/False/0/1)\n",
        "    if not df['has_summary_detected'].isin([True, False, 0, 1]).all():\n",
        "         validation_errors.append(\"Column 'has_summary_detected' contains non-boolean values.\")\n",
        "\n",
        "    # Validate text_extraction_mode (non-null, controlled vocabulary)\n",
        "    if df['text_extraction_mode'].isnull().any():\n",
        "        validation_errors.append(\"Column 'text_extraction_mode' contains null values.\")\n",
        "    valid_modes = {'native_text', 'ocr'}\n",
        "    if not df['text_extraction_mode'].isin(valid_modes).all():\n",
        "        validation_errors.append(f\"Column 'text_extraction_mode' contains invalid values. Allowed: {valid_modes}\")\n",
        "\n",
        "    # Validate numeric counts (page_count, image_count, report_lines_count)\n",
        "    for col in ['page_count', 'image_count', 'report_lines_count']:\n",
        "        if df[col].isnull().any():\n",
        "            validation_errors.append(f\"Column '{col}' contains null values.\")\n",
        "        elif not is_integer_like(df[col]):\n",
        "            validation_errors.append(f\"Column '{col}' contains non-integer values.\")\n",
        "        elif (df[col].dropna() < 0).any():\n",
        "            validation_errors.append(f\"Column '{col}' contains negative values.\")\n",
        "\n",
        "    # Validate definitions (non-null, non-empty string, constant)\n",
        "    for col in ['image_count_definition', 'lines_count_definition']:\n",
        "        if df[col].isnull().any():\n",
        "            validation_errors.append(f\"Column '{col}' contains null values.\")\n",
        "        elif (df[col].astype(str).str.strip() == \"\").any():\n",
        "            validation_errors.append(f\"Column '{col}' contains empty strings.\")\n",
        "        # Check constancy (uniqueness of definition)\n",
        "        unique_defs = df[col].dropna().unique()\n",
        "        if len(unique_defs) > 1:\n",
        "            validation_errors.append(f\"Column '{col}' is not constant across the dataset. Found: {unique_defs}\")\n",
        "\n",
        "    # Validate pdf_source (non-null, non-empty)\n",
        "    if df['pdf_source'].isnull().any():\n",
        "        validation_errors.append(\"Column 'pdf_source' contains null values.\")\n",
        "    elif (df['pdf_source'].astype(str).str.strip() == \"\").any():\n",
        "        validation_errors.append(\"Column 'pdf_source' contains empty strings.\")\n",
        "\n",
        "    # Raise error if any validations failed\n",
        "    if validation_errors:\n",
        "        error_message = \"Schema validation failed for df_raw_corpus:\\n\" + \"\\n\".join(validation_errors)\n",
        "        logger.error(error_message)\n",
        "        raise ValueError(error_message)\n",
        "\n",
        "    logger.info(\"Schema validation passed for df_raw_corpus.\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Step 2: Validate key uniqueness and referential consistency\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_corpus_uniqueness(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Validates the uniqueness of primary and composite keys in the raw corpus DataFrame.\n",
        "\n",
        "    This function ensures that `report_id` is globally unique, which is critical for\n",
        "    downstream aggregation. It also checks for duplicate (municipality_id, lottery, year)\n",
        "    tuples, which would indicate data duplication or ambiguous audit records.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The raw corpus DataFrame.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If `report_id` is not unique or if unjustified composite key duplicates exist.\n",
        "    \"\"\"\n",
        "    # 1. Validate global uniqueness of report_id\n",
        "    # Convert to string to handle potential mixed types and ensure strict uniqueness\n",
        "    report_ids = df['report_id'].astype(str)\n",
        "    if not report_ids.is_unique:\n",
        "        duplicates = report_ids[report_ids.duplicated()].unique()\n",
        "        error_msg = f\"Duplicate report_id values found: {duplicates}\"\n",
        "        logger.error(error_msg)\n",
        "        raise ValueError(error_msg)\n",
        "\n",
        "    # 2. Validate composite key uniqueness (municipality_id, lottery, year)\n",
        "    # These define the audit event. Duplicates here imply multiple reports for the same audit event.\n",
        "    composite_cols = ['municipality_id', 'lottery', 'year']\n",
        "\n",
        "    # Check for duplicates\n",
        "    if df.duplicated(subset=composite_cols).any():\n",
        "        duplicate_rows = df[df.duplicated(subset=composite_cols, keep=False)]\n",
        "        # Log the duplicates for inspection\n",
        "        logger.error(f\"Duplicate composite keys found for columns {composite_cols}.\")\n",
        "        logger.error(f\"Duplicate rows sample:\\n{duplicate_rows.head()}\")\n",
        "\n",
        "        # Per instructions: Hard fail unless explicit justification exists (none in schema)\n",
        "        raise ValueError(f\"Duplicate composite keys found in {composite_cols}. This implies data duplication or ambiguous audits.\")\n",
        "\n",
        "    # 3. Cross-validate state vs municipality_id (Conditional)\n",
        "    # Note: Since no external IBGE mapping is provided in the function signature,\n",
        "    # we perform a basic consistency check: same municipality_id should have same state.\n",
        "\n",
        "    # Group by municipality_id and count unique states\n",
        "    state_counts = df.groupby('municipality_id')['state'].nunique()\n",
        "    inconsistent_munis = state_counts[state_counts > 1]\n",
        "\n",
        "    if not inconsistent_munis.empty:\n",
        "        error_msg = f\"Inconsistent 'state' values found for municipality_ids: {inconsistent_munis.index.tolist()}\"\n",
        "        logger.error(error_msg)\n",
        "        # This is a data integrity error (a muni cannot move states)\n",
        "        raise ValueError(error_msg)\n",
        "\n",
        "    logger.info(\"Uniqueness and referential consistency checks passed.\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Step 3: Validate logical consistency between lottery round and summary presence\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_lottery_summary_consistency(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Validates the logical consistency between the lottery round number and the presence of\n",
        "    summary text, enforcing the extraction regime logic.\n",
        "\n",
        "    Rules:\n",
        "    1. If lottery >= 8 AND has_summary_detected is True, report_summary_text should be present.\n",
        "       (Violation is a Warning, as fallback is allowed).\n",
        "    2. If lottery < 8, report_full_text must be non-empty.\n",
        "       (Violation is a Critical Error, as extraction depends on full text).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The raw corpus DataFrame.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If critical consistency rules (Rule 2) are violated.\n",
        "    \"\"\"\n",
        "    # Create a copy to avoid SettingWithCopy warnings on temporary columns\n",
        "    df_check = df.copy()\n",
        "\n",
        "    # Ensure text columns are treated as strings for length checks\n",
        "    df_check['report_summary_text'] = df_check['report_summary_text'].astype(str)\n",
        "    df_check['report_full_text'] = df_check['report_full_text'].astype(str)\n",
        "\n",
        "    # Rule 1: Lottery >= 8 consistency\n",
        "    # Condition: Lottery >= 8 AND Summary Detected\n",
        "    mask_summary_expected = (df_check['lottery'] >= 8) & (df_check['has_summary_detected'] == True)\n",
        "\n",
        "    # Check: Is summary text null or empty/whitespace?\n",
        "    # Note: 'nan' string check handles string conversion of np.nan\n",
        "    mask_summary_missing = (\n",
        "        (df_check['report_summary_text'].str.strip() == \"\") |\n",
        "        (df_check['report_summary_text'].str.lower() == 'nan') |\n",
        "        (df_check['report_summary_text'].str.lower() == 'none')\n",
        "    )\n",
        "\n",
        "    # Identify violations\n",
        "    violations_rule_1 = df_check[mask_summary_expected & mask_summary_missing]\n",
        "\n",
        "    if not violations_rule_1.empty:\n",
        "        count = len(violations_rule_1)\n",
        "        sample_ids = violations_rule_1['report_id'].head(5).tolist()\n",
        "        logger.warning(\n",
        "            f\"Rule 1 Violation: {count} reports with lottery >= 8 and summary detected \"\n",
        "            f\"have missing summary text. Fallback extraction will be required. \"\n",
        "            f\"Sample IDs: {sample_ids}\"\n",
        "        )\n",
        "\n",
        "    # Rule 2: Lottery < 8 consistency\n",
        "    # Condition: Lottery < 8\n",
        "    mask_early_lottery = (df_check['lottery'] < 8)\n",
        "\n",
        "    # Check: Is full text missing/empty?\n",
        "    mask_fulltext_missing = (\n",
        "        (df_check['report_full_text'].str.strip() == \"\") |\n",
        "        (df_check['report_full_text'].str.lower() == 'nan') |\n",
        "        (df_check['report_full_text'].str.lower() == 'none')\n",
        "    )\n",
        "\n",
        "    # Identify violations\n",
        "    violations_rule_2 = df_check[mask_early_lottery & mask_fulltext_missing]\n",
        "\n",
        "    if not violations_rule_2.empty:\n",
        "        count = len(violations_rule_2)\n",
        "        sample_ids = violations_rule_2['report_id'].head(5).tolist()\n",
        "        error_msg = (\n",
        "            f\"Rule 2 Violation: {count} reports with lottery < 8 have missing full text. \"\n",
        "            f\"Extraction is impossible for these records. Sample IDs: {sample_ids}\"\n",
        "        )\n",
        "        logger.error(error_msg)\n",
        "        raise ValueError(error_msg)\n",
        "\n",
        "    logger.info(\"Lottery/Summary logical consistency checks completed.\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_corpus_inputs(df_raw_corpus: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 1: Validates the schema, types, uniqueness, and\n",
        "    logical consistency of the raw corpus DataFrame.\n",
        "\n",
        "    This function executes the validation steps in a strict sequence:\n",
        "    1. Schema and Type Validation: Ensures all 15 required columns exist and contain valid data types.\n",
        "    2. Uniqueness Validation: Ensures report_id is globally unique and composite keys are consistent.\n",
        "    3. Logical Consistency: Checks alignment between lottery rounds and text availability.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_raw_corpus : pd.DataFrame\n",
        "        The raw corpus DataFrame to be validated.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If any critical validation step fails.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 1: Validate schema, types, and uniqueness of df_raw_corpus.\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Schema and Types\n",
        "        validate_corpus_schema(df_raw_corpus)\n",
        "\n",
        "        # Step 2: Uniqueness\n",
        "        validate_corpus_uniqueness(df_raw_corpus)\n",
        "\n",
        "        # Step 3: Logical Consistency\n",
        "        validate_lottery_summary_consistency(df_raw_corpus)\n",
        "\n",
        "        logger.info(\"Task 1 completed successfully: df_raw_corpus is valid.\")\n",
        "\n",
        "    except ValueError as e:\n",
        "        logger.critical(f\"Task 1 Failed: {str(e)}\")\n",
        "        raise e\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"Task 1 Failed with unexpected error: {str(e)}\")\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "D-Wm5UJib4HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2 — Validate schema, types, and join feasibility of df_validation_raw\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 2: Validate schema, types, and join feasibility of df_validation_raw\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Step 1: Verify presence and data types of all required columns\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_validation_schema(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Validates the schema and semantic types of the validation DataFrame.\n",
        "\n",
        "    This function ensures that `df_validation_raw` contains all necessary columns for\n",
        "    reproducing Tables 1, 2, and 3 of the research paper. It enforces type constraints\n",
        "    (e.g., integer-like IDs, numeric covariates) and checks for the existence of\n",
        "    critical identifiers and outcome variables.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The raw validation DataFrame containing human-coded outcomes and covariates.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If required columns are missing or type constraints are violated.\n",
        "    \"\"\"\n",
        "    # Define required columns\n",
        "    # Note: population_log OR population_total is required; handled in logic below\n",
        "    required_base = [\n",
        "        \"municipality_id\", \"state\", \"year\",\n",
        "        \"ff_corruption_count\", \"gt_corruption_count\", \"cgu_severe_count\",\n",
        "        \"literacy_rate\", \"gdp_per_capita\", \"urban_population_share\",\n",
        "        \"distance_to_capital_km\", \"has_radio_fm\", \"covariate_reference_year\"\n",
        "    ]\n",
        "\n",
        "    # 1. Check for missing columns\n",
        "    existing_columns = set(df.columns)\n",
        "    missing_base = set(required_base) - existing_columns\n",
        "\n",
        "    # Handle population requirement (need at least one)\n",
        "    has_pop_log = \"population_log\" in existing_columns\n",
        "    has_pop_total = \"population_total\" in existing_columns\n",
        "\n",
        "    if missing_base:\n",
        "        error_msg = f\"Missing required columns in df_validation_raw: {missing_base}\"\n",
        "        logger.error(error_msg)\n",
        "        raise ValueError(error_msg)\n",
        "\n",
        "    if not (has_pop_log or has_pop_total):\n",
        "        error_msg = \"Missing population data: either 'population_log' or 'population_total' must be present.\"\n",
        "        logger.error(error_msg)\n",
        "        raise ValueError(error_msg)\n",
        "\n",
        "    # 2. Validate semantic types\n",
        "    validation_errors = []\n",
        "\n",
        "    # Helper for integer-likeness (reused concept from Task 1 for modularity)\n",
        "    def is_integer_like(series: pd.Series) -> bool:\n",
        "        clean = series.dropna()\n",
        "        if clean.empty: return True\n",
        "        is_num = pd.to_numeric(clean, errors='coerce').notna().all()\n",
        "        if not is_num: return False\n",
        "        return np.all(np.mod(clean, 1) == 0)\n",
        "\n",
        "    # Validate municipality_id (non-null, integer-like, 7 digits)\n",
        "    if df['municipality_id'].isnull().any():\n",
        "        validation_errors.append(\"Column 'municipality_id' contains null values.\")\n",
        "    elif not is_integer_like(df['municipality_id']):\n",
        "        validation_errors.append(\"Column 'municipality_id' contains non-integer values.\")\n",
        "\n",
        "    # Validate state (non-null, 2 chars)\n",
        "    if df['state'].isnull().any():\n",
        "        validation_errors.append(\"Column 'state' contains null values.\")\n",
        "    else:\n",
        "        state_lens = df['state'].astype(str).str.strip().str.len()\n",
        "        if not (state_lens == 2).all():\n",
        "            validation_errors.append(\"Column 'state' contains values that are not 2 characters long.\")\n",
        "\n",
        "    # Validate year (integer-like if present)\n",
        "    # Note: Year might be null if the row is purely for time-invariant covariates,\n",
        "    # but schema implies it's an audit year key. We check integer-likeness for non-nulls.\n",
        "    if not is_integer_like(df['year']):\n",
        "        validation_errors.append(\"Column 'year' contains non-integer values.\")\n",
        "\n",
        "    # Validate outcomes (float-like, NaNs permitted)\n",
        "    for col in ['ff_corruption_count', 'gt_corruption_count', 'cgu_severe_count']:\n",
        "        # Check if non-null values are numeric\n",
        "        clean = df[col].dropna()\n",
        "        if not pd.to_numeric(clean, errors='coerce').notna().all():\n",
        "            validation_errors.append(f\"Column '{col}' contains non-numeric values.\")\n",
        "\n",
        "    # Validate covariates (float-like)\n",
        "    covariates = [\n",
        "        \"literacy_rate\", \"gdp_per_capita\", \"urban_population_share\",\n",
        "        \"distance_to_capital_km\"\n",
        "    ]\n",
        "    if has_pop_log: covariates.append(\"population_log\")\n",
        "    if has_pop_total: covariates.append(\"population_total\")\n",
        "\n",
        "    for col in covariates:\n",
        "        clean = df[col].dropna()\n",
        "        if not pd.to_numeric(clean, errors='coerce').notna().all():\n",
        "            validation_errors.append(f\"Column '{col}' contains non-numeric values.\")\n",
        "\n",
        "    # Validate has_radio_fm (boolean-like: 0/1/True/False)\n",
        "    clean_radio = df['has_radio_fm'].dropna()\n",
        "    if not clean_radio.isin([0, 1, True, False]).all():\n",
        "        validation_errors.append(\"Column 'has_radio_fm' contains values other than 0/1/True/False.\")\n",
        "\n",
        "    # Validate covariate_reference_year (integer-like, must contain 2000)\n",
        "    if not is_integer_like(df['covariate_reference_year']):\n",
        "        validation_errors.append(\"Column 'covariate_reference_year' contains non-integer values.\")\n",
        "    elif 2000 not in df['covariate_reference_year'].values:\n",
        "        # Warning rather than error? No, Table 3 requires 2000. If not present, we can't reproduce.\n",
        "        validation_errors.append(\"Column 'covariate_reference_year' does not contain the required year 2000.\")\n",
        "\n",
        "    if validation_errors:\n",
        "        error_msg = \"Schema validation failed for df_validation_raw:\\n\" + \"\\n\".join(validation_errors)\n",
        "        logger.error(error_msg)\n",
        "        raise ValueError(error_msg)\n",
        "\n",
        "    logger.info(\"Schema validation passed for df_validation_raw.\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Step 2: Validate join feasibility with corpus table\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_join_feasibility(df_validation: pd.DataFrame, df_corpus: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Validates that the validation DataFrame can be successfully joined with the corpus DataFrame.\n",
        "\n",
        "    This function checks for the existence of a valid join key (canonical municipality_id)\n",
        "    and verifies that the intersection of keys is sufficient for analysis. It also checks\n",
        "    for potential many-to-many join issues by analyzing key uniqueness.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_validation : pd.DataFrame\n",
        "        The validation DataFrame.\n",
        "    df_corpus : pd.DataFrame\n",
        "        The raw corpus DataFrame.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If the intersection of municipality IDs is zero or if join keys are fundamentally incompatible.\n",
        "    \"\"\"\n",
        "    # 1. Canonicalize keys for comparison (temporary copies)\n",
        "    # Ensure 7-digit zero-padded strings\n",
        "    muni_corpus = df_corpus['municipality_id'].dropna().astype(int).astype(str).str.zfill(7)\n",
        "    muni_val = df_validation['municipality_id'].dropna().astype(int).astype(str).str.zfill(7)\n",
        "\n",
        "    # 2. Check Intersection\n",
        "    common_munis = set(muni_corpus) & set(muni_val)\n",
        "    intersection_count = len(common_munis)\n",
        "\n",
        "    if intersection_count == 0:\n",
        "        error_msg = \"Zero intersection between corpus and validation municipality IDs. Check ID formats.\"\n",
        "        logger.error(error_msg)\n",
        "        raise ValueError(error_msg)\n",
        "\n",
        "    logger.info(f\"Found {intersection_count} common municipalities between corpus and validation data.\")\n",
        "\n",
        "    # 3. Check for Many-to-Many Risk\n",
        "    # The corpus naturally has multiple reports per municipality (across years/lotteries).\n",
        "    # The validation data structure determines the join strategy.\n",
        "\n",
        "    # Check uniqueness of municipality_id in validation data\n",
        "    val_muni_is_unique = muni_val.is_unique\n",
        "\n",
        "    # Check uniqueness of (municipality_id, year) in validation data\n",
        "    # (Use original df columns for composite check)\n",
        "    val_composite_is_unique = df_validation.set_index(['municipality_id', 'year']).index.is_unique\n",
        "\n",
        "    if val_muni_is_unique:\n",
        "        logger.info(\"Validation data is unique at municipality level. Safe for Many-to-One join.\")\n",
        "    elif val_composite_is_unique:\n",
        "        logger.info(\"Validation data is unique at (municipality, year) level. Join must use composite key.\")\n",
        "    else:\n",
        "        # If neither is unique, we have duplicates in validation data that could cause row inflation\n",
        "        logger.warning(\n",
        "            \"Validation data is NOT unique at municipality OR (municipality, year) level. \"\n",
        "            \"Risk of Many-to-Many join inflation. Inspect duplicates in df_validation_raw.\"\n",
        "        )\n",
        "        # We don't hard fail here, but we flag it for the join implementation task (Task 11)\n",
        "\n",
        "    # 4. Check coverage for specific outcomes\n",
        "    # How many corpus reports have a match in FF/GT/CGU?\n",
        "    # We simulate a join on municipality_id (most optimistic coverage)\n",
        "\n",
        "    # Filter validation to rows with outcomes\n",
        "    has_ff = df_validation[df_validation['ff_corruption_count'].notna()]['municipality_id'].astype(int).astype(str).str.zfill(7)\n",
        "    has_gt = df_validation[df_validation['gt_corruption_count'].notna()]['municipality_id'].astype(int).astype(str).str.zfill(7)\n",
        "\n",
        "    ff_coverage = len(set(muni_corpus) & set(has_ff))\n",
        "    gt_coverage = len(set(muni_corpus) & set(has_gt))\n",
        "\n",
        "    logger.info(f\"Potential corpus reports with FF data: {ff_coverage} municipalities.\")\n",
        "    logger.info(f\"Potential corpus reports with GT data: {gt_coverage} municipalities.\")\n",
        "\n",
        "    if ff_coverage < 10 or gt_coverage < 10:\n",
        "        logger.warning(\"Very low overlap with FF/GT validation data. Check if datasets cover the same geographic/temporal scope.\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Step 3: Validate covariate completeness for Table 3\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_covariate_completeness(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Validates the completeness of covariates required for Table 3 reproduction.\n",
        "\n",
        "    This function filters the validation data to the reference year 2000 (as specified\n",
        "    in the paper's Table 3 notes) and calculates the missingness rate for each\n",
        "    required covariate.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The validation DataFrame.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If the reference year 2000 is missing from the data.\n",
        "    \"\"\"\n",
        "    # 1. Filter to reference year 2000\n",
        "    df_2000 = df[df['covariate_reference_year'] == 2000]\n",
        "\n",
        "    if df_2000.empty:\n",
        "        error_msg = \"No data found for covariate_reference_year == 2000. Cannot reproduce Table 3.\"\n",
        "        logger.error(error_msg)\n",
        "        raise ValueError(error_msg)\n",
        "\n",
        "    n_samples = len(df_2000)\n",
        "    logger.info(f\"Found {n_samples} observations for Table 3 (Year 2000).\")\n",
        "\n",
        "    # 2. Check missingness for required covariates\n",
        "    covariates = [\n",
        "        \"literacy_rate\", \"gdp_per_capita\", \"urban_population_share\",\n",
        "        \"distance_to_capital_km\", \"has_radio_fm\"\n",
        "    ]\n",
        "    # Add population variable (prefer log, fallback to total)\n",
        "    if \"population_log\" in df.columns:\n",
        "        covariates.append(\"population_log\")\n",
        "    elif \"population_total\" in df.columns:\n",
        "        covariates.append(\"population_total\")\n",
        "\n",
        "    missing_stats = {}\n",
        "    for col in covariates:\n",
        "        n_missing = df_2000[col].isnull().sum()\n",
        "        pct_missing = (n_missing / n_samples) * 100\n",
        "        missing_stats[col] = pct_missing\n",
        "\n",
        "        if pct_missing > 10:\n",
        "            logger.warning(f\"High missingness for Table 3 covariate '{col}': {pct_missing:.2f}%\")\n",
        "\n",
        "    logger.info(f\"Table 3 Covariate Missingness (Year 2000): {missing_stats}\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_validation_inputs(df_validation_raw: pd.DataFrame, df_raw_corpus: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 2: Validates the schema, types, and join feasibility\n",
        "    of the validation DataFrame.\n",
        "\n",
        "    Executes validation steps in sequence:\n",
        "    1. Schema Validation: Checks for required columns and types.\n",
        "    2. Join Feasibility: Verifies overlap with corpus and checks for join risks.\n",
        "    3. Covariate Completeness: Checks data availability for Table 3 (Year 2000).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_validation_raw : pd.DataFrame\n",
        "        The raw validation DataFrame.\n",
        "    df_raw_corpus : pd.DataFrame\n",
        "        The raw corpus DataFrame (for join checking).\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If critical validation steps fail.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 2: Validate schema, types, and join feasibility of df_validation_raw.\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Schema and Types\n",
        "        validate_validation_schema(df_validation_raw)\n",
        "\n",
        "        # Step 2: Join Feasibility\n",
        "        validate_join_feasibility(df_validation_raw, df_raw_corpus)\n",
        "\n",
        "        # Step 3: Covariate Completeness\n",
        "        validate_covariate_completeness(df_validation_raw)\n",
        "\n",
        "        logger.info(\"Task 2 completed successfully: df_validation_raw is valid.\")\n",
        "\n",
        "    except ValueError as e:\n",
        "        logger.critical(f\"Task 2 Failed: {str(e)}\")\n",
        "        raise e\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"Task 2 Failed with unexpected error: {str(e)}\")\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "JhTgPbBkgeSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3 — Validate language, raw_lexicon_list, and study_configuration\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 3: Validate language, raw_lexicon_list, and study_configuration\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Step 1: Validate language parameter\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_language_parameter(language: str) -> str:\n",
        "    \"\"\"\n",
        "    Validates the language parameter against allowed values and enforces replication fidelity.\n",
        "\n",
        "    Ensures the language is either \"English\" or \"Portuguese\". For strict replication of the\n",
        "    Brazilian audit paper, \"Portuguese\" is expected. Emits warnings if \"English\" is selected,\n",
        "    as this implies translation artifacts not present in the original study.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    language : str\n",
        "        The language of the corpus and lexicon.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        The validated, title-cased language string.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If the language is not in the allowed set.\n",
        "    \"\"\"\n",
        "    allowed_languages = {\"English\", \"Portuguese\"}\n",
        "\n",
        "    # Normalize input\n",
        "    lang_normalized = language.strip().title()\n",
        "\n",
        "    if lang_normalized not in allowed_languages:\n",
        "        error_msg = f\"Invalid language '{language}'. Allowed values: {allowed_languages}\"\n",
        "        logger.error(error_msg)\n",
        "        raise ValueError(error_msg)\n",
        "\n",
        "    # Fidelity check\n",
        "    if lang_normalized == \"English\":\n",
        "        logger.warning(\n",
        "            \"Language set to 'English'. The original study uses Portuguese text and dictionaries. \"\n",
        "            \"Results will not match the paper unless the corpus has been perfectly translated.\"\n",
        "        )\n",
        "    else:\n",
        "        logger.info(\"Language set to 'Portuguese', consistent with the original study.\")\n",
        "\n",
        "    return lang_normalized\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Step 2: Validate lexicon structure and content\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_lexicon_content(raw_lexicon_list: List[str], dictionary_config: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Validates the structure and semantic content of the raw lexicon list.\n",
        "\n",
        "    Ensures the lexicon is a non-empty list of strings and contains at least one known\n",
        "    severe irregularity term from the study (Appendix A2). Checks consistency with\n",
        "    external resource configuration.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    raw_lexicon_list : List[str]\n",
        "        The list of raw n-grams/tokens for the dictionary.\n",
        "    dictionary_config : Dict[str, Any]\n",
        "        The dictionary configuration section from study parameters.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If the lexicon is empty, malformed, or lacks critical semantic terms.\n",
        "    \"\"\"\n",
        "    # 1. Structure check\n",
        "    if not isinstance(raw_lexicon_list, list):\n",
        "        raise ValueError(\"raw_lexicon_list must be a list.\")\n",
        "\n",
        "    if not raw_lexicon_list:\n",
        "        raise ValueError(\"raw_lexicon_list is empty.\")\n",
        "\n",
        "    # Check for non-string entries\n",
        "    non_strings = [x for x in raw_lexicon_list if not isinstance(x, str)]\n",
        "    if non_strings:\n",
        "        raise ValueError(f\"raw_lexicon_list contains non-string entries: {non_strings[:5]}...\")\n",
        "\n",
        "    # 2. Content check (Sanity check for known terms)\n",
        "    # Terms from Appendix A2 (stemmed or raw)\n",
        "    known_terms = {\n",
        "        \"empresa fantasma\", \"empresa inexistente\", \"fraud\", \"conluio\", \"fals\",\n",
        "        \"simulacao licitat\", \"montagem licitat\"\n",
        "    }\n",
        "\n",
        "    # Check if any known term (or a substring of it) is present in the list\n",
        "    # We check loosely here because the input list might be stemmed or unstemmed\n",
        "    found_known_term = False\n",
        "    for term in raw_lexicon_list:\n",
        "        term_lower = term.lower()\n",
        "        if any(k in term_lower for k in known_terms):\n",
        "            found_known_term = True\n",
        "            break\n",
        "\n",
        "    if not found_known_term:\n",
        "        logger.warning(\n",
        "            \"raw_lexicon_list does not appear to contain known severe terms from Appendix A2 \"\n",
        "            f\"(e.g., {known_terms}). Ensure the dictionary is correct.\"\n",
        "        )\n",
        "    else:\n",
        "        logger.info(\"raw_lexicon_list contains expected semantic terms.\")\n",
        "\n",
        "    # 3. External resource check\n",
        "    dict_url = dictionary_config.get(\"dictionary_url\")\n",
        "    if dict_url:\n",
        "        logger.info(f\"Using lexicon list provided in memory. Verify consistency with external resource: {dict_url}\")\n",
        "\n",
        "    logger.info(f\"Lexicon validation passed. Size: {len(raw_lexicon_list)} entries.\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Step 3: Validate study_configuration coherence\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_study_config_coherence(config: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Validates the internal coherence of the study configuration dictionary against\n",
        "    the research design specifications.\n",
        "\n",
        "    Enforces critical parameters for replication:\n",
        "    - Parsing: lottery cutoff = 8.\n",
        "    - PCA: eigenvalue threshold = 1.0, exact 5 input features.\n",
        "    - Econometrics: state fixed effects, robust SEs.\n",
        "    - Dictionary: match mode resolution.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    config : Dict[str, Any]\n",
        "        The full study configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        A validated 'run manifest' dictionary containing the active configuration.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If any critical parameter violates the research design.\n",
        "    \"\"\"\n",
        "    pipeline_params = config.get(\"pipeline_parameters\", {})\n",
        "\n",
        "    # 1. Parsing Config\n",
        "    parsing = pipeline_params.get(\"parsing_config\", {})\n",
        "    if parsing.get(\"lottery_cutoff\") != 8:\n",
        "        raise ValueError(f\"Invalid lottery_cutoff: {parsing.get('lottery_cutoff')}. Must be 8 per Section 2.\")\n",
        "\n",
        "    # Check markers\n",
        "    start_marker = parsing.get(\"regex_start_marker\")\n",
        "    if not start_marker or \"Constatação\" not in start_marker:\n",
        "        logger.warning(f\"regex_start_marker '{start_marker}' may not match Appendix A1.3 requirements.\")\n",
        "\n",
        "    # 2. PCA Config\n",
        "    pca = pipeline_params.get(\"pca_config\", {})\n",
        "    if pca.get(\"eigenvalue_threshold\") != 1.0:\n",
        "        raise ValueError(\"PCA eigenvalue_threshold must be 1.0 (Kaiser criterion).\")\n",
        "\n",
        "    required_pca_features = [\n",
        "        \"image_count\", \"severe_irregularities_count\", \"page_count\",\n",
        "        \"report_lines_count\", \"total_irregularities_count\"\n",
        "    ]\n",
        "    configured_features = pca.get(\"pca_input_features\", [])\n",
        "    if configured_features != required_pca_features:\n",
        "        raise ValueError(\n",
        "            f\"PCA input features mismatch.\\nExpected: {required_pca_features}\\nFound: {configured_features}\"\n",
        "        )\n",
        "\n",
        "    # 3. Econometrics Config\n",
        "    metrics = pipeline_params.get(\"econometrics_config\", {})\n",
        "    if metrics.get(\"validation_fixed_effects\") != \"state\":\n",
        "        raise ValueError(\"validation_fixed_effects must be 'state' per Equation (1).\")\n",
        "\n",
        "    if not metrics.get(\"robust_se_type\"):\n",
        "        # Enforce explicit setting\n",
        "        raise ValueError(\"robust_se_type must be explicitly set (e.g., 'HC1').\")\n",
        "\n",
        "    # 4. Dictionary Config\n",
        "    dict_conf = pipeline_params.get(\"dictionary_config\", {})\n",
        "    match_mode = dict_conf.get(\"match_mode\")\n",
        "    if match_mode == \"regex_or_ngram_search\":\n",
        "        # Resolve ambiguity: Default to n-gram search for fidelity unless regex is strictly needed\n",
        "        logger.info(\"Resolving ambiguous match_mode 'regex_or_ngram_search' to 'ngram_search' for primary pipeline.\")\n",
        "        dict_conf[\"resolved_match_mode\"] = \"ngram_search\"\n",
        "    elif match_mode not in [\"ngram_search\", \"regex_search\"]:\n",
        "        raise ValueError(f\"Unknown match_mode: {match_mode}\")\n",
        "\n",
        "    # 5. NLP Config\n",
        "    nlp = pipeline_params.get(\"nlp_config\", {})\n",
        "    if nlp.get(\"stemmer_algorithm\") != \"nltk.stem.PorterStemmer\":\n",
        "        logger.warning(\"Stemmer is not PorterStemmer. This deviates from Appendix A4.\")\n",
        "\n",
        "    logger.info(\"Study configuration coherence check passed.\")\n",
        "\n",
        "    # Return the validated config as the run manifest\n",
        "    return config\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_config_inputs(\n",
        "    language: str,\n",
        "    raw_lexicon_list: List[str],\n",
        "    study_configuration: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 3: Validates the language, lexicon, and study configuration.\n",
        "\n",
        "    Executes validation steps in sequence:\n",
        "    1. Language Validation: Checks against allowed set.\n",
        "    2. Lexicon Validation: Checks structure and semantic content.\n",
        "    3. Configuration Coherence: Enforces research design parameters.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    language : str\n",
        "        The language of the study.\n",
        "    raw_lexicon_list : List[str]\n",
        "        The list of raw dictionary terms.\n",
        "    study_configuration : Dict[str, Any]\n",
        "        The full study configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        The validated study configuration (run manifest).\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If any validation step fails.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 3: Validate language, raw_lexicon_list, and study_configuration.\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Language\n",
        "        validated_lang = validate_language_parameter(language)\n",
        "\n",
        "        # Step 2: Lexicon\n",
        "        dict_config = study_configuration.get(\"pipeline_parameters\", {}).get(\"dictionary_config\", {})\n",
        "        validate_lexicon_content(raw_lexicon_list, dict_config)\n",
        "\n",
        "        # Step 3: Config Coherence\n",
        "        run_manifest = validate_study_config_coherence(study_configuration)\n",
        "\n",
        "        # Inject validated language into manifest for downstream reference\n",
        "        run_manifest[\"validated_language\"] = validated_lang\n",
        "\n",
        "        logger.info(\"Task 3 completed successfully: Configuration is valid.\")\n",
        "        return run_manifest\n",
        "\n",
        "    except ValueError as e:\n",
        "        logger.critical(f\"Task 3 Failed: {str(e)}\")\n",
        "        raise e\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"Task 3 Failed with unexpected error: {str(e)}\")\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "-gcRypg2hZlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4 — Cleanse identifiers and categorical fields in both DataFrames\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 4: Cleanse identifiers and categorical fields in both DataFrames\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Step 1: Normalize state codes and identifiers\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def normalize_identifiers(df: pd.DataFrame, id_col: str = 'municipality_id', state_col: str = 'state') -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Canonicalizes municipality identifiers and state codes.\n",
        "\n",
        "    Creates 'municipality_id_canon' (7-digit zero-padded string) and 'state_canon'\n",
        "    (2-char uppercase string). Preserves original columns.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Input DataFrame (corpus or validation).\n",
        "    id_col : str\n",
        "        Name of the municipality ID column.\n",
        "    state_col : str\n",
        "        Name of the state column.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame with added canonical columns.\n",
        "    \"\"\"\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # 1. Canonicalize Municipality ID\n",
        "    # Handle potential float representation (e.g. 1234567.0) -> int -> str -> zfill\n",
        "    # Coerce to numeric first to handle strings with decimals, then to int (floor), then string\n",
        "\n",
        "    # Helper to safely convert to canonical string\n",
        "    def to_canon_id(series):\n",
        "        # Force numeric, coerce errors to NaN\n",
        "        nums = pd.to_numeric(series, errors='coerce')\n",
        "        # Drop NaNs for formatting, fill with placeholder to avoid error, then mask back\n",
        "        # We use -1 as placeholder for NaN\n",
        "        filled = nums.fillna(-1).astype(int).astype(str)\n",
        "        # Pad with zeros to 7 digits\n",
        "        padded = filled.str.zfill(7)\n",
        "        # Restore NaNs\n",
        "        return padded.where(nums.notna(), other=None)\n",
        "\n",
        "    df_out[f'{id_col}_canon'] = to_canon_id(df_out[id_col])\n",
        "\n",
        "    # 2. Canonicalize State\n",
        "    # Strip whitespace, uppercase\n",
        "    if state_col in df_out.columns:\n",
        "        df_out[f'{state_col}_canon'] = df_out[state_col].astype(str).str.strip().str.upper()\n",
        "        # Handle \"NAN\" or \"NONE\" strings resulting from null conversion\n",
        "        mask_null = df_out[state_col].isnull() | df_out[f'{state_col}_canon'].isin(['NAN', 'NONE', ''])\n",
        "        df_out.loc[mask_null, f'{state_col}_canon'] = None\n",
        "\n",
        "        # Validate length (2 chars)\n",
        "        mask_invalid_len = df_out[f'{state_col}_canon'].str.len() != 2\n",
        "        df_out.loc[mask_invalid_len, f'{state_col}_canon'] = None\n",
        "\n",
        "    return df_out\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Step 2: Normalize categorical fields\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def normalize_categoricals(df_corpus: pd.DataFrame, df_validation: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Normalizes categorical fields to controlled vocabularies.\n",
        "\n",
        "    - text_extraction_mode -> {'native_text', 'ocr'}\n",
        "    - has_summary_detected -> boolean\n",
        "    - has_radio_fm -> {0, 1}\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_corpus : pd.DataFrame\n",
        "        Raw corpus DataFrame.\n",
        "    df_validation : pd.DataFrame\n",
        "        Raw validation DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, pd.DataFrame]\n",
        "        Normalized corpus and validation DataFrames.\n",
        "    \"\"\"\n",
        "    df_c = df_corpus.copy()\n",
        "    df_v = df_validation.copy()\n",
        "\n",
        "    # 1. Normalize text_extraction_mode\n",
        "    # Map various inputs to controlled vocabulary\n",
        "    mode_map = {\n",
        "        'native_text': 'native_text', 'native': 'native_text', 'text': 'native_text',\n",
        "        'ocr': 'ocr', 'scanned': 'ocr', 'image': 'ocr'\n",
        "    }\n",
        "\n",
        "    # Normalize input string first\n",
        "    raw_modes = df_c['text_extraction_mode'].astype(str).str.lower().str.strip()\n",
        "    df_c['text_extraction_mode'] = raw_modes.map(mode_map)\n",
        "\n",
        "    # Log unmapped values\n",
        "    unmapped = df_c[df_c['text_extraction_mode'].isnull()]['text_extraction_mode']\n",
        "    if not unmapped.empty:\n",
        "        logger.warning(f\"Found {len(unmapped)} rows with invalid text_extraction_mode. Setting to None.\")\n",
        "\n",
        "    # 2. Normalize has_summary_detected\n",
        "    # Coerce to boolean\n",
        "    # Handle string 'true'/'false', 1/0\n",
        "    def to_bool(val):\n",
        "        if pd.isna(val): return None\n",
        "        if isinstance(val, bool): return val\n",
        "        s = str(val).lower().strip()\n",
        "        if s in ['true', '1', '1.0', 'yes']: return True\n",
        "        if s in ['false', '0', '0.0', 'no']: return False\n",
        "        return None\n",
        "\n",
        "    df_c['has_summary_detected'] = df_c['has_summary_detected'].apply(to_bool)\n",
        "\n",
        "    # 3. Normalize has_radio_fm\n",
        "    # Coerce to 0/1 integer\n",
        "    def to_int_bool(val):\n",
        "        if pd.isna(val): return None\n",
        "        if isinstance(val, (bool, np.bool_)): return int(val)\n",
        "        try:\n",
        "            f = float(val)\n",
        "            return int(f) if f in [0.0, 1.0] else None\n",
        "        except (ValueError, TypeError):\n",
        "            s = str(val).lower().strip()\n",
        "            if s in ['true', 'yes']: return 1\n",
        "            if s in ['false', 'no']: return 0\n",
        "            return None\n",
        "\n",
        "    df_v['has_radio_fm'] = df_v['has_radio_fm'].apply(to_int_bool)\n",
        "\n",
        "    return df_c, df_v\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Step 3: Handle missing or malformed identifiers\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def quarantine_invalid_rows(df: pd.DataFrame, id_cols: list, context: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Splits DataFrame into clean and quarantined subsets based on identifier validity.\n",
        "\n",
        "    Rows with null values in any of the specified `id_cols` are moved to the quarantine DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Input DataFrame.\n",
        "    id_cols : list\n",
        "        List of column names that must be non-null (e.g., ['report_id', 'municipality_id_canon']).\n",
        "    context : str\n",
        "        Label for logging (e.g., \"Corpus\", \"Validation\").\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, pd.DataFrame]\n",
        "        (df_clean, df_quarantined)\n",
        "    \"\"\"\n",
        "    # Check for nulls in required columns\n",
        "    mask_valid = df[id_cols].notna().all(axis=1)\n",
        "\n",
        "    df_clean = df[mask_valid].copy()\n",
        "    df_quarantined = df[~mask_valid].copy()\n",
        "\n",
        "    if not df_quarantined.empty:\n",
        "        count = len(df_quarantined)\n",
        "        logger.warning(f\"[{context}] Quarantined {count} rows due to missing/invalid identifiers in {id_cols}.\")\n",
        "        # Log sample of issues\n",
        "        sample = df_quarantined[id_cols].head()\n",
        "        logger.info(f\"[{context}] Quarantine sample:\\n{sample}\")\n",
        "\n",
        "        # Add reason column\n",
        "        df_quarantined['quarantine_reason'] = 'Missing identifiers: ' + str(id_cols)\n",
        "\n",
        "    return df_clean, df_quarantined\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def cleanse_identifiers(df_raw_corpus: pd.DataFrame, df_validation_raw: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 4: Cleanse identifiers and categorical fields.\n",
        "\n",
        "    Executes:\n",
        "    1. Identifier Normalization (Canonicalization).\n",
        "    2. Categorical Normalization.\n",
        "    3. Quarantine of rows with invalid identifiers.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_raw_corpus : pd.DataFrame\n",
        "        Raw corpus DataFrame.\n",
        "    df_validation_raw : pd.DataFrame\n",
        "        Raw validation DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]\n",
        "        (df_corpus_clean, df_validation_clean, df_corpus_quarantined, df_validation_quarantined)\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 4: Cleanse identifiers and categorical fields.\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Normalize Identifiers\n",
        "        df_c_norm = normalize_identifiers(df_raw_corpus, id_col='municipality_id', state_col='state')\n",
        "        df_v_norm = normalize_identifiers(df_validation_raw, id_col='municipality_id', state_col='state')\n",
        "\n",
        "        # Step 2: Normalize Categoricals\n",
        "        df_c_cat, df_v_cat = normalize_categoricals(df_c_norm, df_v_norm)\n",
        "\n",
        "        # Step 3: Quarantine\n",
        "        # Corpus requires report_id, municipality_id_canon, state_canon\n",
        "        # Note: state_canon is required for FE later.\n",
        "        corpus_req_cols = ['report_id', 'municipality_id_canon', 'state_canon']\n",
        "        df_c_clean, df_c_quar = quarantine_invalid_rows(df_c_cat, corpus_req_cols, \"Corpus\")\n",
        "\n",
        "        # Validation requires municipality_id_canon, state_canon\n",
        "        val_req_cols = ['municipality_id_canon', 'state_canon']\n",
        "        df_v_clean, df_v_quar = quarantine_invalid_rows(df_v_cat, val_req_cols, \"Validation\")\n",
        "\n",
        "        logger.info(f\"Task 4 completed. Retained {len(df_c_clean)} corpus rows, {len(df_v_clean)} validation rows.\")\n",
        "\n",
        "        return df_c_clean, df_v_clean, df_c_quar, df_v_quar\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"Task 4 Failed with unexpected error: {str(e)}\")\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "npVxS8ydifHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5 — Cleanse text fields without destroying evidence\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 5: Cleanse text fields without destroying evidence\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Step 1: Enforce UTF-8 validity and normalize line endings\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def normalize_text_encoding(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Normalizes text encoding and line endings while preserving original content.\n",
        "\n",
        "    - Creates '_original' copies of text columns.\n",
        "    - Enforces UTF-8 validity (replacing errors with U+FFFD).\n",
        "    - Normalizes line endings (\\r\\n, \\r -> \\n).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Input corpus DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame with normalized text columns and preserved originals.\n",
        "    \"\"\"\n",
        "    df_out = df.copy()\n",
        "    text_cols = ['report_full_text', 'report_summary_text']\n",
        "\n",
        "    for col in text_cols:\n",
        "        if col not in df_out.columns:\n",
        "            continue\n",
        "\n",
        "        # 1. Preserve Evidence\n",
        "        df_out[f'{col}_original'] = df_out[col]\n",
        "\n",
        "        # 2. Enforce UTF-8 and Normalize Line Endings\n",
        "        # We use a helper to apply logic safely to non-null values\n",
        "        def clean_text(series):\n",
        "            # Ensure string type, handle NaNs\n",
        "            # decode/encode cycle to fix bad bytes if strictly needed,\n",
        "            # but pandas usually handles UTF-8.\n",
        "            # Here we focus on line endings and ensuring valid string objects.\n",
        "\n",
        "            # Coerce to string, preserving NaNs\n",
        "            s = series.astype(str).where(series.notna(), None)\n",
        "\n",
        "            # Replace line endings\n",
        "            # Note: regex=True is default for str.replace in newer pandas, but explicit is better\n",
        "            s = s.str.replace(r'\\r\\n', '\\n', regex=True).str.replace(r'\\r', '\\n', regex=True)\n",
        "\n",
        "            # Force UTF-8 validity (in case of byte-like garbage)\n",
        "            # This is implicit in Python 3 strings, but if we had bytes we'd decode.\n",
        "            # Assuming input is already object/string dtype.\n",
        "            return s\n",
        "\n",
        "        df_out[col] = clean_text(df_out[col])\n",
        "\n",
        "    return df_out\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Step 2: Handle null and empty text fields\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_text_content(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Validates text content and quarantines rows with critical text failures.\n",
        "\n",
        "    - Sets 'text_extraction_valid' flag.\n",
        "    - Quarantines rows where 'report_full_text' is empty/null.\n",
        "    - Retains rows with missing summaries (valid for early lotteries).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Normalized corpus DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, pd.DataFrame]\n",
        "        (df_clean, df_quarantined)\n",
        "    \"\"\"\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # 1. Check Full Text Validity\n",
        "    # Valid if not null AND not whitespace-only\n",
        "    # Handle potential non-string types gracefully\n",
        "    full_text = df_out['report_full_text'].astype(str).replace('nan', '')\n",
        "    is_valid = full_text.str.strip().str.len() > 0\n",
        "\n",
        "    df_out['text_extraction_valid'] = is_valid\n",
        "\n",
        "    # 2. Quarantine Invalid Rows\n",
        "    # Critical failure: No full text\n",
        "    mask_critical_failure = ~is_valid\n",
        "\n",
        "    df_clean = df_out[~mask_critical_failure].copy()\n",
        "    df_quarantined = df_out[mask_critical_failure].copy()\n",
        "\n",
        "    if not df_quarantined.empty:\n",
        "        count = len(df_quarantined)\n",
        "        logger.warning(f\"Quarantined {count} rows due to empty/missing 'report_full_text'.\")\n",
        "        df_quarantined['quarantine_reason'] = 'Missing report_full_text'\n",
        "\n",
        "    return df_clean, df_quarantined\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Step 3: Log data quality metrics\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_text_quality_metrics(df_clean: pd.DataFrame, df_quarantined: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Computes data quality metrics for text fields.\n",
        "\n",
        "    Metrics:\n",
        "    - Count of empty full text (from quarantine).\n",
        "    - Count of missing summaries for lottery >= 8.\n",
        "    - Distribution of text_extraction_mode.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_clean : pd.DataFrame\n",
        "        Cleansed corpus DataFrame.\n",
        "    df_quarantined : pd.DataFrame\n",
        "        Quarantined corpus DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        Dictionary of QA metrics.\n",
        "    \"\"\"\n",
        "    metrics = {}\n",
        "\n",
        "    # 1. Empty Full Text Count\n",
        "    metrics['count_empty_full_text'] = len(df_quarantined)\n",
        "\n",
        "    # 2. Missing Summaries (Lottery >= 8)\n",
        "    # Check in clean data\n",
        "    if not df_clean.empty:\n",
        "        mask_late_lottery = df_clean['lottery'] >= 8\n",
        "        # Check for null or empty summary\n",
        "        summary_text = df_clean['report_summary_text'].astype(str).replace('nan', '').replace('None', '')\n",
        "        mask_missing_summary = summary_text.str.strip().str.len() == 0\n",
        "\n",
        "        count_missing_summary_late = len(df_clean[mask_late_lottery & mask_missing_summary])\n",
        "        metrics['count_missing_summary_lottery_ge_8'] = count_missing_summary_late\n",
        "\n",
        "        # 3. Extraction Mode Distribution\n",
        "        mode_dist = df_clean['text_extraction_mode'].value_counts().to_dict()\n",
        "        metrics['text_extraction_mode_distribution'] = mode_dist\n",
        "    else:\n",
        "        metrics['count_missing_summary_lottery_ge_8'] = 0\n",
        "        metrics['text_extraction_mode_distribution'] = {}\n",
        "\n",
        "    logger.info(f\"Text Quality Metrics: {metrics}\")\n",
        "    return metrics\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def cleanse_text_fields(df_raw_corpus: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 5: Cleanse text fields.\n",
        "\n",
        "    Executes:\n",
        "    1. Text Normalization (Encoding, Line Endings).\n",
        "    2. Content Validation & Quarantine.\n",
        "    3. Metric Computation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_raw_corpus : pd.DataFrame\n",
        "        Input corpus DataFrame (from Task 4).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, pd.DataFrame, Dict[str, Any]]\n",
        "        (df_clean, df_quarantined, metrics_dict)\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 5: Cleanse text fields.\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Normalize\n",
        "        df_norm = normalize_text_encoding(df_raw_corpus)\n",
        "\n",
        "        # Step 2: Validate & Quarantine\n",
        "        df_clean, df_quar = validate_text_content(df_norm)\n",
        "\n",
        "        # Step 3: Metrics\n",
        "        metrics = compute_text_quality_metrics(df_clean, df_quar)\n",
        "\n",
        "        logger.info(f\"Task 5 completed. Retained {len(df_clean)} rows.\")\n",
        "        return df_clean, df_quar, metrics\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"Task 5 Failed with unexpected error: {str(e)}\")\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "H86r_6bfjaFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6 — Cleanse numeric metadata fields\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 6: Cleanse numeric metadata fields\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Step 1: Validate and coerce numeric types\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_numeric_types(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Validates and coerces numeric metadata columns to integers.\n",
        "\n",
        "    Enforces:\n",
        "    - Integer-likeness (no fractional parts).\n",
        "    - Non-negativity.\n",
        "    - Non-nullness.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Input corpus DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame with validated integer columns.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If validation fails (non-integer, negative, or null values).\n",
        "    \"\"\"\n",
        "    df_out = df.copy()\n",
        "    numeric_cols = ['page_count', 'image_count', 'report_lines_count']\n",
        "\n",
        "    for col in numeric_cols:\n",
        "        # 1. Check Nulls\n",
        "        if df_out[col].isnull().any():\n",
        "            raise ValueError(f\"Column '{col}' contains null values.\")\n",
        "\n",
        "        # 2. Check Integer-likeness\n",
        "        # Coerce to numeric, raising error on non-numeric strings\n",
        "        series = pd.to_numeric(df_out[col], errors='raise')\n",
        "\n",
        "        # Check for fractional parts\n",
        "        if not np.all(np.mod(series, 1) == 0):\n",
        "            raise ValueError(f\"Column '{col}' contains non-integer values (fractional parts).\")\n",
        "\n",
        "        # 3. Check Non-negativity\n",
        "        if (series < 0).any():\n",
        "            raise ValueError(f\"Column '{col}' contains negative values.\")\n",
        "\n",
        "        # 4. Cast to Integer\n",
        "        df_out[col] = series.astype(int)\n",
        "\n",
        "    return df_out\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Step 2: Validate definition consistency\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_definition_consistency(df: pd.DataFrame) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Validates that counting definitions are constant across the dataset.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Input corpus DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, str]\n",
        "        Dictionary of accepted definitions.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If definitions vary across rows.\n",
        "    \"\"\"\n",
        "    definitions = {}\n",
        "    def_cols = ['image_count_definition', 'lines_count_definition']\n",
        "\n",
        "    for col in def_cols:\n",
        "        # Normalize strings (strip whitespace)\n",
        "        normalized = df[col].astype(str).str.strip()\n",
        "\n",
        "        # Check uniqueness\n",
        "        unique_defs = normalized.unique()\n",
        "\n",
        "        if len(unique_defs) == 0:\n",
        "             raise ValueError(f\"Column '{col}' is empty.\")\n",
        "\n",
        "        if len(unique_defs) > 1:\n",
        "            raise ValueError(f\"Column '{col}' has multiple definitions: {unique_defs}\")\n",
        "\n",
        "        definitions[col] = unique_defs[0]\n",
        "        logger.info(f\"Accepted definition for {col}: {unique_defs[0]}\")\n",
        "\n",
        "    return definitions\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Step 3: Handle outliers and implausible values\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def flag_outliers(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Flags outliers and computes summary statistics for numeric fields.\n",
        "\n",
        "    Flags:\n",
        "    - page_count > 1000\n",
        "    - report_lines_count == 0 (if full text exists)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Validated corpus DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, Dict[str, Any]]\n",
        "        (DataFrame with flag columns, Statistics dictionary)\n",
        "    \"\"\"\n",
        "    df_out = df.copy()\n",
        "    stats = {}\n",
        "\n",
        "    # 1. Compute Summary Statistics\n",
        "    numeric_cols = ['page_count', 'image_count', 'report_lines_count']\n",
        "    for col in numeric_cols:\n",
        "        desc = df_out[col].describe()\n",
        "        stats[col] = {\n",
        "            'min': int(desc['min']),\n",
        "            'median': int(desc['50%']),\n",
        "            'max': int(desc['max']),\n",
        "            'mean': float(desc['mean'])\n",
        "        }\n",
        "\n",
        "    # 2. Flag Page Count Outliers\n",
        "    df_out['flag_page_count_outlier'] = df_out['page_count'] > 1000\n",
        "    if df_out['flag_page_count_outlier'].any():\n",
        "        count = df_out['flag_page_count_outlier'].sum()\n",
        "        logger.warning(f\"Flagged {count} rows with page_count > 1000.\")\n",
        "\n",
        "    # 3. Flag Zero Lines with Text\n",
        "    # Condition: lines == 0 AND text_extraction_valid == True\n",
        "    # Note: text_extraction_valid was created in Task 5\n",
        "    if 'text_extraction_valid' in df_out.columns:\n",
        "        mask_zero_lines = (df_out['report_lines_count'] == 0) & (df_out['text_extraction_valid'])\n",
        "        df_out['flag_zero_lines_with_text'] = mask_zero_lines\n",
        "\n",
        "        if mask_zero_lines.any():\n",
        "            count = mask_zero_lines.sum()\n",
        "            logger.warning(f\"Flagged {count} rows with 0 lines but valid text.\")\n",
        "    else:\n",
        "        logger.warning(\"Skipping zero-line check: 'text_extraction_valid' column missing.\")\n",
        "\n",
        "    return df_out, stats\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def cleanse_numeric_fields(df_raw_corpus: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 6: Cleanse numeric metadata fields.\n",
        "\n",
        "    Executes:\n",
        "    1. Type Validation & Coercion.\n",
        "    2. Definition Consistency Check.\n",
        "    3. Outlier Flagging & Statistics.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_raw_corpus : pd.DataFrame\n",
        "        Input corpus DataFrame (from Task 5).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, Dict[str, Any]]\n",
        "        (df_clean, numeric_stats_dict)\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 6: Cleanse numeric metadata fields.\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Validate Types\n",
        "        df_typed = validate_numeric_types(df_raw_corpus)\n",
        "\n",
        "        # Step 2: Validate Definitions\n",
        "        definitions = validate_definition_consistency(df_typed)\n",
        "\n",
        "        # Step 3: Outliers & Stats\n",
        "        df_clean, stats = flag_outliers(df_typed)\n",
        "\n",
        "        # Merge definitions into stats for the manifest\n",
        "        stats['definitions'] = definitions\n",
        "\n",
        "        logger.info(\"Task 6 completed successfully.\")\n",
        "        return df_clean, stats\n",
        "\n",
        "    except ValueError as e:\n",
        "        logger.critical(f\"Task 6 Failed: {str(e)}\")\n",
        "        raise e\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"Task 6 Failed with unexpected error: {str(e)}\")\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "vFNCZ5gukSgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7 — Extract irregularity text units from each report (parsing algorithm)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 7: Extract irregularity text units from each report (parsing algorithm)\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Step 2: Define sub-procedures for extraction\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def parse_enumerated_summary(text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Parses a summary section into individual irregularity descriptions based on enumeration.\n",
        "\n",
        "    Splits text by patterns like \"1. \", \"1.1 \", \"2.3.1 \".\n",
        "    Trims whitespace and filters empty strings.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        The summary text.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    List[str]\n",
        "        List of extracted irregularity descriptions.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return []\n",
        "\n",
        "    # Regex for enumeration: Start of line, digits, dots, digits..., whitespace\n",
        "    # We use a lookahead or split pattern.\n",
        "    # Splitting by the enumeration pattern is robust.\n",
        "    # Pattern: newline (optional) + whitespace + digits + (dot digits)* + dot/whitespace\n",
        "    pattern = r'(?:^|\\n)\\s*\\d+(?:\\.\\d+)*[.)]?\\s+'\n",
        "\n",
        "    # Split\n",
        "    parts = re.split(pattern, text)\n",
        "\n",
        "    # Filter empty and strip\n",
        "    irregularities = [p.strip() for p in parts if p.strip()]\n",
        "\n",
        "    return irregularities\n",
        "\n",
        "\n",
        "def regex_extract_between_markers(\n",
        "    text: str,\n",
        "    start_marker: str,\n",
        "    end_markers: List[str],\n",
        "    flags: int = re.DOTALL | re.IGNORECASE\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Extracts text segments located between a start marker and any end marker.\n",
        "\n",
        "    Implements: start_marker(.+?)(end_marker_1|end_marker_2|...)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        The full report text.\n",
        "    start_marker : str\n",
        "        The start delimiter (regex safe string).\n",
        "    end_markers : List[str]\n",
        "        List of end delimiters.\n",
        "    flags : int\n",
        "        Regex flags (default DOTALL | IGNORECASE).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    List[str]\n",
        "        List of extracted segments (captured groups).\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return []\n",
        "\n",
        "    # Escape markers to ensure they are treated as literals if they contain special chars\n",
        "    # However, the config might provide regex-ready markers. Assuming literals for safety\n",
        "    # unless they look like regex. The task implies they are phrases.\n",
        "    # We escape them to be safe.\n",
        "    s_esc = re.escape(start_marker)\n",
        "    e_esc = \"|\".join(re.escape(m) for m in end_markers)\n",
        "\n",
        "    # Pattern: Start + (Capture Minimal) + End\n",
        "    pattern = f\"{s_esc}(.+?)(?:{e_esc})\"\n",
        "\n",
        "    matches = re.findall(pattern, text, flags=flags)\n",
        "\n",
        "    return [m.strip() for m in matches if m.strip()]\n",
        "\n",
        "\n",
        "def find_paragraphs_starting_with(text: str, markers: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Identifies paragraphs that begin with any of the specified markers.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        The full report text.\n",
        "    markers : List[str]\n",
        "        List of marker phrases.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    List[str]\n",
        "        List of matching paragraphs.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return []\n",
        "\n",
        "    # Normalize line endings (already done in Task 5, but safe to assume \\n)\n",
        "    # Split into paragraphs (double newline)\n",
        "    paragraphs = re.split(r'\\n\\s*\\n', text)\n",
        "\n",
        "    matching_paragraphs = []\n",
        "    for p in paragraphs:\n",
        "        p_clean = p.strip()\n",
        "        if not p_clean:\n",
        "            continue\n",
        "\n",
        "        # Check if starts with any marker (case insensitive)\n",
        "        for m in markers:\n",
        "            if p_clean.lower().startswith(m.lower()):\n",
        "                matching_paragraphs.append(p_clean)\n",
        "                break\n",
        "\n",
        "    return matching_paragraphs\n",
        "\n",
        "\n",
        "def extract_first_sentence(text: str, terminators: List[str]) -> str:\n",
        "    \"\"\"\n",
        "    Extracts the first sentence from a text block based on terminators.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        The paragraph text.\n",
        "    terminators : List[str]\n",
        "        List of sentence terminator characters (e.g., ['.', ';']).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        The first sentence (including the terminator).\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # Find the earliest occurrence of any terminator\n",
        "    min_idx = len(text)\n",
        "    found = False\n",
        "\n",
        "    for t in terminators:\n",
        "        idx = text.find(t)\n",
        "        if idx != -1 and idx < min_idx:\n",
        "            min_idx = idx\n",
        "            found = True\n",
        "\n",
        "    if found:\n",
        "        # Return up to and including the terminator\n",
        "        return text[:min_idx+1].strip()\n",
        "    else:\n",
        "        # Return full text if no terminator found\n",
        "        return text.strip()\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Step 1: Implement the two-regime extraction logic\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def extract_irregularities_from_row(\n",
        "    row: pd.Series,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[List[str], str]:\n",
        "    \"\"\"\n",
        "    Applies the extraction logic to a single report row.\n",
        "\n",
        "    Logic:\n",
        "    1. If lottery >= cutoff AND summary detected AND summary text exists:\n",
        "       -> Parse Summary.\n",
        "    2. Else:\n",
        "       -> Regex Extract.\n",
        "       -> If Regex yields nothing:\n",
        "          -> Fallback (Paragraph First Sentence).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    row : pd.Series\n",
        "        A row from df_raw_corpus.\n",
        "    config : Dict[str, Any]\n",
        "        Parsing configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[List[str], str]\n",
        "        (List of irregularity strings, Extraction Source label)\n",
        "    \"\"\"\n",
        "    # Unpack config\n",
        "    parsing_conf = config['pipeline_parameters']['parsing_config']\n",
        "    cutoff = parsing_conf['lottery_cutoff']\n",
        "    regex_start = parsing_conf['regex_start_marker']\n",
        "    regex_ends = parsing_conf['regex_end_markers']\n",
        "    para_starts = parsing_conf['paragraph_start_markers']\n",
        "    terminators = parsing_conf['sentence_terminators']\n",
        "\n",
        "    # Row values\n",
        "    lottery = row['lottery']\n",
        "    has_summary = row['has_summary_detected']\n",
        "    summary_text = str(row['report_summary_text']) if pd.notna(row['report_summary_text']) else \"\"\n",
        "    full_text = str(row['report_full_text']) if pd.notna(row['report_full_text']) else \"\"\n",
        "\n",
        "    # Regime 1: Summary Parsing\n",
        "    # Check if summary text is effectively present (not just whitespace/nan)\n",
        "    summary_valid = len(summary_text.strip()) > 0 and summary_text.lower() != 'nan'\n",
        "\n",
        "    if lottery >= cutoff and has_summary and summary_valid:\n",
        "        irregularities = parse_enumerated_summary(summary_text)\n",
        "        if irregularities:\n",
        "            return irregularities, \"summary_parsed\"\n",
        "        # If summary parsing fails (yields 0), fall through to 'summary_parsed_empty'\n",
        "        return [], \"summary_parsed_empty\"\n",
        "\n",
        "    # Regime 2: Full Text Extraction\n",
        "    irregularities = regex_extract_between_markers(full_text, regex_start, regex_ends)\n",
        "\n",
        "    if irregularities:\n",
        "        return irregularities, \"regex_between_markers\"\n",
        "\n",
        "    # Fallback\n",
        "    paragraphs = find_paragraphs_starting_with(full_text, para_starts)\n",
        "    fallback_irregularities = []\n",
        "    for p in paragraphs:\n",
        "        sent = extract_first_sentence(p, terminators)\n",
        "        if sent:\n",
        "            fallback_irregularities.append(sent)\n",
        "\n",
        "    if fallback_irregularities:\n",
        "        return fallback_irregularities, \"fallback_first_sentence\"\n",
        "\n",
        "    return [], \"failed_no_extraction\"\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Step 3: Materialize the irregularity-level table\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def materialize_irregularities(\n",
        "    df_corpus: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies extraction to the entire corpus and materializes the irregularity-level DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_corpus : pd.DataFrame\n",
        "        The cleansed corpus DataFrame.\n",
        "    config : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame with one row per extracted irregularity.\n",
        "        Columns: report_id, municipality_id_canon, lottery, state_canon, year,\n",
        "                 irregularity_index, irregularity_text_raw, extraction_source.\n",
        "    \"\"\"\n",
        "    records = []\n",
        "\n",
        "    for idx, row in df_corpus.iterrows():\n",
        "        irregs, source = extract_irregularities_from_row(row, config)\n",
        "\n",
        "        if not irregs:\n",
        "            # Log warning for empty extraction\n",
        "            logger.warning(f\"Report {row['report_id']}: No irregularities extracted (Source: {source})\")\n",
        "            continue\n",
        "\n",
        "        for i, text in enumerate(irregs):\n",
        "            records.append({\n",
        "                'report_id': row['report_id'],\n",
        "                'municipality_id_canon': row['municipality_id_canon'],\n",
        "                'lottery': row['lottery'],\n",
        "                'state_canon': row['state_canon'],\n",
        "                'year': row['year'],\n",
        "                'irregularity_index': i + 1,\n",
        "                'irregularity_text_raw': text,\n",
        "                'extraction_source': source\n",
        "            })\n",
        "\n",
        "    df_irreg = pd.DataFrame(records)\n",
        "\n",
        "    # Ensure types\n",
        "    if not df_irreg.empty:\n",
        "        df_irreg['irregularity_index'] = df_irreg['irregularity_index'].astype(int)\n",
        "\n",
        "    return df_irreg\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def extract_irregularities(\n",
        "    df_raw_corpus: pd.DataFrame,\n",
        "    study_configuration: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 7: Extract irregularity text units.\n",
        "\n",
        "    Executes:\n",
        "    1. Row-wise extraction using the two-regime algorithm.\n",
        "    2. Materialization of the irregularity-level DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_raw_corpus : pd.DataFrame\n",
        "        Cleansed corpus DataFrame.\n",
        "    study_configuration : Dict[str, Any]\n",
        "        Study configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        The irregularity-level DataFrame.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 7: Extract irregularity text units.\")\n",
        "\n",
        "    try:\n",
        "        # Step 1 & 2 & 3 combined in materialization loop\n",
        "        df_irregularities = materialize_irregularities(df_raw_corpus, study_configuration)\n",
        "\n",
        "        # Validation\n",
        "        n_reports = df_raw_corpus['report_id'].nunique()\n",
        "        n_reports_with_irregs = df_irregularities['report_id'].nunique()\n",
        "        n_irregs = len(df_irregularities)\n",
        "\n",
        "        logger.info(f\"Extraction complete. Found {n_irregs} irregularities across {n_reports_with_irregs} reports.\")\n",
        "        logger.info(f\"Reports with zero extracted irregularities: {n_reports - n_reports_with_irregs}\")\n",
        "\n",
        "        if not df_irregularities.empty:\n",
        "            source_dist = df_irregularities['extraction_source'].value_counts().to_dict()\n",
        "            logger.info(f\"Extraction Source Distribution: {source_dist}\")\n",
        "\n",
        "        return df_irregularities\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"Task 7 Failed with unexpected error: {str(e)}\")\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "nXurO2-dlREy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8 — Normalize text and lexicon into shared matching space\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 8: Normalize text and lexicon into shared matching space\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Step 1: Define the normalization function phi(.) exactly as configured\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def get_normalization_function(config: Dict[str, Any]):\n",
        "    \"\"\"\n",
        "    Factory that returns the normalization function phi(s) configured for the study.\n",
        "\n",
        "    Pipeline:\n",
        "    1. NFD Normalization (Unicode decomposition).\n",
        "    2. Remove Accents (Strip combining marks).\n",
        "    3. Lowercase.\n",
        "    4. Remove Punctuation.\n",
        "    5. Tokenize (Split by whitespace).\n",
        "    6. Stem (PorterStemmer).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    config : Dict[str, Any]\n",
        "        Study configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Callable[[str], List[str]]\n",
        "        Function taking a string and returning a list of normalized stems.\n",
        "    \"\"\"\n",
        "    nlp_config = config['pipeline_parameters']['nlp_config']\n",
        "\n",
        "    # Initialize Stemmer\n",
        "    # Note: Config pins 'nltk.stem.PorterStemmer'.\n",
        "    # We instantiate it once here to avoid overhead.\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    # Compile regex for punctuation removal\n",
        "    # Matches any character that is not a word character (alphanumeric) or whitespace\n",
        "    # This effectively removes punctuation symbols.\n",
        "    punct_re = re.compile(r'[^\\w\\s]')\n",
        "\n",
        "    def phi(text: str) -> List[str]:\n",
        "        if not text:\n",
        "            return []\n",
        "\n",
        "        # 1. NFD Normalization\n",
        "        # Decompose characters (e.g., 'ç' -> 'c' + '¸')\n",
        "        text_nfd = unicodedata.normalize('NFD', str(text))\n",
        "\n",
        "        # 2. Remove Accents\n",
        "        # Filter out non-spacing mark characters (Mn)\n",
        "        text_no_accents = \"\".join(c for c in text_nfd if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "        # 3. Lowercase\n",
        "        text_lower = text_no_accents.lower()\n",
        "\n",
        "        # 4. Remove Punctuation\n",
        "        # Replace punctuation with space to prevent merging words (e.g. \"word1,word2\" -> \"word1 word2\")\n",
        "        # Then collapse whitespace later during tokenization\n",
        "        text_no_punct = punct_re.sub(' ', text_lower)\n",
        "\n",
        "        # 5. Tokenize\n",
        "        # Split by whitespace (handles multiple spaces)\n",
        "        tokens = text_no_punct.split()\n",
        "\n",
        "        # 6. Stem\n",
        "        stems = [stemmer.stem(t) for t in tokens]\n",
        "\n",
        "        return stems\n",
        "\n",
        "    return phi\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Step 2: Normalize the lexicon\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def normalize_lexicon(\n",
        "    raw_lexicon_list: List[str],\n",
        "    phi: Any\n",
        ") -> Tuple[Set[str], Dict[str, List[str]]]:\n",
        "    \"\"\"\n",
        "    Normalizes the raw lexicon into the shared matching space.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    raw_lexicon_list : List[str]\n",
        "        List of raw dictionary terms.\n",
        "    phi : Callable\n",
        "        The normalization function.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[Set[str], Dict[str, List[str]]]\n",
        "        (normalized_lexicon_set, normalization_map)\n",
        "        - normalized_lexicon_set: Set of space-joined normalized stems.\n",
        "        - normalization_map: Mapping from normalized string to list of original raw terms.\n",
        "    \"\"\"\n",
        "    normalized_lexicon = set()\n",
        "    normalization_map = {}\n",
        "\n",
        "    for raw_term in raw_lexicon_list:\n",
        "        # Apply phi to get list of stems\n",
        "        stems = phi(raw_term)\n",
        "\n",
        "        # Join back to string for set storage/matching\n",
        "        # e.g., [\"empres\", \"fantasm\"] -> \"empres fantasm\"\n",
        "        norm_term = \" \".join(stems)\n",
        "\n",
        "        if not norm_term:\n",
        "            continue\n",
        "\n",
        "        normalized_lexicon.add(norm_term)\n",
        "\n",
        "        if norm_term not in normalization_map:\n",
        "            normalization_map[norm_term] = []\n",
        "        normalization_map[norm_term].append(raw_term)\n",
        "\n",
        "    return normalized_lexicon, normalization_map\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Step 3: Normalize each irregularity text\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def normalize_irregularities(\n",
        "    df_irregularities: pd.DataFrame,\n",
        "    phi: Any\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies normalization to the irregularity text in the DataFrame.\n",
        "\n",
        "    Adds column:\n",
        "    - 'irregularity_tokens_normalized': List of normalized stems.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_irregularities : pd.DataFrame\n",
        "        DataFrame containing 'irregularity_text_raw'.\n",
        "    phi : Callable\n",
        "        The normalization function.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame with added normalization column.\n",
        "    \"\"\"\n",
        "    df_out = df_irregularities.copy()\n",
        "\n",
        "    # Apply phi row-wise\n",
        "    # We use a lambda to handle potential non-string types safely, though Task 7 ensures strings.\n",
        "    df_out['irregularity_tokens_normalized'] = df_out['irregularity_text_raw'].apply(lambda x: phi(str(x)) if pd.notna(x) else [])\n",
        "\n",
        "    return df_out\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def normalize_text_and_lexicon(\n",
        "    df_irregularities: pd.DataFrame,\n",
        "    raw_lexicon_list: List[str],\n",
        "    study_configuration: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, Set[str], Dict[str, List[str]]]:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 8: Normalize text and lexicon.\n",
        "\n",
        "    Executes:\n",
        "    1. Instantiation of normalization function phi.\n",
        "    2. Normalization of the lexicon.\n",
        "    3. Normalization of irregularity texts.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_irregularities : pd.DataFrame\n",
        "        Irregularity-level DataFrame.\n",
        "    raw_lexicon_list : List[str]\n",
        "        Raw dictionary terms.\n",
        "    study_configuration : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, Set[str], Dict[str, List[str]]]\n",
        "        (df_irregularities_normalized, normalized_lexicon_set, normalization_map)\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 8: Normalize text and lexicon.\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Define phi\n",
        "        phi = get_normalization_function(study_configuration)\n",
        "\n",
        "        # Step 2: Normalize Lexicon\n",
        "        norm_lexicon, norm_map = normalize_lexicon(raw_lexicon_list, phi)\n",
        "        logger.info(f\"Lexicon normalized. {len(raw_lexicon_list)} raw terms -> {len(norm_lexicon)} unique normalized terms.\")\n",
        "\n",
        "        # Step 3: Normalize Irregularities\n",
        "        df_norm = normalize_irregularities(df_irregularities, phi)\n",
        "        logger.info(f\"Irregularities normalized. Processed {len(df_norm)} rows.\")\n",
        "\n",
        "        return df_norm, norm_lexicon, norm_map\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"Task 8 Failed with unexpected error: {str(e)}\")\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "rz0wHfbJnA3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9 — Classify severe irregularities and compute report-level counts\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 9: Classify severe irregularities and compute report-level counts\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Step 1: Define the severe classification rule (Appendix A2)\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def classify_irregularity(\n",
        "    tokens: List[str],\n",
        "    normalized_lexicon: Set[str],\n",
        "    max_ngram_size: int\n",
        ") -> Tuple[int, List[str]]:\n",
        "    \"\"\"\n",
        "    Classifies a single irregularity as severe if it contains any lexicon n-gram.\n",
        "\n",
        "    Implements the rule: Severe = 1 if exists l in L such that l is a contiguous\n",
        "    subsequence of the irregularity tokens.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    tokens : List[str]\n",
        "        List of normalized tokens for the irregularity.\n",
        "    normalized_lexicon : Set[str]\n",
        "        Set of normalized lexicon entries (space-joined strings).\n",
        "    max_ngram_size : int\n",
        "        Maximum token length of entries in the lexicon.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[int, List[str]]\n",
        "        (is_severe (0 or 1), list of matched lexicon terms)\n",
        "    \"\"\"\n",
        "    if not tokens:\n",
        "        return 0, []\n",
        "\n",
        "    matched_terms = []\n",
        "    is_severe = 0\n",
        "\n",
        "    # Generate n-grams of length 1 to max_ngram_size\n",
        "    # We iterate through positions and lengths\n",
        "    n_tokens = len(tokens)\n",
        "\n",
        "    # Optimization: Iterate through the text once?\n",
        "    # Or generate all candidate n-grams.\n",
        "    # Given max_ngram_size is likely small (e.g. 3-5), generating candidates is fast.\n",
        "    for n in range(1, min(n_tokens, max_ngram_size) + 1):\n",
        "        for i in range(n_tokens - n + 1):\n",
        "            ngram_tokens = tokens[i : i + n]\n",
        "            ngram_str = \" \".join(ngram_tokens)\n",
        "\n",
        "            if ngram_str in normalized_lexicon:\n",
        "                is_severe = 1\n",
        "                matched_terms.append(ngram_str)\n",
        "\n",
        "    # Deduplicate matches\n",
        "    matched_terms = sorted(list(set(matched_terms)))\n",
        "\n",
        "    return is_severe, matched_terms\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Step 2: Apply classification to all irregularities\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def apply_classification(\n",
        "    df_irregularities: pd.DataFrame,\n",
        "    normalized_lexicon: Set[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies the classification rule to the entire irregularities DataFrame.\n",
        "\n",
        "    Adds columns:\n",
        "    - 'is_severe': int (0 or 1)\n",
        "    - 'matched_lexicon_terms': List[str]\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_irregularities : pd.DataFrame\n",
        "        DataFrame with 'irregularity_tokens_normalized'.\n",
        "    normalized_lexicon : Set[str]\n",
        "        Normalized lexicon set.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame with classification columns.\n",
        "    \"\"\"\n",
        "    df_out = df_irregularities.copy()\n",
        "\n",
        "    # Determine max n-gram size from lexicon for optimization\n",
        "    if not normalized_lexicon:\n",
        "        max_ngram_size = 0\n",
        "    else:\n",
        "        max_ngram_size = max(len(term.split()) for term in normalized_lexicon)\n",
        "\n",
        "    logger.info(f\"Max lexicon n-gram size: {max_ngram_size}\")\n",
        "\n",
        "    # Apply row-wise\n",
        "    # We use a list comprehension zip for speed over .apply if possible, but .apply is clearer\n",
        "    results = df_out['irregularity_tokens_normalized'].apply(\n",
        "        lambda tokens: classify_irregularity(tokens, normalized_lexicon, max_ngram_size)\n",
        "    )\n",
        "\n",
        "    # Unpack results\n",
        "    df_out['is_severe'] = [res[0] for res in results]\n",
        "    df_out['matched_lexicon_terms'] = [res[1] for res in results]\n",
        "\n",
        "    return df_out\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Step 3: Aggregate to report-level feature counts\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def aggregate_counts(\n",
        "    df_classified: pd.DataFrame,\n",
        "    df_corpus: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregates irregularity counts to the report level and merges with the corpus.\n",
        "\n",
        "    Computes:\n",
        "    - total_irregularities_count\n",
        "    - severe_irregularities_count\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_classified : pd.DataFrame\n",
        "        Classified irregularities DataFrame.\n",
        "    df_corpus : pd.DataFrame\n",
        "        Base corpus DataFrame (to ensure all reports are present).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Corpus DataFrame with added count columns.\n",
        "    \"\"\"\n",
        "    # 1. Aggregate\n",
        "    # Group by report_id\n",
        "    agg = df_classified.groupby('report_id').agg(\n",
        "        total_irregularities_count=('irregularity_index', 'count'),\n",
        "        severe_irregularities_count=('is_severe', 'sum')\n",
        "    ).reset_index()\n",
        "\n",
        "    # 2. Merge with Corpus\n",
        "    # Left join to keep all reports (even those with 0 irregularities)\n",
        "    df_merged = pd.merge(\n",
        "        df_corpus,\n",
        "        agg,\n",
        "        on='report_id',\n",
        "        how='left',\n",
        "        validate='one_to_one'\n",
        "    )\n",
        "\n",
        "    # 3. Fill NaNs with 0\n",
        "    # Reports with no extracted irregularities will have NaN counts\n",
        "    cols_to_fill = ['total_irregularities_count', 'severe_irregularities_count']\n",
        "    df_merged[cols_to_fill] = df_merged[cols_to_fill].fillna(0).astype(int)\n",
        "\n",
        "    return df_merged\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def classify_severe_and_count(\n",
        "    df_irregularities: pd.DataFrame,\n",
        "    normalized_lexicon: Set[str],\n",
        "    df_raw_corpus: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 9: Classify and Count.\n",
        "\n",
        "    Executes:\n",
        "    1. Classification of each irregularity.\n",
        "    2. Aggregation of counts to report level.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_irregularities : pd.DataFrame\n",
        "        Normalized irregularities.\n",
        "    normalized_lexicon : Set[str]\n",
        "        Normalized lexicon.\n",
        "    df_raw_corpus : pd.DataFrame\n",
        "        Base corpus.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, pd.DataFrame]\n",
        "        (df_classified_irregularities, df_corpus_with_counts)\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 9: Classify severe irregularities and compute counts.\")\n",
        "\n",
        "    try:\n",
        "        # Step 1 & 2: Classify\n",
        "        df_classified = apply_classification(df_irregularities, normalized_lexicon)\n",
        "\n",
        "        # Step 3: Aggregate\n",
        "        df_corpus_counts = aggregate_counts(df_classified, df_raw_corpus)\n",
        "\n",
        "        # Log stats\n",
        "        total_severe = df_classified['is_severe'].sum()\n",
        "        total_irregs = len(df_classified)\n",
        "        pct_severe = (total_severe / total_irregs * 100) if total_irregs > 0 else 0\n",
        "\n",
        "        logger.info(f\"Classification complete. {total_severe}/{total_irregs} ({pct_severe:.2f}%) irregularities classified as severe.\")\n",
        "\n",
        "        return df_classified, df_corpus_counts\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"Task 9 Failed with unexpected error: {str(e)}\")\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "O7Zy4F_GoKZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10 — Construct the PCA input matrix and compute the Corruption Index\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 10: Construct the PCA input matrix and compute the Corruption Index\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Step 1: Build the PCA input matrix X\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def build_pca_matrix(df: pd.DataFrame, config: Dict[str, Any]) -> Tuple[np.ndarray, List[Any]]:\n",
        "    \"\"\"\n",
        "    Constructs the PCA input matrix X from the corpus DataFrame.\n",
        "\n",
        "    Selects the 5 specific features in the exact order required by the study.\n",
        "    Ensures no missing values.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Corpus DataFrame with counts.\n",
        "    config : Dict[str, Any]\n",
        "        Study configuration containing 'pca_input_features'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[np.ndarray, List[Any]]\n",
        "        (X matrix (n_samples, 5), list of report_ids corresponding to rows)\n",
        "    \"\"\"\n",
        "    # Get feature list from config\n",
        "    features = config['pipeline_parameters']['pca_config']['pca_input_features']\n",
        "\n",
        "    # Validate columns exist\n",
        "    missing = [f for f in features if f not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing PCA input features in DataFrame: {missing}\")\n",
        "\n",
        "    # Extract data\n",
        "    # Ensure sorting by report_id for deterministic row order\n",
        "    df_sorted = df.sort_values('report_id')\n",
        "\n",
        "    # Check for nulls\n",
        "    if df_sorted[features].isnull().any().any():\n",
        "        raise ValueError(\"PCA input matrix contains null values. Imputation is not permitted.\")\n",
        "\n",
        "    X = df_sorted[features].values.astype(float)\n",
        "    report_ids = df_sorted['report_id'].tolist()\n",
        "\n",
        "    logger.info(f\"PCA Matrix X constructed. Shape: {X.shape}. Features: {features}\")\n",
        "\n",
        "    return X, report_ids\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Step 2: Standardize features\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def standardize_matrix(X: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Standardizes the input matrix X (Z-score normalization).\n",
        "\n",
        "    Computes column-wise mean and sample standard deviation (ddof=1).\n",
        "    Z = (X - mu) / sigma\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : np.ndarray\n",
        "        Input matrix (n_samples, n_features).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[np.ndarray, np.ndarray, np.ndarray]\n",
        "        (Z matrix, mu vector, sigma vector)\n",
        "    \"\"\"\n",
        "    # Compute mean and sample std (ddof=1)\n",
        "    mu = np.mean(X, axis=0)\n",
        "    sigma = np.std(X, axis=0, ddof=1)\n",
        "\n",
        "    # Check for zero variance\n",
        "    if np.any(sigma == 0):\n",
        "        zero_var_indices = np.where(sigma == 0)[0]\n",
        "        raise ValueError(f\"PCA features at indices {zero_var_indices} have zero variance. Standardization undefined.\")\n",
        "\n",
        "    # Standardize\n",
        "    Z = (X - mu) / sigma\n",
        "\n",
        "    return Z, mu, sigma\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Step 3: Fit PCA, apply Kaiser criterion, and score the index\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_pca(Z: np.ndarray, report_ids: List[Any]) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Performs PCA on the standardized matrix Z.\n",
        "\n",
        "    Computes covariance matrix, eigendecomposition, and PC1 scores.\n",
        "    Enforces Kaiser criterion check and sign convention.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    Z : np.ndarray\n",
        "        Standardized input matrix.\n",
        "    report_ids : List[Any]\n",
        "        List of report_ids corresponding to rows of Z.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, Dict[str, Any]]\n",
        "        (DataFrame with 'report_id' and 'corruption_index', PCA artifacts dict)\n",
        "    \"\"\"\n",
        "    n_samples = Z.shape[0]\n",
        "\n",
        "    # 1. Covariance Matrix\n",
        "    # Sigma = (1 / (n-1)) * Z.T @ Z\n",
        "    Sigma = (1 / (n_samples - 1)) * np.dot(Z.T, Z)\n",
        "\n",
        "    # 2. Eigendecomposition\n",
        "    # eigh for symmetric matrices returns eigenvalues in ascending order\n",
        "    eigenvalues, eigenvectors = np.linalg.eigh(Sigma)\n",
        "\n",
        "    # Sort descending\n",
        "    idx = np.argsort(eigenvalues)[::-1]\n",
        "    eigenvalues = eigenvalues[idx]\n",
        "    eigenvectors = eigenvectors[:, idx]\n",
        "\n",
        "    # 3. Kaiser Criterion & Explained Variance\n",
        "    # Check PC1 eigenvalue\n",
        "    pc1_eigenvalue = eigenvalues[0]\n",
        "    if pc1_eigenvalue <= 1.0:\n",
        "        logger.warning(f\"PC1 Eigenvalue {pc1_eigenvalue:.4f} <= 1.0. Kaiser criterion not strictly met.\")\n",
        "\n",
        "    # Check if PC2 > 1 (Paper says only PC1 > 1)\n",
        "    if len(eigenvalues) > 1 and eigenvalues[1] > 1.0:\n",
        "        logger.warning(f\"PC2 Eigenvalue {eigenvalues[1]:.4f} > 1.0. Deviation from paper findings.\")\n",
        "\n",
        "    explained_variance_ratio = eigenvalues / np.sum(eigenvalues)\n",
        "    pc1_var_ratio = explained_variance_ratio[0]\n",
        "    logger.info(f\"PC1 Explained Variance: {pc1_var_ratio:.4f}\")\n",
        "\n",
        "    # 4. PC1 Loadings & Sign Convention\n",
        "    pc1_loadings = eigenvectors[:, 0]\n",
        "\n",
        "    # Paper says \"All variables load positively\".\n",
        "    # If sum of loadings is negative, flip sign to align with \"Corruption\" direction.\n",
        "    # (Or if majority are negative). Sum is robust.\n",
        "    if np.sum(pc1_loadings) < 0:\n",
        "        logger.info(\"Flipping PC1 sign to enforce positive loading convention.\")\n",
        "        pc1_loadings = -pc1_loadings\n",
        "\n",
        "    # 5. Compute Scores\n",
        "    # Score = Z @ loadings\n",
        "    scores = np.dot(Z, pc1_loadings)\n",
        "\n",
        "    # 6. Package Results\n",
        "    df_scores = pd.DataFrame({\n",
        "        'report_id': report_ids,\n",
        "        'corruption_index': scores\n",
        "    })\n",
        "\n",
        "    artifacts = {\n",
        "        'eigenvalues': eigenvalues.tolist(),\n",
        "        'explained_variance_ratio': explained_variance_ratio.tolist(),\n",
        "        'pc1_loadings': pc1_loadings.tolist(),\n",
        "        'covariance_matrix': Sigma.tolist()\n",
        "    }\n",
        "\n",
        "    return df_scores, artifacts\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def build_pca_index(\n",
        "    df_corpus_with_counts: pd.DataFrame,\n",
        "    study_configuration: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 10: Build PCA Index.\n",
        "\n",
        "    Executes:\n",
        "    1. Matrix Construction.\n",
        "    2. Standardization.\n",
        "    3. PCA Computation.\n",
        "    4. Merging index back to corpus.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_corpus_with_counts : pd.DataFrame\n",
        "        Corpus DataFrame with feature counts.\n",
        "    study_configuration : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, Dict[str, Any]]\n",
        "        (Corpus DataFrame with 'corruption_index', PCA artifacts)\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 10: Construct PCA input matrix and compute Corruption Index.\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Build Matrix\n",
        "        X, report_ids = build_pca_matrix(df_corpus_with_counts, study_configuration)\n",
        "\n",
        "        # Step 2: Standardize\n",
        "        Z, mu, sigma = standardize_matrix(X)\n",
        "\n",
        "        # Step 3: Compute PCA\n",
        "        df_scores, artifacts = compute_pca(Z, report_ids)\n",
        "\n",
        "        # Add standardization params to artifacts\n",
        "        artifacts['mu'] = mu.tolist()\n",
        "        artifacts['sigma'] = sigma.tolist()\n",
        "\n",
        "        # Step 4: Merge back\n",
        "        # Left join to original df (though ids should match exactly if sorted)\n",
        "        df_final = pd.merge(\n",
        "            df_corpus_with_counts,\n",
        "            df_scores,\n",
        "            on='report_id',\n",
        "            how='left',\n",
        "            validate='one_to_one'\n",
        "        )\n",
        "\n",
        "        logger.info(\"Task 10 completed. Corruption Index computed.\")\n",
        "        return df_final, artifacts\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"Task 10 Failed with unexpected error: {str(e)}\")\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "AfLb58YNpJVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11 — Join index to validation data and construct agreement samples\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 11: Join index to validation data and construct agreement samples\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Step 1: Merge corruption index with validation data\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def merge_corpus_validation(\n",
        "    df_corpus: pd.DataFrame,\n",
        "    df_validation: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Merges the corpus (with Corruption Index) and validation DataFrames.\n",
        "\n",
        "    Prioritizes joining on ['municipality_id_canon', 'year'] if possible,\n",
        "    otherwise falls back to ['municipality_id_canon'].\n",
        "    Enforces that the merge does not inflate the corpus row count (Left Join).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_corpus : pd.DataFrame\n",
        "        Corpus DataFrame with 'corruption_index'.\n",
        "    df_validation : pd.DataFrame\n",
        "        Validation DataFrame with FF/GT/CGU data.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Merged DataFrame.\n",
        "    \"\"\"\n",
        "    # Determine join keys\n",
        "    # Check if validation is unique on (muni, year)\n",
        "    val_muni_year_unique = not df_validation.duplicated(subset=['municipality_id_canon', 'year']).any()\n",
        "    val_muni_unique = not df_validation.duplicated(subset=['municipality_id_canon']).any()\n",
        "\n",
        "    # Corpus always has (muni, year)\n",
        "    if val_muni_year_unique and 'year' in df_validation.columns:\n",
        "        join_keys = ['municipality_id_canon', 'year']\n",
        "        logger.info(\"Joining on ['municipality_id_canon', 'year']\")\n",
        "    elif val_muni_unique:\n",
        "        join_keys = ['municipality_id_canon']\n",
        "        logger.info(\"Joining on ['municipality_id_canon']\")\n",
        "    else:\n",
        "        # Fallback: Try muni+year even if duplicates exist? No, that risks inflation.\n",
        "        # If validation has duplicates on join keys, we must deduplicate or fail.\n",
        "        # Assuming validation data is well-formed (Task 2 checked this).\n",
        "        # We will use muni+year as the most specific key.\n",
        "        join_keys = ['municipality_id_canon', 'year']\n",
        "        logger.warning(\"Validation data not unique on join keys. Risk of row inflation.\")\n",
        "\n",
        "    # Perform Merge\n",
        "    # Suffixes: _corpus is preserved, _val for validation columns\n",
        "    df_merged = pd.merge(\n",
        "        df_corpus,\n",
        "        df_validation,\n",
        "        on=join_keys,\n",
        "        how='left',\n",
        "        suffixes=('', '_val'),\n",
        "        validate='many_to_one' if val_muni_year_unique or val_muni_unique else None\n",
        "    )\n",
        "\n",
        "    # Check for row inflation\n",
        "    if len(df_merged) != len(df_corpus):\n",
        "        logger.error(f\"Merge inflated rows! Corpus: {len(df_corpus)}, Merged: {len(df_merged)}\")\n",
        "        # In a strict pipeline, we might raise Error. Here we log critical.\n",
        "\n",
        "    # Check State Consistency\n",
        "    # state_canon from corpus vs state_canon_val\n",
        "    if 'state_canon_val' in df_merged.columns:\n",
        "        mask_mismatch = (\n",
        "            df_merged['state_canon'].notna() &\n",
        "            df_merged['state_canon_val'].notna() &\n",
        "            (df_merged['state_canon'] != df_merged['state_canon_val'])\n",
        "        )\n",
        "        if mask_mismatch.any():\n",
        "            n_mismatch = mask_mismatch.sum()\n",
        "            logger.warning(f\"State mismatch between corpus and validation for {n_mismatch} rows.\")\n",
        "\n",
        "    return df_merged\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Step 2: Construct the strict and near agreement samples\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def construct_agreement_flags(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Constructs boolean flags for strict and near agreement samples.\n",
        "\n",
        "    Strict: FF == GT\n",
        "    Near: |FF - GT| <= 1\n",
        "    (Both must be non-null)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Merged DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame with 'is_strict_agreement' and 'is_near_agreement' columns.\n",
        "    \"\"\"\n",
        "    df_out = df.copy()\n",
        "\n",
        "    ff = df_out['ff_corruption_count']\n",
        "    gt = df_out['gt_corruption_count']\n",
        "\n",
        "    # Ensure numeric\n",
        "    ff = pd.to_numeric(ff, errors='coerce')\n",
        "    gt = pd.to_numeric(gt, errors='coerce')\n",
        "\n",
        "    mask_valid = ff.notna() & gt.notna()\n",
        "\n",
        "    # Strict\n",
        "    df_out['is_strict_agreement'] = False\n",
        "    df_out.loc[mask_valid, 'is_strict_agreement'] = (ff[mask_valid] == gt[mask_valid])\n",
        "\n",
        "    # Near\n",
        "    df_out['is_near_agreement'] = False\n",
        "    df_out.loc[mask_valid, 'is_near_agreement'] = (np.abs(ff[mask_valid] - gt[mask_valid]) <= 1)\n",
        "\n",
        "    # Log counts\n",
        "    n_strict = df_out['is_strict_agreement'].sum()\n",
        "    n_near = df_out['is_near_agreement'].sum()\n",
        "    logger.info(f\"Agreement Samples: Strict N={n_strict}, Near N={n_near}\")\n",
        "\n",
        "    return df_out\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Step 3: Compute the mean human-coded outcome for agreement samples\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_hc_mean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes the mean of FF and GT corruption counts.\n",
        "\n",
        "    hc_mean = (FF + GT) / 2\n",
        "    Only computed where both are non-null.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        DataFrame with FF and GT counts.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame with 'hc_mean' column.\n",
        "    \"\"\"\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # Compute the mean of FF and GT corruption counts\n",
        "    ff = pd.to_numeric(df_out['ff_corruption_count'], errors='coerce')\n",
        "    gt = pd.to_numeric(df_out['gt_corruption_count'], errors='coerce')\n",
        "\n",
        "    df_out['hc_mean'] = (ff + gt) / 2.0\n",
        "\n",
        "    return df_out\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def prepare_validation_samples(\n",
        "    df_corpus_with_index: pd.DataFrame,\n",
        "    df_validation_raw: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 11: Prepare validation samples.\n",
        "\n",
        "    Executes:\n",
        "    1. Merge corpus and validation data.\n",
        "    2. Construct agreement flags.\n",
        "    3. Compute mean human-coded outcome.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_corpus_with_index : pd.DataFrame\n",
        "        Corpus DataFrame with Corruption Index.\n",
        "    df_validation_raw : pd.DataFrame\n",
        "        Validation DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Analysis DataFrame ready for regression.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 11: Prepare validation samples.\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Merge\n",
        "        df_merged = merge_corpus_validation(df_corpus_with_index, df_validation_raw)\n",
        "\n",
        "        # Step 2: Agreement Flags\n",
        "        df_flags = construct_agreement_flags(df_merged)\n",
        "\n",
        "        # Step 3: HC Mean\n",
        "        df_analysis = compute_hc_mean(df_flags)\n",
        "\n",
        "        logger.info(\"Task 11 completed. Analysis DataFrame prepared.\")\n",
        "        return df_analysis\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"Task 11 Failed with unexpected error: {str(e)}\")\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "vL7H2COoqJpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12 — Reproduce Table 1: validation regressions against FF/GT\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 12: Reproduce Table 1: validation regressions against FF/GT\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Step 1: Standardize the Corruption Index for interpretability\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def standardize_series(series: pd.Series) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Standardizes a pandas Series (Z-score) using sample standard deviation (ddof=1).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    series : pd.Series\n",
        "        Input numeric series.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        Standardized series (mean ~0, std ~1).\n",
        "    \"\"\"\n",
        "    if series.dropna().empty:\n",
        "        return series\n",
        "\n",
        "    # Standardize a pandas Series (Z-score) using sample standard deviation (ddof=1)\n",
        "    mu = series.mean()\n",
        "    sigma = series.std(ddof=1)\n",
        "\n",
        "    if sigma == 0:\n",
        "        logger.warning(\"Standardization skipped: Zero variance.\")\n",
        "        return series - mu\n",
        "\n",
        "    return (series - mu) / sigma\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Step 2: Estimate Equation (1) with state fixed effects and robust SE\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def estimate_validation_model(\n",
        "    df: pd.DataFrame,\n",
        "    outcome_col: str,\n",
        "    predictor_col: str,\n",
        "    fe_col: str,\n",
        "    robust_type: str = 'HC1'\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Estimates the validation regression model: Outcome ~ Predictor + FE.\n",
        "\n",
        "    Standardizes the predictor within the estimation sample.\n",
        "    Uses HC1 robust standard errors.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Estimation sample DataFrame.\n",
        "    outcome_col : str\n",
        "        Name of outcome variable.\n",
        "    predictor_col : str\n",
        "        Name of predictor variable (Corruption Index).\n",
        "    fe_col : str\n",
        "        Name of fixed effect variable (State).\n",
        "    robust_type : str\n",
        "        Type of robust covariance (default 'HC1').\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        Dictionary containing regression results and diagnostics.\n",
        "    \"\"\"\n",
        "    # 1. Prepare Data\n",
        "    # Drop missing\n",
        "    cols = [outcome_col, predictor_col, fe_col]\n",
        "    data = df[cols].dropna().copy()\n",
        "\n",
        "    if data.empty:\n",
        "        return {'N': 0, 'error': 'Empty sample'}\n",
        "\n",
        "    # 2. Standardize Predictor (Sample-Specific)\n",
        "    data['X_std'] = standardize_series(data[predictor_col])\n",
        "\n",
        "    # 3. Define Formula\n",
        "    # Outcome ~ X_std + C(State)\n",
        "    # We rely on patsy to handle the dummy encoding (LSDV)\n",
        "    formula = f\"{outcome_col} ~ X_std + C({fe_col})\"\n",
        "\n",
        "    # 4. Fit Model\n",
        "    try:\n",
        "        model = smf.ols(formula, data=data)\n",
        "        results = model.fit(cov_type=robust_type)\n",
        "\n",
        "        # 5. Extract Metrics\n",
        "        # Predictor is 'X_std'\n",
        "        beta = results.params['X_std']\n",
        "        se = results.bse['X_std']\n",
        "        t_stat = results.tvalues['X_std']\n",
        "        p_val = results.pvalues['X_std']\n",
        "        r2 = results.rsquared\n",
        "        n_obs = int(results.nobs)\n",
        "\n",
        "        return {\n",
        "            'beta': beta,\n",
        "            'se': se,\n",
        "            't_stat': t_stat,\n",
        "            'p_value': p_val,\n",
        "            'R2': r2,\n",
        "            'N': n_obs,\n",
        "            'sample_ids': df.index.tolist() # Assuming index is report_id or unique\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Regression failed: {str(e)}\")\n",
        "        return {'N': 0, 'error': str(e)}\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Step 3: Record and validate results against paper targets\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_results(results: Dict[str, Any], targets: Dict[str, Any], spec_name: str) -> None:\n",
        "    \"\"\"\n",
        "    Validates regression results against paper targets.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    results : Dict[str, Any]\n",
        "        Computed regression results.\n",
        "    targets : Dict[str, Any]\n",
        "        Target values (N, R2, Beta).\n",
        "    spec_name : str\n",
        "        Name of the specification for logging.\n",
        "    \"\"\"\n",
        "    if 'error' in results:\n",
        "        logger.warning(f\"[{spec_name}] Validation skipped due to error.\")\n",
        "        return\n",
        "\n",
        "    # Check N\n",
        "    if 'N' in targets:\n",
        "        diff_n = results['N'] - targets['N']\n",
        "        if diff_n != 0:\n",
        "            logger.warning(f\"[{spec_name}] Sample size mismatch. Got {results['N']}, Expected {targets['N']}.\")\n",
        "\n",
        "    # Check R2\n",
        "    if 'R2' in targets:\n",
        "        diff_r2 = abs(results['R2'] - targets['R2'])\n",
        "        if diff_r2 > 0.05:\n",
        "             logger.warning(f\"[{spec_name}] R2 deviation > 0.05. Got {results['R2']:.3f}, Expected {targets['R2']:.3f}.\")\n",
        "        else:\n",
        "             logger.info(f\"[{spec_name}] R2 match ({results['R2']:.3f} vs {targets['R2']:.3f}).\")\n",
        "\n",
        "    # Check Beta\n",
        "    if 'Beta' in targets:\n",
        "        diff_beta = abs(results['beta'] - targets['Beta'])\n",
        "        if diff_beta > 0.2: # Allow some variance due to standardization/sample diffs\n",
        "             logger.warning(f\"[{spec_name}] Beta deviation > 0.2. Got {results['beta']:.3f}, Expected {targets['Beta']:.3f}.\")\n",
        "        else:\n",
        "             logger.info(f\"[{spec_name}] Beta match ({results['beta']:.3f} vs {targets['Beta']:.3f}).\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_table1_regressions(\n",
        "    df_analysis: pd.DataFrame,\n",
        "    study_configuration: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 12: Reproduce Table 1.\n",
        "\n",
        "    Runs 4 specifications:\n",
        "    1. Strict Agreement (FF=GT)\n",
        "    2. Near Agreement (|FF-GT|<=1)\n",
        "    3. Full FF\n",
        "    4. Full GT\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_analysis : pd.DataFrame\n",
        "        Analysis DataFrame with agreement flags and outcomes.\n",
        "    study_configuration : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        Dictionary of results for all specifications.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 12: Reproduce Table 1 regressions.\")\n",
        "\n",
        "    fe_col = 'state_canon' # Assuming this is the column name for state\n",
        "\n",
        "    # Ensure state column exists, fallback to 'state' if 'state_canon' missing\n",
        "    if fe_col not in df_analysis.columns and 'state' in df_analysis.columns:\n",
        "        fe_col = 'state'\n",
        "\n",
        "    specs = {\n",
        "        'Strict Agreement': {\n",
        "            'mask': df_analysis['is_strict_agreement'] == True,\n",
        "            'outcome': 'hc_mean',\n",
        "            'targets': {'N': 34, 'R2': 0.726, 'Beta': 1.492}\n",
        "        },\n",
        "        'Near Agreement': {\n",
        "            'mask': df_analysis['is_near_agreement'] == True,\n",
        "            'outcome': 'hc_mean',\n",
        "            'targets': {'N': 99, 'R2': 0.714, 'Beta': 1.276}\n",
        "        },\n",
        "        'Full FF': {\n",
        "            'mask': df_analysis['ff_corruption_count'].notna(),\n",
        "            'outcome': 'ff_corruption_count',\n",
        "            'targets': {'R2': 0.382, 'Beta': 0.628} # From Table 1 Col 3\n",
        "        },\n",
        "        'Full GT': {\n",
        "            'mask': df_analysis['gt_corruption_count'].notna(),\n",
        "            'outcome': 'gt_corruption_count',\n",
        "            'targets': {'R2': 0.488, 'Beta': 2.120} # From Table 1 Col 4\n",
        "        }\n",
        "    }\n",
        "\n",
        "    all_results = {}\n",
        "\n",
        "    for name, spec in specs.items():\n",
        "        logger.info(f\"Running Spec: {name}\")\n",
        "\n",
        "        # Filter Data\n",
        "        df_spec = df_analysis[spec['mask']].copy()\n",
        "\n",
        "        # Run Regression\n",
        "        res = estimate_validation_model(\n",
        "            df_spec,\n",
        "            outcome_col=spec['outcome'],\n",
        "            predictor_col='corruption_index',\n",
        "            fe_col=fe_col\n",
        "        )\n",
        "\n",
        "        # Validate\n",
        "        validate_results(res, spec['targets'], name)\n",
        "\n",
        "        all_results[name] = res\n",
        "\n",
        "    return all_results\n"
      ],
      "metadata": {
        "id": "cLzCPXHqrRsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13 — Reproduce Table 2: validation regressions against CGU\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 13: Reproduce Table 2: validation regressions against CGU\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Step 1: Construct the CGU estimation sample\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def prepare_cgu_samples(df: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Constructs estimation samples for CGU levels and log specifications.\n",
        "\n",
        "    Levels sample: Rows with non-null CGU counts.\n",
        "    Log sample: Rows with positive CGU counts (drops zeros).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Analysis DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, pd.DataFrame]\n",
        "        Dictionary with 'levels' and 'log' DataFrames.\n",
        "    \"\"\"\n",
        "    # Base: Non-null CGU\n",
        "    mask_valid = df['cgu_severe_count'].notna()\n",
        "    df_levels = df[mask_valid].copy()\n",
        "\n",
        "    # Log: Positive CGU\n",
        "    # Ensure numeric\n",
        "    cgu_vals = pd.to_numeric(df_levels['cgu_severe_count'], errors='coerce')\n",
        "    mask_pos = cgu_vals > 0\n",
        "    df_log = df_levels[mask_pos].copy()\n",
        "\n",
        "    n_dropped = len(df_levels) - len(df_log)\n",
        "    if n_dropped > 0:\n",
        "        logger.info(f\"Dropped {n_dropped} non-positive CGU observations for log specification.\")\n",
        "\n",
        "    return {'levels': df_levels, 'log': df_log}\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Step 2: Estimate the levels specification\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# We reuse estimate_validation_model from Task 12 (available in environment)\n",
        "# No new code needed for estimation logic, just orchestration.\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Step 3: Estimate the log specification\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def prepare_log_outcome(df: pd.DataFrame, col_name: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Creates the log-transformed outcome variable.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Input DataFrame (already filtered for positivity).\n",
        "    col_name : str\n",
        "        Name of the column to log-transform.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame with new 'log_{col_name}' column.\n",
        "    \"\"\"\n",
        "    df_out = df.copy()\n",
        "    df_out[f'log_{col_name}'] = np.log(df_out[col_name].astype(float))\n",
        "    return df_out\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_table2_regressions(\n",
        "    df_analysis: pd.DataFrame,\n",
        "    study_configuration: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 13: Reproduce Table 2.\n",
        "\n",
        "    Runs 2 specifications:\n",
        "    1. CGU Levels\n",
        "    2. CGU Log\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_analysis : pd.DataFrame\n",
        "        Analysis DataFrame.\n",
        "    study_configuration : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        Dictionary of results.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 13: Reproduce Table 2 regressions.\")\n",
        "\n",
        "    # Prepare Samples\n",
        "    samples = prepare_cgu_samples(df_analysis)\n",
        "\n",
        "    fe_col = 'state_canon'\n",
        "    if fe_col not in df_analysis.columns and 'state' in df_analysis.columns:\n",
        "        fe_col = 'state'\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Spec 1: Levels\n",
        "    logger.info(\"Running Spec: CGU Levels\")\n",
        "    res_levels = estimate_validation_model(\n",
        "        samples['levels'],\n",
        "        outcome_col='cgu_severe_count',\n",
        "        predictor_col='corruption_index',\n",
        "        fe_col=fe_col\n",
        "    )\n",
        "    validate_results(res_levels, {'R2': 0.311, 'Beta': 4.584}, \"CGU Levels\")\n",
        "    results['CGU Levels'] = res_levels\n",
        "\n",
        "    # Spec 2: Log\n",
        "    logger.info(\"Running Spec: CGU Log\")\n",
        "    df_log_ready = prepare_log_outcome(samples['log'], 'cgu_severe_count')\n",
        "    res_log = estimate_validation_model(\n",
        "        df_log_ready,\n",
        "        outcome_col='log_cgu_severe_count',\n",
        "        predictor_col='corruption_index',\n",
        "        fe_col=fe_col\n",
        "    )\n",
        "    validate_results(res_log, {'R2': 0.293, 'Beta': 0.169}, \"CGU Log\")\n",
        "    results['CGU Log'] = res_log\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "9Xi0M3zHsvZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 14 — Reproduce Table 3: correlates and construct validity\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 14: Reproduce Table 3: correlates and construct validity\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Step 1: Prepare covariates with correct scaling\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def prepare_table3_covariates(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Prepares covariates for Table 3 analysis.\n",
        "\n",
        "    - Filters to reference year 2000.\n",
        "    - Scales distance to capital (km -> 1000km).\n",
        "    - Derives log population if missing.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Analysis DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame with prepared covariates.\n",
        "    \"\"\"\n",
        "    # 1. Filter Year 2000\n",
        "    # Note: Covariates are in df_validation_raw, merged into df_analysis.\n",
        "    # The merge in Task 11 might have duplicated year columns if not careful.\n",
        "    # We assume 'covariate_reference_year' is present.\n",
        "    if 'covariate_reference_year' not in df.columns:\n",
        "        # If missing, check if we can infer or if it was dropped.\n",
        "        # Assuming it's present from Task 2 validation.\n",
        "        logger.warning(\"covariate_reference_year missing. Assuming all rows valid for now (risk).\")\n",
        "        df_2000 = df.copy()\n",
        "    else:\n",
        "        df_2000 = df[df['covariate_reference_year'] == 2000].copy()\n",
        "\n",
        "    if df_2000.empty:\n",
        "        logger.warning(\"No observations for year 2000. Table 3 will be empty.\")\n",
        "        return df_2000\n",
        "\n",
        "    # 2. Scale Distance\n",
        "    if 'distance_to_capital_km' in df_2000.columns:\n",
        "        df_2000['distance_scaled'] = df_2000['distance_to_capital_km'] * 0.001\n",
        "    else:\n",
        "        logger.warning(\"distance_to_capital_km missing.\")\n",
        "\n",
        "    # 3. Log Population\n",
        "    if 'population_log' not in df_2000.columns:\n",
        "        if 'population_total' in df_2000.columns:\n",
        "            # Handle zeros/negatives\n",
        "            pop = pd.to_numeric(df_2000['population_total'], errors='coerce')\n",
        "            df_2000['population_log'] = np.log(pop.where(pop > 0))\n",
        "        else:\n",
        "            logger.warning(\"population_log and population_total missing.\")\n",
        "\n",
        "    return df_2000\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Step 2: Estimate bivariate regressions (columns 1–6 of Table 3)\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def estimate_bivariate_models(df: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Estimates bivariate regressions for Table 3.\n",
        "\n",
        "    Model: Corruption Index ~ Covariate\n",
        "    SE: HC1\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Prepared DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        Dictionary of regression results.\n",
        "    \"\"\"\n",
        "    covariates = [\n",
        "        'literacy_rate',\n",
        "        'gdp_per_capita',\n",
        "        'urban_population_share',\n",
        "        'population_log',\n",
        "        'distance_scaled',\n",
        "        'has_radio_fm'\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for cov in covariates:\n",
        "        if cov not in df.columns:\n",
        "            logger.warning(f\"Covariate {cov} missing. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Drop missing for this specific regression\n",
        "        data = df[['corruption_index', cov]].dropna()\n",
        "\n",
        "        if data.empty:\n",
        "            logger.warning(f\"Empty sample for {cov}.\")\n",
        "            continue\n",
        "\n",
        "        formula = f\"corruption_index ~ {cov}\"\n",
        "\n",
        "        try:\n",
        "            model = smf.ols(formula, data=data)\n",
        "            res = model.fit(cov_type='HC1')\n",
        "\n",
        "            results[cov] = {\n",
        "                'beta': res.params[cov],\n",
        "                'se': res.bse[cov],\n",
        "                'p_value': res.pvalues[cov],\n",
        "                'R2': res.rsquared,\n",
        "                'N': int(res.nobs)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Regression failed for {cov}: {str(e)}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Step 3: Estimate multivariate regression (column 7 of Table 3)\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def estimate_multivariate_model(df: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Estimates the multivariate regression for Table 3.\n",
        "\n",
        "    Model: Corruption Index ~ All Covariates\n",
        "    SE: HC1\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Prepared DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        Regression results.\n",
        "    \"\"\"\n",
        "    covariates = [\n",
        "        'literacy_rate',\n",
        "        'gdp_per_capita',\n",
        "        'urban_population_share',\n",
        "        'population_log',\n",
        "        'distance_scaled',\n",
        "        'has_radio_fm'\n",
        "    ]\n",
        "\n",
        "    # Check existence\n",
        "    valid_covs = [c for c in covariates if c in df.columns]\n",
        "\n",
        "    if not valid_covs:\n",
        "        return {'error': 'No covariates available'}\n",
        "\n",
        "    # Drop missing listwise\n",
        "    cols = ['corruption_index'] + valid_covs\n",
        "    data = df[cols].dropna()\n",
        "\n",
        "    if data.empty:\n",
        "        return {'error': 'Empty sample'}\n",
        "\n",
        "    formula = \"corruption_index ~ \" + \" + \".join(valid_covs)\n",
        "\n",
        "    try:\n",
        "        model = smf.ols(formula, data=data)\n",
        "        res = model.fit(cov_type='HC1')\n",
        "\n",
        "        return {\n",
        "            'params': res.params.to_dict(),\n",
        "            'bse': res.bse.to_dict(),\n",
        "            'pvalues': res.pvalues.to_dict(),\n",
        "            'R2': res.rsquared,\n",
        "            'N': int(res.nobs)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Multivariate regression failed: {str(e)}\")\n",
        "        return {'error': str(e)}\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_table3_regressions(\n",
        "    df_analysis: pd.DataFrame,\n",
        "    study_configuration: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 14: Reproduce Table 3.\n",
        "\n",
        "    Executes:\n",
        "    1. Covariate preparation.\n",
        "    2. Bivariate regressions.\n",
        "    3. Multivariate regression.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_analysis : pd.DataFrame\n",
        "        Analysis DataFrame.\n",
        "    study_configuration : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        Results dictionary.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 14: Reproduce Table 3 regressions.\")\n",
        "\n",
        "    # Step 1: Prepare\n",
        "    df_prep = prepare_table3_covariates(df_analysis)\n",
        "\n",
        "    # Step 2: Bivariate\n",
        "    bivariate_results = estimate_bivariate_models(df_prep)\n",
        "\n",
        "    # Step 3: Multivariate\n",
        "    multivariate_results = estimate_multivariate_model(df_prep)\n",
        "\n",
        "    # Validate Joint R2\n",
        "    if 'R2' in multivariate_results:\n",
        "        target_r2 = 0.172\n",
        "        diff = abs(multivariate_results['R2'] - target_r2)\n",
        "        if diff > 0.05:\n",
        "            logger.warning(f\"Table 3 Multivariate R2 deviation. Got {multivariate_results['R2']:.3f}, Expected {target_r2}.\")\n",
        "        else:\n",
        "            logger.info(f\"Table 3 Multivariate R2 match ({multivariate_results['R2']:.3f}).\")\n",
        "\n",
        "    return {\n",
        "        'bivariate': bivariate_results,\n",
        "        'multivariate': multivariate_results\n",
        "    }\n"
      ],
      "metadata": {
        "id": "VCVmpKL0tonM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 15 — Create orchestrator function for main pipeline and LOO robustness\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 15: Create orchestrator function for main pipeline and LOO robustness\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Step 2: Define orchestrator input/output contract\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "@dataclass\n",
        "class PipelineArtifacts:\n",
        "    \"\"\"\n",
        "    Container for all artifacts produced by the main pipeline.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    df_corpus_clean : pd.DataFrame\n",
        "        Cleansed corpus data.\n",
        "    df_validation_clean : pd.DataFrame\n",
        "        Cleansed validation data.\n",
        "    df_irregularities : pd.DataFrame\n",
        "        Extracted irregularity segments.\n",
        "    df_corpus_with_index : pd.DataFrame\n",
        "        Corpus with Corruption Index and PCA features.\n",
        "    df_analysis : pd.DataFrame\n",
        "        Merged analysis DataFrame with agreement flags.\n",
        "    pca_artifacts : Dict[str, Any]\n",
        "        PCA eigenvalues, vectors, and variance.\n",
        "    table1_results : Dict[str, Any]\n",
        "        Regression results for Table 1.\n",
        "    table2_results : Dict[str, Any]\n",
        "        Regression results for Table 2.\n",
        "    table3_results : Dict[str, Any]\n",
        "        Regression results for Table 3.\n",
        "    run_manifest : Dict[str, Any]\n",
        "        Metadata, configuration, and QA metrics.\n",
        "    \"\"\"\n",
        "    df_corpus_clean: pd.DataFrame\n",
        "    df_validation_clean: pd.DataFrame\n",
        "    df_irregularities: pd.DataFrame\n",
        "    df_corpus_with_index: pd.DataFrame\n",
        "    df_analysis: pd.DataFrame\n",
        "    pca_artifacts: Dict[str, Any]\n",
        "    table1_results: Dict[str, Any]\n",
        "    table2_results: Dict[str, Any]\n",
        "    table3_results: Dict[str, Any]\n",
        "    run_manifest: Dict[str, Any]\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Step 3: Define determinism and reproducibility guarantees\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def generate_run_manifest(config: Dict[str, Any], metrics: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Generates a run manifest with environment metadata and configuration hash.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    config : Dict[str, Any]\n",
        "        Study configuration.\n",
        "    metrics : Dict[str, Any]\n",
        "        Collected QA metrics from pipeline steps.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        The run manifest.\n",
        "    \"\"\"\n",
        "    manifest = {\n",
        "        'timestamp': datetime.utcnow().isoformat(),\n",
        "        'python_version': sys.version,\n",
        "        'libraries': {\n",
        "            'pandas': pd.__version__,\n",
        "            'numpy': np.__version__,\n",
        "            # Add others as needed/available\n",
        "        },\n",
        "        'configuration': config,\n",
        "        'config_hash': hashlib.md5(json.dumps(config, sort_keys=True).encode('utf-8')).hexdigest(),\n",
        "        'metrics': metrics\n",
        "    }\n",
        "    return manifest\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_main_pipeline(\n",
        "    df_raw_corpus: pd.DataFrame,\n",
        "    df_validation_raw: pd.DataFrame,\n",
        "    language: str,\n",
        "    raw_lexicon_list: List[str],\n",
        "    study_configuration: Dict[str, Any]\n",
        ") -> PipelineArtifacts:\n",
        "    \"\"\"\n",
        "    Orchestrator function for the main pipeline (Tasks 1-14).\n",
        "\n",
        "    Executes the full end-to-end workflow:\n",
        "    1. Validation & Cleansing\n",
        "    2. Extraction & NLP\n",
        "    3. Classification & Index Construction\n",
        "    4. Validation Regressions (Tables 1, 2, 3)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_raw_corpus : pd.DataFrame\n",
        "        Raw corpus data.\n",
        "    df_validation_raw : pd.DataFrame\n",
        "        Raw validation data.\n",
        "    language : str\n",
        "        Study language.\n",
        "    raw_lexicon_list : List[str]\n",
        "        Raw dictionary terms.\n",
        "    study_configuration : Dict[str, Any]\n",
        "        Pipeline configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    PipelineArtifacts\n",
        "        Container with all produced artifacts.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Main Pipeline Execution.\")\n",
        "    metrics_accumulator = {}\n",
        "\n",
        "    try:\n",
        "        # --- Phase 1: Validation & Cleansing ---\n",
        "        # Task 1: Validate Corpus\n",
        "        validate_corpus_inputs(df_raw_corpus)\n",
        "\n",
        "        # Task 2: Validate Validation Data\n",
        "        validate_validation_inputs(df_validation_raw, df_raw_corpus)\n",
        "\n",
        "        # Task 3: Validate Config\n",
        "        validated_config = validate_config_inputs(language, raw_lexicon_list, study_configuration)\n",
        "\n",
        "        # Task 4: Cleanse Identifiers\n",
        "        df_c_clean, df_v_clean, df_c_quar, df_v_quar = cleanse_identifiers(df_raw_corpus, df_validation_raw)\n",
        "        metrics_accumulator['quarantine_corpus_ids'] = len(df_c_quar)\n",
        "        metrics_accumulator['quarantine_validation_ids'] = len(df_v_quar)\n",
        "\n",
        "        # Task 5: Cleanse Text\n",
        "        df_c_text, df_c_text_quar, text_metrics = cleanse_text_fields(df_c_clean)\n",
        "        metrics_accumulator.update(text_metrics)\n",
        "\n",
        "        # Task 6: Cleanse Numeric\n",
        "        df_c_num, num_stats = cleanse_numeric_fields(df_c_text)\n",
        "        metrics_accumulator['numeric_stats'] = num_stats\n",
        "\n",
        "        # --- Phase 2: Extraction & NLP ---\n",
        "        # Task 7: Extract Irregularities\n",
        "        df_irreg = extract_irregularities(df_c_num, validated_config)\n",
        "        metrics_accumulator['total_irregularities_extracted'] = len(df_irreg)\n",
        "\n",
        "        # Task 8: Normalize\n",
        "        df_irreg_norm, norm_lexicon, norm_map = normalize_text_and_lexicon(df_irreg, raw_lexicon_list, validated_config)\n",
        "        metrics_accumulator['normalized_lexicon_size'] = len(norm_lexicon)\n",
        "\n",
        "        # Task 9: Classify & Count\n",
        "        df_irreg_class, df_corpus_counts = classify_severe_and_count(df_irreg_norm, norm_lexicon, df_c_num)\n",
        "        metrics_accumulator['total_severe_irregularities'] = df_irreg_class['is_severe'].sum()\n",
        "\n",
        "        # --- Phase 3: Index Construction ---\n",
        "        # Task 10: PCA Index\n",
        "        df_corpus_index, pca_artifacts = build_pca_index(df_corpus_counts, validated_config)\n",
        "        metrics_accumulator['pca_explained_variance_pc1'] = pca_artifacts['explained_variance_ratio'][0]\n",
        "\n",
        "        # --- Phase 4: Validation Analysis ---\n",
        "        # Task 11: Prepare Samples\n",
        "        df_analysis = prepare_validation_samples(df_corpus_index, df_v_clean)\n",
        "        metrics_accumulator['analysis_sample_size'] = len(df_analysis)\n",
        "\n",
        "        # Task 12: Table 1\n",
        "        t1_results = run_table1_regressions(df_analysis, validated_config)\n",
        "\n",
        "        # Task 13: Table 2\n",
        "        t2_results = run_table2_regressions(df_analysis, validated_config)\n",
        "\n",
        "        # Task 14: Table 3\n",
        "        t3_results = run_table3_regressions(df_analysis, validated_config)\n",
        "\n",
        "        # --- Finalize ---\n",
        "        manifest = generate_run_manifest(validated_config, metrics_accumulator)\n",
        "\n",
        "        artifacts = PipelineArtifacts(\n",
        "            df_corpus_clean=df_c_num,\n",
        "            df_validation_clean=df_v_clean,\n",
        "            df_irregularities=df_irreg_class,\n",
        "            df_corpus_with_index=df_corpus_index,\n",
        "            df_analysis=df_analysis,\n",
        "            pca_artifacts=pca_artifacts,\n",
        "            table1_results=t1_results,\n",
        "            table2_results=t2_results,\n",
        "            table3_results=t3_results,\n",
        "            run_manifest=manifest\n",
        "        )\n",
        "\n",
        "        logger.info(\"Main Pipeline Execution Completed Successfully.\")\n",
        "        return artifacts\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"Pipeline Execution Failed: {str(e)}\")\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "1uhbJmJKvAjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 16 — Leave-one-out robustness analysis (Figure 2)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 16: Leave-one-out robustness analysis (Figure 2)\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Step 1: Define the LOO algorithm\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def execute_loo_iteration(\n",
        "    df_sample: pd.DataFrame,\n",
        "    outcome_col: str,\n",
        "    predictor_col: str,\n",
        "    fe_col: str\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Executes Leave-One-Out estimation for a given sample.\n",
        "\n",
        "    Iterates through each row, drops it, re-standardizes the predictor,\n",
        "    and estimates the model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_sample : pd.DataFrame\n",
        "        The full estimation sample.\n",
        "    outcome_col : str\n",
        "        Outcome variable name.\n",
        "    predictor_col : str\n",
        "        Predictor variable name (Corruption Index).\n",
        "    fe_col : str\n",
        "        Fixed effect variable name.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    List[Dict[str, Any]]\n",
        "        List of results for each iteration.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    indices = df_sample.index.tolist()\n",
        "\n",
        "    # Ensure data is clean\n",
        "    data = df_sample[[outcome_col, predictor_col, fe_col]].dropna()\n",
        "\n",
        "    # We iterate over the *cleaned* data indices\n",
        "    clean_indices = data.index.tolist()\n",
        "\n",
        "    for drop_idx in clean_indices:\n",
        "        # 1. Create LOO Sample\n",
        "        loo_data = data.drop(drop_idx).copy()\n",
        "\n",
        "        # 2. Estimate Model (using Task 12 function)\n",
        "        # Note: estimate_validation_model handles standardization internally\n",
        "        res = estimate_validation_model(\n",
        "            loo_data,\n",
        "            outcome_col=outcome_col,\n",
        "            predictor_col=predictor_col,\n",
        "            fe_col=fe_col\n",
        "        )\n",
        "\n",
        "        if 'error' not in res:\n",
        "            results.append({\n",
        "                'dropped_id': drop_idx,\n",
        "                'beta': res['beta'],\n",
        "                'R2': res['R2'],\n",
        "                'N': res['N']\n",
        "            })\n",
        "        else:\n",
        "            logger.warning(f\"LOO iteration failed for dropped_id {drop_idx}: {res['error']}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Step 2: Execute LOO for both agreement samples\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_loo_for_samples(df_analysis: pd.DataFrame) -> Dict[str, List[Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Runs LOO analysis for Strict and Near agreement samples.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_analysis : pd.DataFrame\n",
        "        Analysis DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, List[Dict[str, Any]]]\n",
        "        Dictionary of LOO results for 'Strict' and 'Near'.\n",
        "    \"\"\"\n",
        "    loo_results = {}\n",
        "\n",
        "    fe_col = 'state_canon'\n",
        "    if fe_col not in df_analysis.columns and 'state' in df_analysis.columns:\n",
        "        fe_col = 'state'\n",
        "\n",
        "    # 1. Strict Agreement\n",
        "    mask_strict = df_analysis['is_strict_agreement'] == True\n",
        "    df_strict = df_analysis[mask_strict].copy()\n",
        "\n",
        "    if not df_strict.empty:\n",
        "        logger.info(f\"Running LOO for Strict Agreement (N={len(df_strict)})\")\n",
        "        loo_results['Strict'] = execute_loo_iteration(\n",
        "            df_strict, 'hc_mean', 'corruption_index', fe_col\n",
        "        )\n",
        "    else:\n",
        "        logger.warning(\"Strict Agreement sample empty. Skipping LOO.\")\n",
        "        loo_results['Strict'] = []\n",
        "\n",
        "    # 2. Near Agreement\n",
        "    mask_near = df_analysis['is_near_agreement'] == True\n",
        "    df_near = df_analysis[mask_near].copy()\n",
        "\n",
        "    if not df_near.empty:\n",
        "        logger.info(f\"Running LOO for Near Agreement (N={len(df_near)})\")\n",
        "        loo_results['Near'] = execute_loo_iteration(\n",
        "            df_near, 'hc_mean', 'corruption_index', fe_col\n",
        "        )\n",
        "    else:\n",
        "        logger.warning(\"Near Agreement sample empty. Skipping LOO.\")\n",
        "        loo_results['Near'] = []\n",
        "\n",
        "    return loo_results\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Step 3: Summarize and validate stability\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def summarize_loo_stability(loo_results: Dict[str, List[Dict[str, Any]]]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Summarizes LOO results (min/max Beta and R2) and validates against targets.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    loo_results : Dict[str, List[Dict[str, Any]]]\n",
        "        LOO results from Step 2.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        Summary statistics.\n",
        "    \"\"\"\n",
        "    summary = {}\n",
        "\n",
        "    targets = {\n",
        "        'Strict': {'beta_min': 1.20, 'beta_max': 1.73, 'r2_min': 0.67, 'r2_max': 0.83},\n",
        "        'Near': {'beta_min': 1.17, 'beta_max': 1.35, 'r2_min': 0.66, 'r2_max': 0.74}\n",
        "    }\n",
        "\n",
        "    # Summarize LOO results (min/max Beta and R2) and validate against targets\n",
        "    for sample_name, results in loo_results.items():\n",
        "        if not results:\n",
        "            continue\n",
        "\n",
        "        betas = [r['beta'] for r in results]\n",
        "        r2s = [r['R2'] for r in results]\n",
        "\n",
        "        stats = {\n",
        "            'beta_min': min(betas),\n",
        "            'beta_max': max(betas),\n",
        "            'r2_min': min(r2s),\n",
        "            'r2_max': max(r2s)\n",
        "        }\n",
        "        summary[sample_name] = stats\n",
        "\n",
        "        # Validate\n",
        "        t = targets.get(sample_name)\n",
        "        if t:\n",
        "            # Check ranges (allow small tolerance)\n",
        "            tol = 0.1\n",
        "            if (stats['beta_min'] < t['beta_min'] - tol) or (stats['beta_max'] > t['beta_max'] + tol):\n",
        "                logger.warning(f\"[{sample_name}] LOO Beta range mismatch. Got [{stats['beta_min']:.2f}, {stats['beta_max']:.2f}], Expected [{t['beta_min']}, {t['beta_max']}]\")\n",
        "            else:\n",
        "                logger.info(f\"[{sample_name}] LOO Beta range match.\")\n",
        "\n",
        "            if (stats['r2_min'] < t['r2_min'] - tol) or (stats['r2_max'] > t['r2_max'] + tol):\n",
        "                logger.warning(f\"[{sample_name}] LOO R2 range mismatch. Got [{stats['r2_min']:.2f}, {stats['r2_max']:.2f}], Expected [{t['r2_min']}, {t['r2_max']}]\")\n",
        "            else:\n",
        "                logger.info(f\"[{sample_name}] LOO R2 range match.\")\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_loo_analysis(\n",
        "    df_analysis: pd.DataFrame,\n",
        "    study_configuration: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 16: Leave-One-Out Robustness.\n",
        "\n",
        "    Executes:\n",
        "    1. LOO iterations for Strict and Near samples.\n",
        "    2. Summarization and validation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_analysis : pd.DataFrame\n",
        "        Analysis DataFrame.\n",
        "    study_configuration : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        Dictionary containing detailed results and summary stats.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 16: Leave-One-Out Robustness Analysis.\")\n",
        "\n",
        "    # Step 1 & 2\n",
        "    raw_results = run_loo_for_samples(df_analysis)\n",
        "\n",
        "    # Step 3\n",
        "    summary = summarize_loo_stability(raw_results)\n",
        "\n",
        "    return {\n",
        "        'detailed_results': raw_results,\n",
        "        'summary': summary\n",
        "    }\n"
      ],
      "metadata": {
        "id": "_3WJDyPKwjKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 17 — Create orchestrator function for supervised-learning robustness\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 17: Create orchestrator function for supervised-learning robustness\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 17, Step 2: Define supervised orchestrator input/output contract\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "@dataclass\n",
        "class SupervisedArtifacts:\n",
        "    \"\"\"\n",
        "    Container for artifacts produced by the supervised learning robustness pipeline.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    classification_reports : Dict[str, Dict[str, Any]]\n",
        "        Metrics (precision, recall, f1) for each classifier (LR, NB, SVM).\n",
        "    ml_severe_counts : pd.DataFrame\n",
        "        DataFrame with 'report_id' and 'severe_irregularities_count_ml'.\n",
        "    ml_pca_index : pd.DataFrame\n",
        "        DataFrame with 'report_id' and 'corruption_index_ml'.\n",
        "    comparison_stats : Dict[str, float]\n",
        "        Comparison metrics (R2, correlation) between Dictionary-PCA and ML-PCA.\n",
        "    top_features : Dict[str, List[str]]\n",
        "        Top predictive bigrams for high/low corruption classes.\n",
        "    training_metadata : Dict[str, Any]\n",
        "        Metadata about training sample size, class balance, and vocabulary.\n",
        "    \"\"\"\n",
        "    classification_reports: Dict[str, Dict[str, Any]]\n",
        "    ml_severe_counts: pd.DataFrame\n",
        "    ml_pca_index: pd.DataFrame\n",
        "    comparison_stats: Dict[str, float]\n",
        "    top_features: Dict[str, List[str]]\n",
        "    training_metadata: Dict[str, Any]\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 17, Step 3: Freeze randomness and evaluation protocol\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def set_random_seed(seed: int = 42) -> None:\n",
        "    \"\"\"\n",
        "    Sets the global random seed for reproducibility.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    seed : int\n",
        "        Random seed.\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    # If using other libraries with global seeds, set them here.\n",
        "    # Sklearn estimators will accept random_state locally.\n"
      ],
      "metadata": {
        "id": "IJkSGwu6x4ZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 18 — Supervised-learning robustness: label construction and TF-IDF features\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 18: Supervised-learning robustness: label construction and TF-IDF features\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 18, Step 1: Create binary labels via median split (Appendix A4)\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def construct_labels(df_analysis: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Constructs binary labels for supervised learning based on human coder agreement.\n",
        "\n",
        "    Logic:\n",
        "    1. Filter to rows where both FF and GT are non-null.\n",
        "    2. Compute Score = (FF + GT) / 2.\n",
        "    3. Compute Median(Score).\n",
        "    4. Define High/Low class for each coder relative to median.\n",
        "    5. Keep rows where coders agree on class.\n",
        "    6. Label y=1 (High) if Score >= Median, else 0.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_analysis : pd.DataFrame\n",
        "        Analysis DataFrame with 'ff_corruption_count' and 'gt_corruption_count'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame with 'report_id' and 'label' (y), indexed by report_id.\n",
        "    \"\"\"\n",
        "    # 1. Filter valid\n",
        "    mask_valid = df_analysis['ff_corruption_count'].notna() & df_analysis['gt_corruption_count'].notna()\n",
        "    df_valid = df_analysis[mask_valid].copy()\n",
        "\n",
        "    if df_valid.empty:\n",
        "        raise ValueError(\"No valid rows with both FF and GT counts for label construction.\")\n",
        "\n",
        "    # 2. Compute Score\n",
        "    ff = df_valid['ff_corruption_count']\n",
        "    gt = df_valid['gt_corruption_count']\n",
        "    scores = (ff + gt) / 2.0\n",
        "\n",
        "    # 3. Compute Median\n",
        "    median_score = scores.median()\n",
        "    logger.info(f\"Label Construction: Median Score = {median_score}\")\n",
        "\n",
        "    # 4. Define Classes per Coder\n",
        "    # High (1) if >= median, Low (0) if < median\n",
        "    class_ff = (ff >= median_score).astype(int)\n",
        "    class_gt = (gt >= median_score).astype(int)\n",
        "\n",
        "    # 5. Agreement Filter\n",
        "    mask_agree = (class_ff == class_gt)\n",
        "    df_train = df_valid[mask_agree].copy()\n",
        "\n",
        "    # 6. Final Label\n",
        "    # Since they agree, we can use either class, or recompute on score (same result)\n",
        "    df_train['label'] = class_ff[mask_agree]\n",
        "\n",
        "    logger.info(f\"Label Construction: {len(df_train)} training samples retained from {len(df_valid)} candidates.\")\n",
        "    logger.info(f\"Class Balance: {df_train['label'].value_counts().to_dict()}\")\n",
        "\n",
        "    return df_train[['report_id', 'label']]\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 18, Step 2: Build TF-IDF bigram features (Equation 2)\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def build_document_corpus(df_irregularities: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Constructs report-level documents by concatenating irregularity texts.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_irregularities : pd.DataFrame\n",
        "        Irregularity segments.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        Series of document texts, indexed by report_id.\n",
        "    \"\"\"\n",
        "    # Sort by index to preserve order\n",
        "    df_sorted = df_irregularities.sort_values(['report_id', 'irregularity_index'])\n",
        "\n",
        "    # Group and join\n",
        "    # Use a space separator\n",
        "    docs = df_sorted.groupby('report_id')['irregularity_text_raw'].apply(lambda x: \" \".join(x.astype(str)))\n",
        "\n",
        "    return docs\n",
        "\n",
        "\n",
        "def get_custom_preprocessor(config: Dict[str, Any]) -> Callable[[str], str]:\n",
        "    \"\"\"\n",
        "    Factory function that returns a custom text preprocessing callable configured for the TfidfVectorizer.\n",
        "\n",
        "    This preprocessor implements the specific NLP pipeline described in the study (Appendix A4)\n",
        "    to ensure fidelity in feature engineering. The pipeline includes:\n",
        "    1. Unicode Normalization (NFD form).\n",
        "    2. Accent Removal (stripping combining diacritical marks).\n",
        "    3. Case Folding (conversion to lowercase).\n",
        "    4. Punctuation Removal (replacing non-alphanumeric characters with space).\n",
        "    5. Tokenization (splitting by whitespace).\n",
        "    6. Stopword Removal (using Portuguese stopwords).\n",
        "    7. Stemming (using Porter Stemmer).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    config : Dict[str, Any]\n",
        "        The study configuration dictionary containing NLP parameters.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Callable[[str], str]\n",
        "        A function that takes a raw text string and returns a preprocessed, space-joined string of stems.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    LookupError\n",
        "        If NLTK stopwords are not available and cannot be downloaded.\n",
        "    \"\"\"\n",
        "    # Initialize the Porter Stemmer as specified in the study configuration\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    # Compile regex for punctuation removal: matches any character that is NOT a word char or whitespace\n",
        "    punct_re = re.compile(r'[^\\w\\s]')\n",
        "\n",
        "    # Load Portuguese stopwords from NLTK\n",
        "    try:\n",
        "        stops = set(stopwords.words('portuguese'))\n",
        "    except LookupError:\n",
        "        logger.info(\"Downloading NLTK stopwords...\")\n",
        "        nltk.download('stopwords')\n",
        "        stops = set(stopwords.words('portuguese'))\n",
        "\n",
        "    def preprocess(text: str) -> str:\n",
        "        \"\"\"\n",
        "        Preprocesses a single text string according to the study's NLP pipeline.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        text : str\n",
        "            The raw input text.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            The normalized, stemmed text string.\n",
        "        \"\"\"\n",
        "        # Handle null or non-string inputs gracefully\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "\n",
        "        # 1. NFD Normalization: Decompose characters (e.g., 'ç' -> 'c' + '¸')\n",
        "        text_nfd = unicodedata.normalize('NFD', str(text))\n",
        "\n",
        "        # 2. Remove Accents: Filter out non-spacing mark characters (Unicode category 'Mn')\n",
        "        text_no_accents = \"\".join(c for c in text_nfd if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "        # 3. Lowercase: Convert to lowercase for case-insensitivity\n",
        "        text_lower = text_no_accents.lower()\n",
        "\n",
        "        # 4. Remove Punctuation: Replace punctuation with space to prevent merging words\n",
        "        text_no_punct = punct_re.sub(' ', text_lower)\n",
        "\n",
        "        # 5. Tokenize: Split by whitespace\n",
        "        tokens = text_no_punct.split()\n",
        "\n",
        "        # 6. Remove Stopwords and 7. Stem: Apply Porter Stemmer to non-stopword tokens\n",
        "        stems = [stemmer.stem(t) for t in tokens if t not in stops]\n",
        "\n",
        "        # Join stems back into a single string for TfidfVectorizer\n",
        "        return \" \".join(stems)\n",
        "\n",
        "    return preprocess\n",
        "\n",
        "def create_tfidf_vectorizer(config: Dict[str, Any]) -> TfidfVectorizer:\n",
        "    \"\"\"\n",
        "    Creates and configures the TfidfVectorizer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    config : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    TfidfVectorizer\n",
        "        Configured vectorizer.\n",
        "    \"\"\"\n",
        "    sup_config = config['pipeline_parameters']['supervised_learning_config']\n",
        "\n",
        "    # Get preprocessor\n",
        "    preprocessor = get_custom_preprocessor(config)\n",
        "\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        preprocessor=preprocessor,\n",
        "        ngram_range=tuple(sup_config['tfidf_ngram_range']), # (2,2)\n",
        "        norm=sup_config['tfidf_norm'],\n",
        "        use_idf=sup_config['tfidf_use_idf'],\n",
        "        smooth_idf=sup_config['tfidf_smooth_idf']\n",
        "    )\n",
        "\n",
        "    return vectorizer\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 18, Step 3: Materialize the feature matrix and label vector\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def align_features_and_labels(\n",
        "    docs: pd.Series,\n",
        "    labels: pd.DataFrame,\n",
        "    vectorizer: TfidfVectorizer\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Aligns documents and labels, fits vectorizer, and creates training matrices.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    docs : pd.Series\n",
        "        Documents indexed by report_id.\n",
        "    labels : pd.DataFrame\n",
        "        Labels with 'report_id' and 'label'.\n",
        "    vectorizer : TfidfVectorizer\n",
        "        Unfitted vectorizer.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        Bundle containing X_train, y_train, vectorizer, report_ids, metadata.\n",
        "    \"\"\"\n",
        "    # Align indices\n",
        "    # Intersection of report_ids in docs and labels\n",
        "    common_ids = sorted(list(set(docs.index) & set(labels['report_id'])))\n",
        "\n",
        "    if not common_ids:\n",
        "        raise ValueError(\"No overlap between document corpus and labeled samples.\")\n",
        "\n",
        "    # Filter and sort\n",
        "    docs_train = docs.loc[common_ids]\n",
        "    labels_train = labels.set_index('report_id').loc[common_ids]\n",
        "\n",
        "    # Fit Vectorizer\n",
        "    logger.info(\"Fitting TF-IDF Vectorizer...\")\n",
        "    X_train = vectorizer.fit_transform(docs_train)\n",
        "    y_train = labels_train['label'].values\n",
        "\n",
        "    vocab = vectorizer.get_feature_names_out()\n",
        "\n",
        "    metadata = {\n",
        "        'n_samples': X_train.shape[0],\n",
        "        'n_features': X_train.shape[1],\n",
        "        'class_balance': {0: int((y_train==0).sum()), 1: int((y_train==1).sum())},\n",
        "        'vocabulary_size': len(vocab)\n",
        "    }\n",
        "\n",
        "    logger.info(f\"Feature Matrix Built: {metadata}\")\n",
        "\n",
        "    return {\n",
        "        'X_train': X_train,\n",
        "        'y_train': y_train,\n",
        "        'vectorizer': vectorizer,\n",
        "        'report_ids_train': common_ids,\n",
        "        'metadata': metadata\n",
        "    }\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 18, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def build_supervised_data(\n",
        "    df_irregularities: pd.DataFrame,\n",
        "    df_analysis: pd.DataFrame,\n",
        "    study_configuration: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 18: Build Supervised Data.\n",
        "\n",
        "    Executes:\n",
        "    1. Label Construction.\n",
        "    2. Document Construction.\n",
        "    3. Vectorization and Alignment.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_irregularities : pd.DataFrame\n",
        "        Irregularity segments.\n",
        "    df_analysis : pd.DataFrame\n",
        "        Validation data.\n",
        "    study_configuration : Dict[str, Any]\n",
        "        Config.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        Training data bundle.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 18: Build Supervised Data.\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Labels\n",
        "        df_labels = construct_labels(df_analysis)\n",
        "\n",
        "        # Step 2: Docs\n",
        "        docs = build_document_corpus(df_irregularities)\n",
        "\n",
        "        # Step 3: Vectorizer\n",
        "        vectorizer = create_tfidf_vectorizer(study_configuration)\n",
        "\n",
        "        # Step 4: Align & Build\n",
        "        bundle = align_features_and_labels(docs, df_labels, vectorizer)\n",
        "\n",
        "        logger.info(\"Task 18 completed.\")\n",
        "        return bundle\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"Task 18 Failed: {str(e)}\")\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "XEj5aPJf4YDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 19 — Supervised-learning robustness: train classifiers and compare indices\n",
        "\n",
        "# ===============================================================================\n",
        "# Task 19: Supervised-learning robustness: train classifiers and compare indices\n",
        "# ===============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 19, Step 1: Train classifiers per configuration\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def train_classifiers(\n",
        "    training_bundle: Dict[str, Any],\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Trains classifiers (LR, NB, SVM) on the full labeled dataset and computes metrics.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    training_bundle : Dict[str, Any]\n",
        "        Output from Task 18 (X_train, y_train, vectorizer, etc.).\n",
        "    config : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        Dictionary containing:\n",
        "        - 'models': Dict of trained model objects.\n",
        "        - 'reports': Dict of classification reports.\n",
        "        - 'top_features': Dict of top bigrams per class (for LR).\n",
        "    \"\"\"\n",
        "    X_train = training_bundle['X_train']\n",
        "    y_train = training_bundle['y_train']\n",
        "    vocab = training_bundle['vectorizer'].get_feature_names_out()\n",
        "\n",
        "    sup_config = config['pipeline_parameters']['supervised_learning_config']\n",
        "    hyperparams = sup_config.get('classifier_hyperparameters', {})\n",
        "\n",
        "    models = {}\n",
        "    reports = {}\n",
        "    top_features = {}\n",
        "\n",
        "    # 1. Logistic Regression\n",
        "    if sup_config.get('use_logistic_regression', True):\n",
        "        logger.info(\"Training Logistic Regression...\")\n",
        "        params = hyperparams.get('logistic_regression') or {'random_state': 42, 'max_iter': 1000}\n",
        "        lr = LogisticRegression(**params)\n",
        "        lr.fit(X_train, y_train)\n",
        "        models['LogisticRegression'] = lr\n",
        "\n",
        "        y_pred = lr.predict(X_train)\n",
        "        reports['LogisticRegression'] = classification_report(y_train, y_pred, output_dict=True)\n",
        "\n",
        "        # Extract top features\n",
        "        coefs = lr.coef_[0]\n",
        "        top_indices = coefs.argsort()\n",
        "        # Bottom 10 (Low Corruption, class 0) and Top 10 (High Corruption, class 1)\n",
        "        low_features = [vocab[i] for i in top_indices[:10]]\n",
        "        high_features = [vocab[i] for i in top_indices[-10:]]\n",
        "        top_features['LogisticRegression'] = {'low': low_features, 'high': high_features}\n",
        "\n",
        "    # 2. Naive Bayes\n",
        "    if sup_config.get('use_naive_bayes', True):\n",
        "        logger.info(\"Training Naive Bayes...\")\n",
        "        params = hyperparams.get('naive_bayes') or {}\n",
        "        nb = MultinomialNB(**params)\n",
        "        nb.fit(X_train, y_train)\n",
        "        models['NaiveBayes'] = nb\n",
        "\n",
        "        y_pred = nb.predict(X_train)\n",
        "        reports['NaiveBayes'] = classification_report(y_train, y_pred, output_dict=True)\n",
        "\n",
        "    # 3. Linear SVM\n",
        "    if sup_config.get('use_linear_svm', True):\n",
        "        logger.info(\"Training Linear SVM...\")\n",
        "        params = hyperparams.get('linear_svm') or {'random_state': 42, 'dual': 'auto'}\n",
        "        svm = LinearSVC(**params)\n",
        "        svm.fit(X_train, y_train)\n",
        "        models['LinearSVM'] = svm\n",
        "\n",
        "        y_pred = svm.predict(X_train)\n",
        "        reports['LinearSVM'] = classification_report(y_train, y_pred, output_dict=True)\n",
        "\n",
        "    return {\n",
        "        'models': models,\n",
        "        'reports': reports,\n",
        "        'top_features': top_features\n",
        "    }\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 19, Step 2: Generate ML-derived severe counts and rebuild PCA index\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def rebuild_ml_index(\n",
        "    df_irregularities: pd.DataFrame,\n",
        "    df_corpus_counts: pd.DataFrame,\n",
        "    model: Any,\n",
        "    vectorizer: TfidfVectorizer,\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Predicts severity for all irregularities using the trained model, aggregates counts,\n",
        "    and rebuilds the PCA index.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_irregularities : pd.DataFrame\n",
        "        All irregularity segments.\n",
        "    df_corpus_counts : pd.DataFrame\n",
        "        Original corpus data (for other PCA features).\n",
        "    model : Any\n",
        "        Trained classifier (e.g., LogisticRegression).\n",
        "    vectorizer : TfidfVectorizer\n",
        "        Fitted vectorizer.\n",
        "    config : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        Dictionary containing:\n",
        "        - 'ml_severe_counts': DataFrame with ML-derived counts.\n",
        "        - 'ml_pca_index': DataFrame with ML-PCA scores.\n",
        "        - 'pca_artifacts': PCA details.\n",
        "    \"\"\"\n",
        "    logger.info(\"Predicting severity for all irregularities...\")\n",
        "\n",
        "    # 1. Transform all irregularities\n",
        "    # Handle NaN texts\n",
        "    texts = df_irregularities['irregularity_text_raw'].fillna(\"\").astype(str)\n",
        "    X_all = vectorizer.transform(texts)\n",
        "\n",
        "    # 2. Predict\n",
        "    y_pred = model.predict(X_all)\n",
        "\n",
        "    # 3. Aggregate\n",
        "    df_preds = df_irregularities[['report_id']].copy()\n",
        "    df_preds['is_severe_ml'] = y_pred\n",
        "\n",
        "    agg = df_preds.groupby('report_id')['is_severe_ml'].sum().reset_index()\n",
        "    agg.rename(columns={'is_severe_ml': 'severe_irregularities_count_ml'}, inplace=True)\n",
        "\n",
        "    # 4. Merge with Corpus Features\n",
        "    # We need the other 4 features from df_corpus_counts\n",
        "    # image_count, page_count, report_lines_count, total_irregularities_count\n",
        "    # severe_irregularities_count is replaced\n",
        "    features_needed = ['report_id', 'image_count', 'page_count', 'report_lines_count', 'total_irregularities_count']\n",
        "    df_features = pd.merge(\n",
        "        df_corpus_counts[features_needed],\n",
        "        agg,\n",
        "        on='report_id',\n",
        "        how='left'\n",
        "    )\n",
        "    # Fill NaN counts with 0 (reports with no irregularities)\n",
        "    df_features['severe_irregularities_count_ml'] = df_features['severe_irregularities_count_ml'].fillna(0).astype(int)\n",
        "\n",
        "    # 5. Rebuild PCA\n",
        "    # We reuse the logic from Task 10, but we need to construct the matrix manually\n",
        "    # because the column name changed.\n",
        "\n",
        "    # Construct X matrix\n",
        "    # Order: image, severe(ML), page, lines, total\n",
        "    X_ml = df_features[[\n",
        "        'image_count',\n",
        "        'severe_irregularities_count_ml',\n",
        "        'page_count',\n",
        "        'report_lines_count',\n",
        "        'total_irregularities_count'\n",
        "    ]].values.astype(float)\n",
        "\n",
        "    # Standardize\n",
        "    mu = np.mean(X_ml, axis=0)\n",
        "    sigma = np.std(X_ml, axis=0, ddof=1)\n",
        "    # Handle zero variance if any (unlikely for counts)\n",
        "    sigma[sigma == 0] = 1.0\n",
        "    Z_ml = (X_ml - mu) / sigma\n",
        "\n",
        "    # PCA\n",
        "    Sigma = (1 / (Z_ml.shape[0] - 1)) * np.dot(Z_ml.T, Z_ml)\n",
        "    evals, evecs = np.linalg.eigh(Sigma)\n",
        "\n",
        "    # Sort\n",
        "    idx = np.argsort(evals)[::-1]\n",
        "    evals = evals[idx]\n",
        "    evecs = evecs[:, idx]\n",
        "\n",
        "    # PC1\n",
        "    pc1_loadings = evecs[:, 0]\n",
        "    if np.sum(pc1_loadings) < 0:\n",
        "        pc1_loadings = -pc1_loadings\n",
        "\n",
        "    scores = np.dot(Z_ml, pc1_loadings)\n",
        "\n",
        "    df_scores = pd.DataFrame({\n",
        "        'report_id': df_features['report_id'],\n",
        "        'corruption_index_ml': scores\n",
        "    })\n",
        "\n",
        "    return {\n",
        "        'ml_severe_counts': agg,\n",
        "        'ml_pca_index': df_scores,\n",
        "        'pca_artifacts': {'eigenvalues': evals, 'loadings': pc1_loadings}\n",
        "    }\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 19, Step 3: Compare dictionary-PCA vs ML-PCA indices\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compare_indices(\n",
        "    df_dict_index: pd.DataFrame,\n",
        "    df_ml_index: pd.DataFrame\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Compares the Dictionary-based PCA index with the ML-based PCA index.\n",
        "\n",
        "    Computes Pearson correlation and R^2.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_dict_index : pd.DataFrame\n",
        "        DataFrame with 'report_id' and 'corruption_index'.\n",
        "    df_ml_index : pd.DataFrame\n",
        "        DataFrame with 'report_id' and 'corruption_index_ml'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, float]\n",
        "        Dictionary with 'correlation', 'R2', 'N'.\n",
        "    \"\"\"\n",
        "    # Merge\n",
        "    merged = pd.merge(\n",
        "        df_dict_index[['report_id', 'corruption_index']],\n",
        "        df_ml_index[['report_id', 'corruption_index_ml']],\n",
        "        on='report_id',\n",
        "        how='inner'\n",
        "    )\n",
        "\n",
        "    if merged.empty:\n",
        "        logger.warning(\"No overlapping reports for index comparison.\")\n",
        "        return {'correlation': 0.0, 'R2': 0.0, 'N': 0}\n",
        "\n",
        "    # Correlation\n",
        "    corr = merged['corruption_index'].corr(merged['corruption_index_ml'])\n",
        "    r2 = corr ** 2\n",
        "\n",
        "    logger.info(f\"Index Comparison: Correlation = {corr:.4f}, R2 = {r2:.4f} (N={len(merged)})\")\n",
        "\n",
        "    return {\n",
        "        'correlation': corr,\n",
        "        'R2': r2,\n",
        "        'N': len(merged)\n",
        "    }\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 19, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def train_and_compare(\n",
        "    training_bundle: Dict[str, Any],\n",
        "    df_irregularities: pd.DataFrame,\n",
        "    df_corpus_counts: pd.DataFrame,\n",
        "    study_configuration: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrator function for Task 19: Train, Predict, Compare.\n",
        "\n",
        "    Executes:\n",
        "    1. Train classifiers.\n",
        "    2. Rebuild PCA with ML counts.\n",
        "    3. Compare indices.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    training_bundle : Dict[str, Any]\n",
        "        Training data.\n",
        "    df_irregularities : pd.DataFrame\n",
        "        Irregularity segments.\n",
        "    df_corpus_counts : pd.DataFrame\n",
        "        Corpus counts (with dict index).\n",
        "    study_configuration : Dict[str, Any]\n",
        "        Config.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        Artifacts bundle.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 19: Train and Compare.\")\n",
        "\n",
        "    # Step 1: Train\n",
        "    models_bundle = train_classifiers(training_bundle, study_configuration)\n",
        "\n",
        "    # Step 2: Rebuild (using LR)\n",
        "    # Check if LR exists\n",
        "    if 'LogisticRegression' in models_bundle['models']:\n",
        "        lr_model = models_bundle['models']['LogisticRegression']\n",
        "        ml_index_bundle = rebuild_ml_index(\n",
        "            df_irregularities,\n",
        "            df_corpus_counts,\n",
        "            lr_model,\n",
        "            training_bundle['vectorizer'],\n",
        "            study_configuration\n",
        "        )\n",
        "\n",
        "        # Step 3: Compare\n",
        "        comparison = compare_indices(\n",
        "            df_corpus_counts,\n",
        "            ml_index_bundle['ml_pca_index']\n",
        "        )\n",
        "    else:\n",
        "        logger.warning(\"Logistic Regression not trained. Skipping index rebuild.\")\n",
        "        ml_index_bundle = {}\n",
        "        comparison = {}\n",
        "\n",
        "    return {\n",
        "        'models_bundle': models_bundle,\n",
        "        'ml_index_bundle': ml_index_bundle,\n",
        "        'comparison': comparison\n",
        "    }\n"
      ],
      "metadata": {
        "id": "eXpY5P9d7xhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Orchestrator function for running Supervised Learning Robustness\n",
        "\n",
        "# ===============================================================================\n",
        "# Orchestrator function for running Supervised Learning Robustness\n",
        "# ===============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Orchestrator function for running Supervised Learning Robustness\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_supervised_robustness(\n",
        "    df_irregularities: pd.DataFrame,\n",
        "    df_analysis: pd.DataFrame,\n",
        "    df_corpus_counts: pd.DataFrame,\n",
        "    study_configuration: Dict[str, Any]\n",
        ") -> SupervisedArtifacts:\n",
        "    \"\"\"\n",
        "    Orchestrator function for running Supervised Learning Robustness.\n",
        "\n",
        "    Executes the full supervised learning pipeline:\n",
        "    1. Label construction & Feature Engineering (Task 18).\n",
        "    2. Model Training & Prediction (Task 19).\n",
        "    3. PCA Rebuild & Comparison (Task 19).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_irregularities : pd.DataFrame\n",
        "        Irregularity segments (text source).\n",
        "    df_analysis : pd.DataFrame\n",
        "        Validation data (label source).\n",
        "    df_corpus_counts : pd.DataFrame\n",
        "        Corpus data with original counts (for PCA rebuild).\n",
        "    study_configuration : Dict[str, Any]\n",
        "        Pipeline configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    SupervisedArtifacts\n",
        "        Container with all supervised results.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Supervised Learning Robustness Pipeline.\")\n",
        "\n",
        "    try:\n",
        "        # Set seed\n",
        "        set_random_seed(42)\n",
        "\n",
        "        # Note: The following functions are defined in Tasks 18 and 19.\n",
        "        # They are called here to define the complete workflow.\n",
        "\n",
        "        # Step 1: Build Labels and Features (Task 18)\n",
        "        # Returns training data bundle and vectorizer for prediction\n",
        "        # Expected return: Dict with keys 'X_train', 'y_train', 'vectorizer', 'metadata', 'report_ids_train'\n",
        "        training_bundle = build_supervised_data(\n",
        "            df_irregularities,\n",
        "            df_analysis,\n",
        "            study_configuration\n",
        "        )\n",
        "\n",
        "        # Step 2: Train Classifiers (Task 19)\n",
        "        # Returns trained models and metrics\n",
        "        # Expected return: Dict with keys 'models', 'reports', 'top_features'\n",
        "        models_bundle = train_classifiers(\n",
        "            training_bundle,\n",
        "            study_configuration\n",
        "        )\n",
        "\n",
        "        # Step 3: Predict & Rebuild Index (Task 19)\n",
        "        # Uses Logistic Regression as primary for index reconstruction\n",
        "        # Expected return: Dict with keys 'ml_severe_counts', 'ml_pca_index'\n",
        "        ml_index_bundle = rebuild_ml_index(\n",
        "            df_irregularities,\n",
        "            df_corpus_counts,\n",
        "            models_bundle['models']['LogisticRegression'],\n",
        "            training_bundle['vectorizer'],\n",
        "            study_configuration\n",
        "        )\n",
        "\n",
        "        # Step 4: Compare (Task 19)\n",
        "        # Expected return: Dict with keys 'correlation', 'R2', 'N'\n",
        "        comparison = compare_indices(\n",
        "            df_corpus_counts, # Contains original 'corruption_index'\n",
        "            ml_index_bundle['ml_pca_index']\n",
        "        )\n",
        "\n",
        "        # Assemble Artifacts\n",
        "        artifacts = SupervisedArtifacts(\n",
        "            classification_reports=models_bundle['reports'],\n",
        "            ml_severe_counts=ml_index_bundle['ml_severe_counts'],\n",
        "            ml_pca_index=ml_index_bundle['ml_pca_index'],\n",
        "            comparison_stats=comparison,\n",
        "            top_features=models_bundle['top_features'],\n",
        "            training_metadata=training_bundle['metadata']\n",
        "        )\n",
        "\n",
        "        logger.info(\"Supervised Robustness Pipeline Completed.\")\n",
        "        return artifacts\n",
        "\n",
        "    except NameError as e:\n",
        "        # This handles the case where functions aren't defined yet in this notebook context\n",
        "        logger.warning(f\"Sub-functions for Tasks 18/19 not yet defined: {e}\")\n",
        "        raise e\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"Supervised Pipeline Failed: {str(e)}\")\n",
        "        raise e\n",
        "\n"
      ],
      "metadata": {
        "id": "M5BXBeG7AsWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-Level Orchestrator Function\n",
        "\n",
        "# ==============================================================================\n",
        "# Top-Level Orchestrator\n",
        "# ==============================================================================\n",
        "\n",
        "def execute_full_research_pipeline(\n",
        "    df_raw_corpus: pd.DataFrame,\n",
        "    df_validation_raw: pd.DataFrame,\n",
        "    language: str,\n",
        "    raw_lexicon_list: List[str],\n",
        "    study_configuration: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Top-level orchestrator that executes the entire end-to-end research pipeline.\n",
        "\n",
        "    This function coordinates the execution of:\n",
        "    1. The Main Pipeline (Data Cleansing, NLP, Index Construction, Validation Regressions).\n",
        "    2. Leave-One-Out (LOO) Robustness Analysis.\n",
        "    3. Supervised Learning Robustness Analysis.\n",
        "\n",
        "    It ensures data flows correctly between these stages, utilizing artifacts produced\n",
        "    in the main pipeline as inputs for the robustness checks.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_raw_corpus : pd.DataFrame\n",
        "        The raw corpus DataFrame containing audit reports and metadata.\n",
        "    df_validation_raw : pd.DataFrame\n",
        "        The raw validation DataFrame containing human-coded data and covariates.\n",
        "    language : str\n",
        "        The language of the study (e.g., \"Portuguese\").\n",
        "    raw_lexicon_list : List[str]\n",
        "        The list of raw dictionary terms for corruption identification.\n",
        "    study_configuration : Dict[str, Any]\n",
        "        The comprehensive configuration dictionary for the study.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        A dictionary containing all artifacts from the research pipeline:\n",
        "        - 'main_pipeline': Dict of artifacts from the main analysis (tables, indices, etc.).\n",
        "        - 'loo_analysis': Dict of results from the Leave-One-Out robustness check.\n",
        "        - 'supervised_robustness': Dict of results from the Supervised Learning check.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting End-to-End Research Pipeline Execution.\")\n",
        "\n",
        "    try:\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Step 1: Execute Main Pipeline\n",
        "        # ----------------------------------------------------------------------\n",
        "        logger.info(\">>> Step 1: Executing Main Pipeline...\")\n",
        "\n",
        "        # run_main_pipeline returns a PipelineArtifacts dataclass\n",
        "        main_artifacts = run_main_pipeline(\n",
        "            df_raw_corpus,\n",
        "            df_validation_raw,\n",
        "            language,\n",
        "            raw_lexicon_list,\n",
        "            study_configuration\n",
        "        )\n",
        "\n",
        "        logger.info(\"Main Pipeline execution successful.\")\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Step 2: Execute Leave-One-Out (LOO) Robustness Analysis\n",
        "        # ----------------------------------------------------------------------\n",
        "        logger.info(\">>> Step 2: Executing Leave-One-Out Robustness Analysis...\")\n",
        "\n",
        "        # Requires the analysis DataFrame (merged corpus + validation) from the main pipeline\n",
        "        loo_results = run_loo_analysis(\n",
        "            main_artifacts.df_analysis,\n",
        "            study_configuration\n",
        "        )\n",
        "\n",
        "        logger.info(\"LOO Analysis execution successful.\")\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Step 3: Execute Supervised Learning Robustness Analysis\n",
        "        # ----------------------------------------------------------------------\n",
        "        logger.info(\">>> Step 3: Executing Supervised Learning Robustness Analysis...\")\n",
        "\n",
        "        # Requires:\n",
        "        # - Irregularity segments (text source)\n",
        "        # - Analysis DataFrame (label source)\n",
        "        # - Corpus with counts (for PCA rebuild features)\n",
        "        supervised_results = run_supervised_robustness(\n",
        "            main_artifacts.df_irregularities,\n",
        "            main_artifacts.df_analysis,\n",
        "            main_artifacts.df_corpus_with_index,\n",
        "            study_configuration\n",
        "        )\n",
        "\n",
        "        logger.info(\"Supervised Robustness execution successful.\")\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Step 4: Assemble Final Output\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Convert dataclasses to dicts for the final return structure if preferred,\n",
        "        # or keep objects. Here we return a structured dictionary.\n",
        "        final_output = {\n",
        "            'main_pipeline': asdict(main_artifacts),\n",
        "            'loo_analysis': loo_results,\n",
        "            'supervised_robustness': asdict(supervised_results)\n",
        "        }\n",
        "\n",
        "        logger.info(\"End-to-End Pipeline Execution Completed Successfully.\")\n",
        "        return final_output\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"End-to-End Pipeline Failed: {str(e)}\")\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "We6OBHnDFupn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}